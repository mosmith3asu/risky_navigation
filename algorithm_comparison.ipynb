{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578ac5ae",
   "metadata": {},
   "source": [
    "# Algorithm Comparison for Behavioral Cloning\n",
    "\n",
    "This notebook provides a comparison of different neural network architectures for behavioral cloning in the risky navigation environment.\n",
    "\n",
    "**Algorithms Evaluated:**\n",
    "- **Linear**: Simple linear regression baseline\n",
    "- **AutoEncoder**: Neural network encoder-decoder architecture  \n",
    "- **Bayesian**: Bayesian neural network with uncertainty quantification\n",
    "- **Transformer**: Self-attention based model\n",
    "- **VAE**: Variational AutoEncoder with probabilistic latent representations\n",
    "\n",
    "**Workflow:**\n",
    "1. **Data Collection**: Load expert demonstrations from optimal visibility graph policy\n",
    "2. **Model Training**: Train each algorithm with simple, transparent training loop\n",
    "3. **Evaluation**: Test models in environment and compare performance\n",
    "4. **Analysis**: Visualize results and compare metrics\n",
    "\n",
    "This simplified approach prioritizes debuggability and clarity over automated hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077109",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn torch torchvision torchaudio gymnasium tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019876de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  if on runpod\n",
    "!rm -rf risky_navigation\n",
    "!git clone https://github.com/mosmith3asu/risky_navigation.git\n",
    "!cd risky_navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel to reload updated modules\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear the module cache for AutoEncoder\n",
    "modules_to_reload = [\n",
    "    'src.algorithms.AutoEncoder.agent',\n",
    "    'src.algorithms.Bayesian.agent', \n",
    "    'src.algorithms.Transformer.agent',\n",
    "    'src.algorithms.Linear.agent',\n",
    "    'src.algorithms.VAE.agent'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "# print current path\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c632d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment and algorithms\n",
    "import sys\n",
    "sys.path.append('/risky_navigation')\n",
    "\n",
    "from src.env.continuous_nav_env import ContinuousNavigationEnv\n",
    "from src.algorithms.AutoEncoder.agent import AutoEncoderAgent\n",
    "from src.algorithms.Bayesian.agent import BayesianAgent\n",
    "from src.algorithms.Transformer.agent import TransformerAgent\n",
    "from src.algorithms.Linear.agent import LinearAgent\n",
    "from src.algorithms.VAE.agent import VAEAgent\n",
    "from src.utils.file_management import save_pickle, load_pickle\n",
    "from src.utils.logger import Logger\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Test AutoEncoder architecture to verify fix (state_dim=8 for full state)\n",
    "print(\"\\nTesting AutoEncoder architecture:\")\n",
    "test_model = AutoEncoderAgent(state_dim=8, action_dim=2, \n",
    "                             latent_dim=32, hidden_dims=[128, 64])\n",
    "print(\"AutoEncoder architecture verification successful!\")\n",
    "print(\"✓ Using state-only approach (8D state, no separate goal)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817ec8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a17c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Configuration\n",
    "CONFIG = {\n",
    "    'num_episodes': 1000,          # Episodes for data collection\n",
    "    'max_steps': 200,             # Max steps per episode\n",
    "    'batch_size': 256,            # Batch size\n",
    "    'num_epochs': 100,            # Training epochs\n",
    "    'val_ratio': 0.2,             # Validation set ratio\n",
    "    'num_test_episodes': 50,      # Episodes for testing\n",
    "    'lr': 1e-3,                   # Learning rate\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "}\n",
    "\n",
    "# Model-specific hyperparameters (manually chosen)\n",
    "MODEL_CONFIGS = {\n",
    "    'AutoEncoder': {'latent_dim': 32, 'hidden_dims': [128, 64]},\n",
    "    'Linear': {},  # No special hyperparameters\n",
    "    'Transformer': {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1},\n",
    "    'Bayesian': {'hidden_dim': 128, 'prior_std': 1.0},\n",
    "    'VAE': {'latent_dim': 32, 'hidden_dim': 128, 'beta': 1.0}\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Training epochs: {CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell RIGHT AFTER the \"Config\" section (after the CONFIG dictionary cell)\n",
    "\n",
    "# ============================================================\n",
    "# GPU OPTIMIZATION SETTINGS FOR RTX 4090\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFIGURING GPU OPTIMIZATIONS FOR RTX 4090\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Enable cuDNN auto-tuner for optimal convolution algorithms\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"✓ cuDNN benchmark mode enabled\")\n",
    "    \n",
    "    # Use TensorFloat32 (TF32) for faster matrix multiplication on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"✓ TF32 enabled for matrix operations\")\n",
    "    \n",
    "    # Set matmul precision for better performance\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"✓ Float32 matmul precision set to 'high'\")\n",
    "    \n",
    "    # Enable memory efficient attention if available\n",
    "    try:\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "        print(\"✓ Memory efficient scaled dot product enabled\")\n",
    "    except:\n",
    "        print(\"⚠ Memory efficient SDP not available (PyTorch < 2.0)\")\n",
    "    \n",
    "    # Pre-allocate GPU memory for better performance\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ GPU cache cleared\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Running on CPU will be very slow!\")\n",
    "\n",
    "# ============================================================\n",
    "# UPDATE CONFIG WITH OPTIMIZED BATCH SIZE FOR RTX 4090\n",
    "# ============================================================\n",
    "\n",
    "# Update batch size to maximize GPU utilization\n",
    "CONFIG['batch_size'] = 256  # Increased from 128 to utilize RTX 4090's 24GB VRAM\n",
    "\n",
    "print(f\"\\n✓ Batch size optimized for RTX 4090: {CONFIG['batch_size']}\")\n",
    "print(f\"✓ Expected GPU memory usage: ~8-12GB out of 24GB available\")\n",
    "print(f\"✓ This should increase GPU utilization from 20% to 80-95%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622191",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Training Function (No Hyperparameter Search)\n",
    "# FIXED: Use state-only (goal info is already in state dimensions 4-5)\n",
    "\n",
    "def train_agent_simple(agent, train_states, train_expert_actions, train_goals,\n",
    "                       val_states, val_expert_actions, val_goals, \n",
    "                       num_epochs=100, batch_size=256, device='cpu', verbose=True):\n",
    "    \"\"\"\n",
    "    Simple training loop with validation tracking.\n",
    "    FIXED: Uses state-only as input (goal info already in state[4:6])\n",
    "    \n",
    "    Args:\n",
    "        agent: Agent instance with train_step method\n",
    "        train/val data: Numpy arrays of states, actions, goals (goals kept for compatibility)\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        device: torch device\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses: Lists of losses per epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    n_train = len(train_states)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(n_train)\n",
    "        \n",
    "        for start_idx in range(0, n_train, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_train)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # Convert to tensors - STATE ONLY (no goal concatenation)\n",
    "            batch_states = torch.tensor(train_states[batch_indices], dtype=torch.float32, device=device)\n",
    "            batch_actions = torch.tensor(train_expert_actions[batch_indices], dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Train step (behavioral cloning: state -> expert_action)\n",
    "            # Pass None for actions and goals (not used in state-only mode)\n",
    "            loss = agent.train_step(batch_states, None, None, batch_actions)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation - STATE ONLY\n",
    "        val_states_t = torch.tensor(val_states, dtype=torch.float32, device=device)\n",
    "        val_actions_t = torch.tensor(val_expert_actions, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Predict on validation set - STATE ONLY\n",
    "            if hasattr(agent, 'model'):\n",
    "                agent.model.eval()\n",
    "                predictions = agent.model(val_states_t)  # State only!\n",
    "            elif hasattr(agent, 'encoder'):  # VAE\n",
    "                agent.encoder.eval()\n",
    "                agent.decoder.eval()\n",
    "                mu, _ = agent.encoder(val_states_t)  # State only!\n",
    "                predictions = agent.decoder(mu)\n",
    "            else:\n",
    "                predictions = agent.predict_action(val_states_t, None)  # State only!\n",
    "            \n",
    "            val_loss = torch.nn.functional.mse_loss(predictions, val_actions_t).item()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"Epoch {epoch:3d}/{num_epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "            \n",
    "            # Check for NaN\n",
    "            if np.isnan(avg_train_loss) or np.isnan(val_loss):\n",
    "                print(f\"WARNING: NaN detected at epoch {epoch}!\")\n",
    "                print(f\"  Train loss: {avg_train_loss}\")\n",
    "                print(f\"  Val loss: {val_loss}\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "print(\"Simple training function defined (FIXED: state-only mode)!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652df29",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the data collection to use correct action format\n",
    "# Need to convert waypoint directions to [throttle, steering] format\n",
    "\n",
    "def collect_rl_experience(env, num_episodes=100, max_steps=200):\n",
    "    \"\"\"\n",
    "    Collect RL training data using optimal policy from visibility graph.\n",
    "    IMPROVED: Use max throttle for a more effective expert.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes), desc='Collecting RL experience (IMPROVED)'):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        episode_transitions = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            # Get optimal action using visibility graph\n",
    "            current_pos = state[:2]\n",
    "            current_theta = state[2]  # Current heading angle\n",
    "            \n",
    "            try:\n",
    "                # Use environment's visibility graph for shortest path\n",
    "                if hasattr(env, 'vgraph'):\n",
    "                    path = env.vgraph.shortest_path(current_pos, goal)\n",
    "                    \n",
    "                    if len(path) > 1:\n",
    "                        # Direction to next waypoint\n",
    "                        next_waypoint = path[1]\n",
    "                        direction = next_waypoint - current_pos\n",
    "                        \n",
    "                        # Convert to [throttle, steering] format\n",
    "                        desired_theta = np.arctan2(direction[1], direction[0])\n",
    "                        \n",
    "                        # Steering: angular difference (normalized to [-pi, pi])\n",
    "                        angle_diff = desired_theta - current_theta\n",
    "                        angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "                        \n",
    "                        # Clip steering to action space\n",
    "                        steering = np.clip(angle_diff, env.action_space.low[1], env.action_space.high[1])\n",
    "                        \n",
    "                        # IMPROVED THROTTLE: Always move at max speed for an optimal expert\n",
    "                        throttle = env.action_space.high[0]\n",
    "                        \n",
    "                        action = np.array([throttle, steering])\n",
    "                    else:\n",
    "                        # At goal, stop\n",
    "                        action = np.array([0.0, 0.0])\n",
    "                else:\n",
    "                    # Fallback: direct to goal\n",
    "                    direction = goal - current_pos\n",
    "                    desired_theta = np.arctan2(direction[1], direction[0])\n",
    "                    angle_diff = desired_theta - current_theta\n",
    "                    angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "                    steering = np.clip(angle_diff, env.action_space.low[1], env.action_space.high[1])\n",
    "                    throttle = env.action_space.high[0]\n",
    "                    action = np.array([throttle, steering])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Fallback: move towards goal\n",
    "                direction = goal - current_pos\n",
    "                desired_theta = np.arctan2(direction[1], direction[0])\n",
    "                angle_diff = desired_theta - current_theta\n",
    "                angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "                steering = np.clip(angle_diff, env.action_space.low[1], env.action_space.high[1])\n",
    "                throttle = env.action_space.high[0]\n",
    "                action = np.array([throttle, steering])\n",
    "            \n",
    "            # Action is already in correct format, no need to clip again\n",
    "            # But we'll clip to be safe\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition (s, a, r, s', done)\n",
    "            episode_transitions.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action.copy(),\n",
    "                'reward': reward,\n",
    "                'next_state': next_state.copy(),\n",
    "                'done': done,\n",
    "                'goal': goal.copy()\n",
    "            })\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    successful_episodes += 1\n",
    "                break\n",
    "        \n",
    "        # Add all transitions from this episode\n",
    "        data.extend(episode_transitions)\n",
    "    \n",
    "    print(f\"Collected {len(data)} transitions from {num_episodes} episodes\")\n",
    "    print(f\"Success rate during data collection: {successful_episodes/num_episodes:.2%}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"IMPROVED data collection function defined!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ACTION REQUIRED:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Run the new cell I am adding to delete the old dataset.\")\n",
    "print(\"2. Then, run the subsequent cells to collect new data and retrain.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a347d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or collect data\n",
    "dataset_path = 'rl_experience_dataset.pickle'\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Loading existing dataset from {dataset_path}...\")\n",
    "    data = load_pickle(dataset_path)\n",
    "    print(f\"✓ Loaded {len(data)} transitions\")\n",
    "else:\n",
    "    print(f\"Dataset not found. Collecting new data...\")\n",
    "    env_collect = ContinuousNavigationEnv()\n",
    "    data = collect_rl_experience(env_collect, num_episodes=CONFIG['num_episodes'], max_steps=CONFIG['max_steps'])\n",
    "    save_pickle(data, dataset_path)\n",
    "    print(f\"✓ Saved dataset to {dataset_path}\")\n",
    "\n",
    "# Extract states, actions, and goals from data\n",
    "states = np.array([d['state'] for d in data])\n",
    "actions = np.array([d['action'] for d in data])\n",
    "next_states = np.array([d['next_state'] for d in data])\n",
    "rewards = np.array([d['reward'] for d in data])\n",
    "dones = np.array([d['done'] for d in data])\n",
    "goals = np.array([d['goal'] for d in data])\n",
    "\n",
    "print(f\"\\nData extracted:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Actions shape: {actions.shape}\")\n",
    "print(f\"  Goals shape: {goals.shape}\")\n",
    "print(f\"  Rewards shape: {rewards.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ACTION REQUIRED: Delete the old dataset\n",
    "# ============================================================\n",
    "# The existing dataset was created with a suboptimal expert.\n",
    "# We need to delete it to force re-collection with the improved expert logic.\n",
    "\n",
    "import os\n",
    "\n",
    "dataset_path = 'rl_experience_dataset.pickle'\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Deleting old dataset at: {dataset_path}\")\n",
    "    os.remove(dataset_path)\n",
    "    print(\"✓ Old dataset deleted.\")\n",
    "else:\n",
    "    print(\"No old dataset found. Ready to collect new data.\")\n",
    "\n",
    "print(\"\\n--> Please run the next cell to collect new, higher-quality data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions from the collected data\n",
    "STATE_DIM = states.shape[1]\n",
    "ACTION_DIM = actions.shape[1]\n",
    "GOAL_DIM = goals.shape[1]\n",
    "\n",
    "print(f\"Data dimensions:\")\n",
    "print(f\"  STATE_DIM = {STATE_DIM}\")\n",
    "print(f\"  ACTION_DIM = {ACTION_DIM}\")\n",
    "print(f\"  GOAL_DIM = {GOAL_DIM}\")\n",
    "print(f\"  Total samples = {len(states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment for evaluation\n",
    "env = ContinuousNavigationEnv()\n",
    "print(f\"Environment initialized: {env}\")\n",
    "print(f\"  State space: {env.observation_space.shape}\")\n",
    "print(f\"  Action space: {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49cc82",
   "metadata": {},
   "source": [
    "## Train All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SIMPLIFIED TRAINING - ALL ALGORITHMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data into train/val\n",
    "n_samples = len(states)\n",
    "n_train = int(n_samples * (1 - CONFIG['val_ratio']))\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "# Split data\n",
    "train_states = states[train_indices]\n",
    "train_actions = actions[train_indices]\n",
    "train_goals = goals[train_indices]\n",
    "\n",
    "val_states = states[val_indices]\n",
    "val_actions = actions[val_indices]\n",
    "val_goals = goals[val_indices]\n",
    "\n",
    "print(f\"Data split: {len(train_states)} train, {len(val_states)} val\")\n",
    "print()\n",
    "\n",
    "# Dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "# Train each algorithm\n",
    "algorithms_to_train = {\n",
    "    'Linear': (LinearAgent, {}),\n",
    "    'AutoEncoder': (AutoEncoderAgent, MODEL_CONFIGS['AutoEncoder']),\n",
    "    'Transformer': (TransformerAgent, MODEL_CONFIGS['Transformer']),\n",
    "    'Bayesian': (BayesianAgent, MODEL_CONFIGS['Bayesian']),\n",
    "    'VAE': (VAEAgent, MODEL_CONFIGS['VAE'])\n",
    "}\n",
    "\n",
    "for algo_name, (AgentClass, model_config) in algorithms_to_train.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Config: {model_config}\")\n",
    "    \n",
    "    # Create agent - FIXED: No goal_dim\n",
    "    agent = AgentClass(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        lr=CONFIG['lr'],\n",
    "        device=CONFIG['device'],\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining for {CONFIG['num_epochs']} epochs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses, val_losses = train_agent_simple(\n",
    "        agent, \n",
    "        train_states, train_actions, train_goals,\n",
    "        val_states, val_actions, val_goals,\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        device=CONFIG['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    all_results[algo_name] = {\n",
    "        'agent': agent,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{algo_name} Training Complete!\")\n",
    "    print(f\"  Train time: {train_time:.2f}s\")\n",
    "    print(f\"  Final train loss: {train_losses[-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {val_losses[-1]:.6f}\")\n",
    "    print(f\"  Min val loss: {min(val_losses):.6f} (epoch {np.argmin(val_losses)})\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for algo_name, results in all_results.items():\n",
    "    epochs = range(len(results['train_losses']))\n",
    "    axes[0].plot(epochs, results['train_losses'], label=algo_name, alpha=0.7)\n",
    "    axes[1].plot(epochs, results['val_losses'], label=algo_name, alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Loss Summary:\")\n",
    "print(f\"{'Algorithm':<15} {'Train Loss':<15} {'Val Loss':<15} {'Train Time (s)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for algo_name, results in all_results.items():\n",
    "    print(f\"{algo_name:<15} {results['train_losses'][-1]:<15.6f} {results['val_losses'][-1]:<15.6f} {results['train_time']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cee6b",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agents\n",
    "\n",
    "Now test the trained agents in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING ALL AGENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluation function (FIXED: state-only)\n",
    "def evaluate_agent_simple(agent, env, num_episodes=50, max_steps=200):\n",
    "    \"\"\"Evaluate agent in environment using state-only.\"\"\"\n",
    "    results = {\n",
    "        'rewards': [],\n",
    "        'successes': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Predict action using state only (goal is in state[4:6])\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device=CONFIG['device'])\n",
    "            action = agent.predict_action(state_t, None)  # No goal parameter\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.cpu().numpy()\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        results['rewards'].append(episode_reward)\n",
    "        # Check if goal was reached\n",
    "        success = info.get('reason', '') == 'goal_reached' if done else False\n",
    "        results['successes'].append(1 if success else 0)\n",
    "        results['steps'].append(step + 1)\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(results['rewards']),\n",
    "        'std_reward': np.std(results['rewards']),\n",
    "        'success_rate': np.mean(results['successes']),\n",
    "        'avg_steps': np.mean(results['steps'])\n",
    "    }\n",
    "\n",
    "# Evaluate each agent\n",
    "eval_results = {}\n",
    "\n",
    "for algo_name, results_dict in all_results.items():\n",
    "    print(f\"\\nEvaluating {algo_name}...\")\n",
    "    agent = results_dict['agent']\n",
    "    \n",
    "    eval_res = evaluate_agent_simple(\n",
    "        agent, env, \n",
    "        num_episodes=CONFIG['num_test_episodes'],\n",
    "        max_steps=CONFIG['max_steps']\n",
    "    )\n",
    "    \n",
    "    eval_results[algo_name] = eval_res\n",
    "    \n",
    "    print(f\"  Avg Reward: {eval_res['avg_reward']:.3f} ± {eval_res['std_reward']:.3f}\")\n",
    "    print(f\"  Success Rate: {eval_res['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Steps: {eval_res['avg_steps']:.1f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "import pandas as pd\n",
    "\n",
    "results_data = []\n",
    "for algo_name in all_results.keys():\n",
    "    train_res = all_results[algo_name]\n",
    "    eval_res = eval_results[algo_name]\n",
    "    \n",
    "    results_data.append({\n",
    "        'Algorithm': algo_name,\n",
    "        'Train Loss': train_res['train_losses'][-1],\n",
    "        'Val Loss': train_res['val_losses'][-1],\n",
    "        'Train Time (s)': train_res['train_time'],\n",
    "        'Avg Reward': eval_res['avg_reward'],\n",
    "        'Success Rate': eval_res['success_rate'],\n",
    "        'Avg Steps': eval_res['avg_steps']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "df_results = df_results.sort_values('Val Loss')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Success Rate\n",
    "axes[0].bar(df_results['Algorithm'], df_results['Success Rate'])\n",
    "axes[0].set_ylabel('Success Rate')\n",
    "axes[0].set_title('Success Rate by Algorithm')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average Reward\n",
    "axes[1].bar(df_results['Algorithm'], df_results['Avg Reward'])\n",
    "axes[1].set_ylabel('Average Reward')\n",
    "axes[1].set_title('Average Reward by Algorithm')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "axes[2].bar(df_results['Algorithm'], df_results['Val Loss'])\n",
    "axes[2].set_ylabel('Validation Loss')\n",
    "axes[2].set_title('Validation Loss by Algorithm')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be17733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEBUGGING: Action and State Analysis (FIXED: state-only)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test one agent on a single episode with detailed logging\n",
    "agent = all_results['Linear']['agent']  # Use Linear for simplicity\n",
    "state = env.reset()\n",
    "goal = env.goal.copy()\n",
    "\n",
    "print(f\"\\nInitial State: {state}\")\n",
    "print(f\"  Position (x,y): {state[:2]}\")\n",
    "print(f\"  Heading (theta): {state[2]:.3f}\")\n",
    "print(f\"  Velocity: {state[3]:.3f}\")\n",
    "print(f\"  Goal direction (dx,dy): {state[4:6]}\")\n",
    "print(f\"  Obstacle direction (dx,dy): {state[6:8]}\")\n",
    "print(f\"\\nGoal Position: {goal}\")\n",
    "print(f\"Initial Distance to Goal: {np.linalg.norm(state[:2] - goal):.3f}\")\n",
    "print(f\"\\nAction Space: [{env.action_space.low}, {env.action_space.high}]\")\n",
    "\n",
    "# Take 5 steps and inspect actions\n",
    "print(f\"\\n{'Step':<6} {'Action':<20} {'State[:2]':<20} {'Distance':<10} {'Reward':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in range(5):\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=CONFIG['device'])\n",
    "    action = agent.predict_action(state_t, None)  # No goal parameter\n",
    "    \n",
    "    if isinstance(action, torch.Tensor):\n",
    "        action = action.cpu().numpy()\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    distance = np.linalg.norm(next_state[:2] - goal)\n",
    "    \n",
    "    print(f\"{step:<6} {str(action):<20} {str(next_state[:2]):<20} {distance:<10.3f} {reward:<8.3f}\")\n",
    "    \n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(f\"\\nEpisode ended: {info.get('reason', 'unknown')}\")\n",
    "        break\n",
    "\n",
    "# Compare with training data actions\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training Data Action Statistics:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Expert actions - Mean: {actions.mean(axis=0)}\")\n",
    "print(f\"Expert actions - Std:  {actions.std(axis=0)}\")\n",
    "print(f\"Expert actions - Min:  {actions.min(axis=0)}\")\n",
    "print(f\"Expert actions - Max:  {actions.max(axis=0)}\")\n",
    "\n",
    "# Check model predictions vs expert on validation set\n",
    "val_states_sample = val_states[:10]\n",
    "val_actions_sample = val_actions[:10]\n",
    "\n",
    "val_states_t = torch.tensor(val_states_sample, dtype=torch.float32, device=CONFIG['device'])\n",
    "predictions = agent.predict_action(val_states_t, None)  # No goal parameter\n",
    "\n",
    "if isinstance(predictions, torch.Tensor):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "print(f\"\\nValidation Set Predictions vs Expert:\")\n",
    "print(f\"{'Sample':<8} {'Predicted Action':<25} {'Expert Action':<25} {'Error':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(5):\n",
    "    pred = predictions[i]\n",
    "    expert = val_actions_sample[i]\n",
    "    error = np.linalg.norm(pred - expert)\n",
    "    print(f\"{i:<8} {str(pred):<25} {str(expert):<25} {error:<10.6f}\")\n",
    "\n",
    "print(f\"\\nAverage prediction error: {np.linalg.norm(predictions - val_actions_sample, axis=1).mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e09e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
