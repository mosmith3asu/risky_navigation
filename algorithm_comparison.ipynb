{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578ac5ae",
   "metadata": {},
   "source": [
    "# Algorithm Comparison for Next-Action Prediction (with Hyperparameter Tuning)\n",
    "\n",
    "This notebook provides a comprehensive comparison of different algorithms for next-action prediction in the risky navigation environment, following machine learning best practices with proper hyperparameter tuning:\n",
    "\n",
    "**Algorithms Evaluated:**\n",
    "- **A2C (Advantage Actor-Critic)**: Reinforcement learning approach\n",
    "- **AutoEncoder**: Neural network encoder-decoder architecture  \n",
    "- **Bayesian**: Bayesian neural network with uncertainty quantification\n",
    "- **Transformer**: Self-attention based model\n",
    "- **Linear**: Simple linear regression baseline\n",
    "- **VAE**: Variational AutoEncoder with probabilistic latent representations\n",
    "\n",
    "**Best Practices Workflow:**\n",
    "1. **Data Collection & Preparation**: Collect training data and split into train/validation sets\n",
    "2. **Hyperparameter Tuning**: For each algorithm, perform cross-validation hyperparameter search\n",
    "3. **Model Training**: Train each algorithm with optimal hyperparameters found\n",
    "4. **Evaluation**: Test final models on unseen data and compare performance\n",
    "5. **Analysis**: Comprehensive results analysis with visualizations\n",
    "\n",
    "This approach ensures fair comparison by optimizing each algorithm's hyperparameters before evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077109",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec3f6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn torch torchvision torchaudio gymnasium tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "019876de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'risky_navigation'...\n",
      "remote: Enumerating objects: 614, done.\u001b[K\n",
      "remote: Enumerating objects: 614, done.\u001b[KK\n",
      "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
      "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
      "remote: Compressing objects: 100% (150/150), done.\u001b[K\n",
      "remote: Compressing objects: 100% (150/150), done.\u001b[K\n",
      "remote: Total 614 (delta 121), reused 161 (delta 69), pack-reused 392 (from 1)\u001b[K\n",
      "Receiving objects: 100% (614/614), 18.98 MiB | 31.97 MiB/s, done.\n",
      "Resolving deltas: 100% (301/301), done.\n",
      "remote: Total 614 (delta 121), reused 161 (delta 69), pack-reused 392 (from 1)\u001b[K\n",
      "Receiving objects: 100% (614/614), 18.98 MiB | 31.97 MiB/s, done.\n",
      "Resolving deltas: 100% (301/301), done.\n"
     ]
    }
   ],
   "source": [
    "#  if on runpod\n",
    "!rm -rf risky_navigation\n",
    "!git clone https://github.com/mosmith3asu/risky_navigation.git\n",
    "!cd risky_navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe2cea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "# Restart the kernel to reload updated modules\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear the module cache for AutoEncoder\n",
    "modules_to_reload = [\n",
    "    'src.algorithms.AutoEncoder.agent',\n",
    "    'src.algorithms.Bayesian.agent', \n",
    "    'src.algorithms.Transformer.agent',\n",
    "    'src.algorithms.Linear.agent',\n",
    "    'src.algorithms.VAE.agent'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "# print current path\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c632d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "Device available: CUDA\n",
      "\n",
      "Testing AutoEncoder architecture:\n",
      "AutoEncoder architecture verification successful!\n"
     ]
    }
   ],
   "source": [
    "# Import environment and algorithms\n",
    "import sys\n",
    "sys.path.append('/risky_navigation')\n",
    "\n",
    "from src.env.continuous_nav_env import ContinuousNavigationEnv\n",
    "from src.algorithms.AutoEncoder.agent import AutoEncoderAgent\n",
    "from src.algorithms.Bayesian.agent import BayesianAgent\n",
    "from src.algorithms.Transformer.agent import TransformerAgent\n",
    "from src.algorithms.Linear.agent import LinearAgent\n",
    "from src.algorithms.VAE.agent import VAEAgent\n",
    "from src.utils.file_management import save_pickle, load_pickle\n",
    "from src.utils.logger import Logger\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Test AutoEncoder architecture to verify fix\n",
    "print(\"\\nTesting AutoEncoder architecture:\")\n",
    "test_model = AutoEncoderAgent(state_dim=4, action_dim=2, goal_dim=2, \n",
    "                             latent_dim=32, hidden_dims=[128, 64])\n",
    "print(\"AutoEncoder architecture verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817ec8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "69a17c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU optimizations enabled for RTX 4090\n",
      "  - CUDA Version: 12.8\n",
      "  - cuDNN Benchmark: Enabled\n",
      "  - TF32 Acceleration: Enabled\n",
      "\n",
      "Environment dimensions:\n",
      "  State dimension: 8\n",
      "  Action dimension: 2\n",
      "  Goal dimension: 2\n",
      "  Device: cuda\n",
      "  Batch size: 256\n",
      "\n",
      "Hyperparameter tuning configuration:\n",
      "  Search method: random\n",
      "  Random trials per algorithm: 25\n",
      "  ✓ This is ~38x faster than grid search!\n",
      "  CV folds: 3\n",
      "  HP search epochs: 30 (with early stopping)\n",
      "  Early stopping patience: 5\n",
      "  Algorithms configured: ['AutoEncoder', 'Bayesian', 'Transformer', 'Linear', 'VAE']\n"
     ]
    }
   ],
   "source": [
    "# Simplified Configuration\n",
    "CONFIG = {\n",
    "    'num_episodes': 1000,          # Episodes for data collection\n",
    "    'max_steps': 200,             # Max steps per episode\n",
    "    'batch_size': 256,            # Batch size\n",
    "    'num_epochs': 100,            # Training epochs\n",
    "    'val_ratio': 0.2,             # Validation set ratio\n",
    "    'num_test_episodes': 50,      # Episodes for testing\n",
    "    'lr': 1e-3,                   # Learning rate\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "}\n",
    "\n",
    "# Model-specific hyperparameters (manually chosen)\n",
    "MODEL_CONFIGS = {\n",
    "    'AutoEncoder': {'latent_dim': 32, 'hidden_dims': [128, 64]},\n",
    "    'Linear': {},  # No special hyperparameters\n",
    "    'Transformer': {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1},\n",
    "    'Bayesian': {'hidden_dim': 128, 'prior_std': 1.0},\n",
    "    'VAE': {'latent_dim': 32, 'hidden_dim': 128, 'beta': 1.0}\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Training epochs: {CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a55f165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONFIGURING GPU OPTIMIZATIONS FOR RTX 4090\n",
      "============================================================\n",
      "✓ cuDNN benchmark mode enabled\n",
      "✓ TF32 enabled for matrix operations\n",
      "✓ Float32 matmul precision set to 'high'\n",
      "✓ Memory efficient scaled dot product enabled\n",
      "✓ GPU cache cleared\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Batch size optimized for RTX 4090: 256\n",
      "✓ Expected GPU memory usage: ~8-12GB out of 24GB available\n",
      "✓ This should increase GPU utilization from 20% to 80-95%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this cell RIGHT AFTER the \"Config\" section (after the CONFIG dictionary cell)\n",
    "\n",
    "# ============================================================\n",
    "# GPU OPTIMIZATION SETTINGS FOR RTX 4090\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFIGURING GPU OPTIMIZATIONS FOR RTX 4090\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Enable cuDNN auto-tuner for optimal convolution algorithms\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"✓ cuDNN benchmark mode enabled\")\n",
    "    \n",
    "    # Use TensorFloat32 (TF32) for faster matrix multiplication on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"✓ TF32 enabled for matrix operations\")\n",
    "    \n",
    "    # Set matmul precision for better performance\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"✓ Float32 matmul precision set to 'high'\")\n",
    "    \n",
    "    # Enable memory efficient attention if available\n",
    "    try:\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "        print(\"✓ Memory efficient scaled dot product enabled\")\n",
    "    except:\n",
    "        print(\"⚠ Memory efficient SDP not available (PyTorch < 2.0)\")\n",
    "    \n",
    "    # Pre-allocate GPU memory for better performance\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ GPU cache cleared\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Running on CPU will be very slow!\")\n",
    "\n",
    "# ============================================================\n",
    "# UPDATE CONFIG WITH OPTIMIZED BATCH SIZE FOR RTX 4090\n",
    "# ============================================================\n",
    "\n",
    "# Update batch size to maximize GPU utilization\n",
    "CONFIG['batch_size'] = 256  # Increased from 128 to utilize RTX 4090's 24GB VRAM\n",
    "\n",
    "print(f\"\\n✓ Batch size optimized for RTX 4090: {CONFIG['batch_size']}\")\n",
    "print(f\"✓ Expected GPU memory usage: ~8-12GB out of 24GB available\")\n",
    "print(f\"✓ This should increase GPU utilization from 20% to 80-95%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622191",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a2b379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Training Function (No Hyperparameter Search)\n",
    "\n",
    "def train_agent_simple(agent, train_states, train_expert_actions, train_goals,\n",
    "                       val_states, val_expert_actions, val_goals, \n",
    "                       num_epochs=100, batch_size=256, device='cpu', verbose=True):\n",
    "    \"\"\"\n",
    "    Simple training loop with validation tracking.\n",
    "    \n",
    "    Args:\n",
    "        agent: Agent instance with train_step method\n",
    "        train/val data: Numpy arrays of states, actions, goals\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        device: torch device\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses: Lists of losses per epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    n_train = len(train_states)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(n_train)\n",
    "        \n",
    "        for start_idx in range(0, n_train, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_train)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # Convert to tensors\n",
    "            batch_states = torch.tensor(train_states[batch_indices], dtype=torch.float32, device=device)\n",
    "            batch_actions = torch.tensor(train_expert_actions[batch_indices], dtype=torch.float32, device=device)\n",
    "            batch_goals = torch.tensor(train_goals[batch_indices], dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Train step (behavioral cloning: state+goal -> expert_action)\n",
    "            loss = agent.train_step(batch_states, None, batch_goals, batch_actions)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_states_t = torch.tensor(val_states, dtype=torch.float32, device=device)\n",
    "        val_actions_t = torch.tensor(val_expert_actions, dtype=torch.float32, device=device)\n",
    "        val_goals_t = torch.tensor(val_goals, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Predict on validation set\n",
    "            inputs = torch.cat([val_states_t, val_goals_t], dim=-1)\n",
    "            if hasattr(agent, 'model'):\n",
    "                agent.model.eval()\n",
    "                predictions = agent.model(inputs)\n",
    "            elif hasattr(agent, 'encoder'):  # VAE\n",
    "                agent.encoder.eval()\n",
    "                agent.decoder.eval()\n",
    "                mu, _ = agent.encoder(inputs)\n",
    "                predictions = agent.decoder(mu)\n",
    "            else:\n",
    "                predictions = agent.predict_action(val_states_t, val_goals_t)\n",
    "            \n",
    "            val_loss = torch.nn.functional.mse_loss(predictions, val_actions_t).item()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"Epoch {epoch:3d}/{num_epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "            \n",
    "            # Check for NaN\n",
    "            if np.isnan(avg_train_loss) or np.isnan(val_loss):\n",
    "                print(f\"WARNING: NaN detected at epoch {epoch}!\")\n",
    "                print(f\"  Train loss: {avg_train_loss}\")\n",
    "                print(f\"  Val loss: {val_loss}\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "print(\"Simple training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652df29",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3e7f97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing RL dataset from rl_experience_dataset.pickle\n",
      "\n",
      "RL Dataset Statistics:\n",
      "  Total transitions: 41998\n",
      "  Avg reward: -1.442\n",
      "  Success rate (goal reached): 97.62%\n",
      "  State shape: (41998, 8)\n",
      "  Action shape: (41998, 2)\n",
      "\n",
      "RL Dataset Statistics:\n",
      "  Total transitions: 41998\n",
      "  Avg reward: -1.442\n",
      "  Success rate (goal reached): 97.62%\n",
      "  State shape: (41998, 8)\n",
      "  Action shape: (41998, 2)\n"
     ]
    }
   ],
   "source": [
    "def collect_rl_experience(env, num_episodes=100, max_steps=200):\n",
    "    \"\"\"\n",
    "    Collect RL training data using optimal policy from visibility graph.\n",
    "    This provides expert demonstrations for imitation learning.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes), desc='Collecting RL experience'):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        episode_transitions = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            # Get optimal action using visibility graph\n",
    "            current_pos = state[:2]\n",
    "            \n",
    "            try:\n",
    "                # Use environment's visibility graph for shortest path\n",
    "                if hasattr(env, 'vgraph'):\n",
    "                    path = env.vgraph.shortest_path(current_pos, goal)\n",
    "                    \n",
    "                    if len(path) > 1:\n",
    "                        # Direction to next waypoint\n",
    "                        next_waypoint = path[1]\n",
    "                        direction = next_waypoint - current_pos\n",
    "                        action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "                    else:\n",
    "                        action = np.zeros(2)\n",
    "                else:\n",
    "                    # Fallback: direct to goal\n",
    "                    direction = goal - current_pos\n",
    "                    action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Fallback: move towards goal\n",
    "                direction = goal - current_pos\n",
    "                action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "            \n",
    "            # Clip to action space\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition (s, a, r, s', done)\n",
    "            episode_transitions.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action.copy(),\n",
    "                'reward': reward,\n",
    "                'next_state': next_state.copy(),\n",
    "                'done': done,\n",
    "                'goal': goal.copy()\n",
    "            })\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    successful_episodes += 1\n",
    "                break\n",
    "        \n",
    "        # Add all transitions from this episode\n",
    "        data.extend(episode_transitions)\n",
    "    \n",
    "    print(f\"Collected {len(data)} transitions from {num_episodes} episodes\")\n",
    "    print(f\"Success rate: {successful_episodes/num_episodes:.2%}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_rl_data(data):\n",
    "    \"\"\"Convert RL experience to arrays for training.\"\"\"\n",
    "    states = np.stack([d['state'] for d in data])\n",
    "    actions = np.stack([d['action'] for d in data])\n",
    "    rewards = np.array([d['reward'] for d in data])\n",
    "    next_states = np.stack([d['next_state'] for d in data])\n",
    "    dones = np.array([d['done'] for d in data])\n",
    "    goals = np.stack([d['goal'] for d in data])\n",
    "    \n",
    "    return states, actions, rewards, next_states, dones, goals\n",
    "\n",
    "# Collect or load RL experience data\n",
    "dataset_path = 'rl_experience_dataset.pickle'\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Loading existing RL dataset from {dataset_path}\")\n",
    "    data = load_pickle(dataset_path)\n",
    "else:\n",
    "    print(\"Collecting RL experience data using optimal policy...\")\n",
    "    data = collect_rl_experience(env, CONFIG['num_episodes'], CONFIG['max_steps'])\n",
    "    save_pickle(data, dataset_path)\n",
    "    print(f\"RL dataset saved to {dataset_path}\")\n",
    "\n",
    "# Prepare RL data\n",
    "states, actions, rewards, next_states, dones, goals = prepare_rl_data(data)\n",
    "\n",
    "print(f\"\\nRL Dataset Statistics:\")\n",
    "print(f\"  Total transitions: {len(states)}\")\n",
    "print(f\"  Avg reward: {rewards.mean():.3f}\")\n",
    "print(f\"  Success rate (goal reached): {(rewards > 0).sum() / len(rewards):.2%}\")\n",
    "print(f\"  State shape: {states.shape}\")\n",
    "print(f\"  Action shape: {actions.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f3f2d",
   "metadata": {},
   "source": [
    "## Helper Functions for Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f50e8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_rl(agent, states, actions, rewards, next_states, dones, goals, num_epochs, batch_size, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train an agent using behavioral cloning from expert demonstrations.\n",
    "    Note: actions parameter contains the EXPERT actions to imitate.\n",
    "    \"\"\"\n",
    "    n_samples = len(states)\n",
    "    num_batches = n_samples // batch_size\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Split into train/val\n",
    "    val_size = int(0.2 * n_samples)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    train_idx, val_idx = indices[val_size:], indices[:val_size]\n",
    "    \n",
    "    print(f\"Training with {len(train_idx)} samples, validating with {len(val_idx)} samples...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data\n",
    "        train_idx_shuffled = np.random.permutation(train_idx)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_num in range(num_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(train_idx_shuffled))\n",
    "            batch_indices = train_idx_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Get batch\n",
    "            batch_states = states[batch_indices]\n",
    "            batch_expert_actions = actions[batch_indices]  # These are expert actions\n",
    "            batch_goals = goals[batch_indices]\n",
    "            \n",
    "            # Convert to tensors\n",
    "            device = getattr(agent, 'device', 'cpu')\n",
    "            batch_states_t = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "            batch_expert_actions_t = torch.tensor(batch_expert_actions, dtype=torch.float32, device=device)\n",
    "            batch_goals_t = torch.tensor(batch_goals, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Behavioral cloning: predict expert action from state + goal\n",
    "            # Pass None for the unused 'action' parameter (2nd arg)\n",
    "            loss_result = agent.train_step(batch_states_t, None, batch_goals_t, batch_expert_actions_t)\n",
    "            if isinstance(loss_result, dict):\n",
    "                loss = loss_result['loss']\n",
    "            else:\n",
    "                loss = loss_result\n",
    "            \n",
    "            epoch_loss += loss\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        \n",
    "        # Validation\n",
    "        val_states_t = torch.tensor(states[val_idx], dtype=torch.float32, device=device)\n",
    "        val_expert_actions_t = torch.tensor(actions[val_idx], dtype=torch.float32, device=device)\n",
    "        val_goals_t = torch.tensor(goals[val_idx], dtype=torch.float32, device=device)\n",
    "        val_loss = compute_validation_loss_rl(agent, val_states_t, val_expert_actions_t, val_goals_t)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}: Train Loss={avg_train_loss:.6f}, Val Loss={val_loss:.6f} {'✓' if patience_counter == 0 else f'[patience {patience_counter}/{CONFIG.get(\"patience\", 5)}]'}\")\n",
    "        \n",
    "        if patience_counter >= CONFIG.get('patience', 5):\n",
    "            print(f\"  Early stopping at epoch {epoch+1} (best val loss: {best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def compute_validation_loss_rl(agent, states, expert_actions, goals):\n",
    "    \"\"\"Compute validation loss for behavioral cloning.\"\"\"\n",
    "    # Set to eval mode\n",
    "    if hasattr(agent, 'model'):\n",
    "        agent.model.eval()\n",
    "    if hasattr(agent, 'encoder'):\n",
    "        agent.encoder.eval()\n",
    "    if hasattr(agent, 'decoder'):\n",
    "        agent.decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # For models that predict actions from state+goal\n",
    "        if hasattr(agent, 'model'):\n",
    "            # Concatenate state and goal as input\n",
    "            inputs = torch.cat([states, goals], dim=-1)\n",
    "            predictions = agent.model(inputs)\n",
    "            val_loss = torch.mean((predictions - expert_actions)**2).item()\n",
    "        elif hasattr(agent, 'encoder') and hasattr(agent, 'decoder'):\n",
    "            # For encoder-decoder architectures (Bayesian, VAE)\n",
    "            # Sample a subset to avoid memory issues\n",
    "            subset_size = min(1000, len(states))\n",
    "            indices = torch.randperm(len(states))[:subset_size]\n",
    "            \n",
    "            states_subset = states[indices]\n",
    "            expert_actions_subset = expert_actions[indices]\n",
    "            goals_subset = goals[indices]\n",
    "            \n",
    "            # Concatenate state and goal\n",
    "            inputs = torch.cat([states_subset, goals_subset], dim=-1)\n",
    "            \n",
    "            # Different handling for different architectures\n",
    "            if hasattr(agent, 'encoder') and hasattr(agent, 'forward'):\n",
    "                # VAE or Bayesian agent\n",
    "                try:\n",
    "                    if hasattr(agent, 'compute_loss'):\n",
    "                        # VAE\n",
    "                        loss_dict = agent.compute_loss(states_subset, goals_subset, expert_actions_subset)\n",
    "                        val_loss = loss_dict['loss'].item() if isinstance(loss_dict['loss'], torch.Tensor) else loss_dict['loss']\n",
    "                    else:\n",
    "                        # Bayesian\n",
    "                        z = agent.encoder(inputs)\n",
    "                        mean, _ = agent.decoder(z)\n",
    "                        val_loss = torch.mean((mean - expert_actions_subset)**2).item()\n",
    "                except Exception as e:\n",
    "                    # Fallback\n",
    "                    val_loss = 0.5\n",
    "            else:\n",
    "                val_loss = 0.5\n",
    "        else:\n",
    "            val_loss = 0.5\n",
    "    \n",
    "    # Restore training mode\n",
    "    if hasattr(agent, 'model'):\n",
    "        agent.model.train()\n",
    "    if hasattr(agent, 'encoder'):\n",
    "        agent.encoder.train()\n",
    "    if hasattr(agent, 'decoder'):\n",
    "        agent.decoder.train()\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes=100, max_steps=200):\n",
    "    \"\"\"\n",
    "    Evaluate an agent in the environment.\n",
    "    Returns average reward, MSE vs optimal actions, and success rate.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    ep_mses = []\n",
    "    success_count = 0\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        ep_reward = 0.0\n",
    "        ep_mses = []\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            # Get action from agent\n",
    "            try:\n",
    "                if hasattr(agent, 'predict_action'):\n",
    "                    action_result = agent.predict_action(state, goal)\n",
    "                else:\n",
    "                    # Fallback to predict_next_action\n",
    "                    dummy_action = np.zeros(env.action_space.shape[0])\n",
    "                    action_result = agent.predict_next_action(state, dummy_action, goal)\n",
    "                \n",
    "                if isinstance(action_result, tuple):\n",
    "                    action = action_result[0]\n",
    "                else:\n",
    "                    action = action_result\n",
    "                \n",
    "                # Ensure correct shape\n",
    "                if isinstance(action, torch.Tensor):\n",
    "                    action = action.detach().cpu().numpy()\n",
    "                if action.ndim > 1:\n",
    "                    action = action.flatten()\n",
    "                \n",
    "                action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Fallback to random action\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Calculate MSE with optimal action\n",
    "            try:\n",
    "                current_pos = state[:2]\n",
    "                if hasattr(env, 'vgraph'):\n",
    "                    path = env.vgraph.shortest_path(current_pos, goal)\n",
    "                    if len(path) > 1:\n",
    "                        direction = path[1] - current_pos\n",
    "                        optimal_action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "                    else:\n",
    "                        optimal_action = np.zeros_like(action)\n",
    "                else:\n",
    "                    direction = goal - current_pos\n",
    "                    optimal_action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "                \n",
    "                optimal_action = np.clip(optimal_action, env.action_space.low, env.action_space.high)\n",
    "                mse = np.mean((action - optimal_action)**2)\n",
    "                ep_mses.append(mse)\n",
    "            except:\n",
    "                ep_mses.append(0.5)\n",
    "            \n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    success_count += 1\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(ep_reward)\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(episode_rewards),\n",
    "        'avg_mse': np.mean(ep_mses) if ep_mses else 0.5,\n",
    "        'success_rate': success_count / num_episodes\n",
    "    }\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model.\"\"\"\n",
    "    if hasattr(model, 'model'):\n",
    "        return sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "    elif hasattr(model, 'encoder') and hasattr(model, 'decoder'):\n",
    "        return sum(p.numel() for p in model.encoder.parameters() if p.requires_grad) + \\\n",
    "               sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49cc82",
   "metadata": {},
   "source": [
    "## AutoEncoder: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9da6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUTOENCODER: HYPERPARAMETER TUNING & TRAINING\n",
      "============================================================\n",
      "Step 1: Hyperparameter Tuning\n",
      "\n",
      "Starting hyperparameter search for AutoEncoder...\n",
      "Search method: random\n",
      "Testing 25 hyperparameter combinations\n",
      "\n",
      "  [1/25] Testing: {'latent_dim': 32, 'hidden_dims': [128], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'GELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000110 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000110 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000136 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000136 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000122 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000123 ± 0.000010 ✓ NEW BEST!\n",
      "\n",
      "  [2/25] Testing: {'latent_dim': 128, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000122 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000123 ± 0.000010 ✓ NEW BEST!\n",
      "\n",
      "  [2/25] Testing: {'latent_dim': 128, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000000 (stopped at epoch 25)\n",
      "    Fold 2/3: Val Loss = 0.000000 (stopped at epoch 25)\n",
      "    Fold 3/3: Val Loss = 0.000000 (stopped at epoch 20)\n",
      "    Average CV Score: 0.000000 ± 0.000000 ✓ NEW BEST!\n",
      "\n",
      "  [3/25] Testing: {'latent_dim': 128, 'hidden_dims': [256, 128], 'lr': 0.01, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000000 (stopped at epoch 20)\n",
      "    Average CV Score: 0.000000 ± 0.000000 ✓ NEW BEST!\n",
      "\n",
      "  [3/25] Testing: {'latent_dim': 128, 'hidden_dims': [256, 128], 'lr': 0.01, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.002031 (stopped at epoch 10)\n",
      "    Fold 1/3: Val Loss = 0.002031 (stopped at epoch 10)\n",
      "    Fold 2/3: Val Loss = 0.000450 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000450 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.001262 (stopped at epoch 11)\n",
      "    Average CV Score: 0.001248 ± 0.000646\n",
      "\n",
      "  [4/25] Testing: {'latent_dim': 128, 'hidden_dims': [64, 32], 'lr': 0.001, 'dropout': 0.1, 'activation': 'GELU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.001262 (stopped at epoch 11)\n",
      "    Average CV Score: 0.001248 ± 0.000646\n",
      "\n",
      "  [4/25] Testing: {'latent_dim': 128, 'hidden_dims': [64, 32], 'lr': 0.001, 'dropout': 0.1, 'activation': 'GELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000021 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000021 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000049 (stopped at epoch 11)\n",
      "    Fold 2/3: Val Loss = 0.000049 (stopped at epoch 11)\n",
      "    Fold 3/3: Val Loss = 0.000019 (stopped at epoch 26)\n",
      "    Average CV Score: 0.000030 ± 0.000014\n",
      "\n",
      "  [5/25] Testing: {'latent_dim': 32, 'hidden_dims': [256, 128], 'lr': 0.001, 'dropout': 0.0, 'activation': 'GELU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000019 (stopped at epoch 26)\n",
      "    Average CV Score: 0.000030 ± 0.000014\n",
      "\n",
      "  [5/25] Testing: {'latent_dim': 32, 'hidden_dims': [256, 128], 'lr': 0.001, 'dropout': 0.0, 'activation': 'GELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000002 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000002 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000002 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000002 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000004 (stopped at epoch 25)\n",
      "    Average CV Score: 0.000003 ± 0.000001\n",
      "\n",
      "  [6/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.0001, 'dropout': 0.0, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000004 (stopped at epoch 25)\n",
      "    Average CV Score: 0.000003 ± 0.000001\n",
      "\n",
      "  [6/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.0001, 'dropout': 0.0, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.000053 (stopped at epoch 25)\n",
      "    Fold 1/3: Val Loss = 0.000053 (stopped at epoch 25)\n",
      "    Fold 2/3: Val Loss = 0.000145 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000145 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000197 (stopped at epoch 18)\n",
      "    Average CV Score: 0.000132 ± 0.000060\n",
      "\n",
      "  [7/25] Testing: {'latent_dim': 32, 'hidden_dims': [64, 32], 'lr': 0.001, 'dropout': 0.1, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000197 (stopped at epoch 18)\n",
      "    Average CV Score: 0.000132 ± 0.000060\n",
      "\n",
      "  [7/25] Testing: {'latent_dim': 32, 'hidden_dims': [64, 32], 'lr': 0.001, 'dropout': 0.1, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000075 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000075 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000067 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000067 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000082 (stopped at epoch 27)\n",
      "    Average CV Score: 0.000075 ± 0.000006\n",
      "\n",
      "  [8/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.01, 'dropout': 0.1, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000082 (stopped at epoch 27)\n",
      "    Average CV Score: 0.000075 ± 0.000006\n",
      "\n",
      "  [8/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.01, 'dropout': 0.1, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000076 (stopped at epoch 26)\n",
      "    Fold 1/3: Val Loss = 0.000076 (stopped at epoch 26)\n",
      "    Fold 2/3: Val Loss = 0.000080 (stopped at epoch 19)\n",
      "    Fold 2/3: Val Loss = 0.000080 (stopped at epoch 19)\n",
      "    Fold 3/3: Val Loss = 0.000086 (stopped at epoch 11)\n",
      "    Average CV Score: 0.000081 ± 0.000004\n",
      "\n",
      "  [9/25] Testing: {'latent_dim': 128, 'hidden_dims': [128], 'lr': 0.01, 'dropout': 0.1, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000086 (stopped at epoch 11)\n",
      "    Average CV Score: 0.000081 ± 0.000004\n",
      "\n",
      "  [9/25] Testing: {'latent_dim': 128, 'hidden_dims': [128], 'lr': 0.01, 'dropout': 0.1, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000316 (stopped at epoch 14)\n",
      "    Fold 1/3: Val Loss = 0.000316 (stopped at epoch 14)\n",
      "    Fold 2/3: Val Loss = 0.000098 (stopped at epoch 22)\n",
      "    Fold 2/3: Val Loss = 0.000098 (stopped at epoch 22)\n",
      "    Fold 3/3: Val Loss = 0.000105 (stopped at epoch 27)\n",
      "    Average CV Score: 0.000173 ± 0.000101\n",
      "\n",
      "  [10/25] Testing: {'latent_dim': 64, 'hidden_dims': [64], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000105 (stopped at epoch 27)\n",
      "    Average CV Score: 0.000173 ± 0.000101\n",
      "\n",
      "  [10/25] Testing: {'latent_dim': 64, 'hidden_dims': [64], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.000264 (stopped at epoch 19)\n",
      "    Fold 1/3: Val Loss = 0.000264 (stopped at epoch 19)\n",
      "    Fold 2/3: Val Loss = 0.000043 (stopped at epoch 28)\n",
      "    Fold 2/3: Val Loss = 0.000043 (stopped at epoch 28)\n",
      "    Fold 3/3: Val Loss = 0.000044 (stopped at epoch 28)\n",
      "    Average CV Score: 0.000117 ± 0.000104\n",
      "\n",
      "  [11/25] Testing: {'latent_dim': 64, 'hidden_dims': [64, 32], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000044 (stopped at epoch 28)\n",
      "    Average CV Score: 0.000117 ± 0.000104\n",
      "\n",
      "  [11/25] Testing: {'latent_dim': 64, 'hidden_dims': [64, 32], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.007765 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.007765 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.006072 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.006072 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.010244 (stopped at epoch 30)\n",
      "    Average CV Score: 0.008027 ± 0.001713\n",
      "\n",
      "  [12/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.010244 (stopped at epoch 30)\n",
      "    Average CV Score: 0.008027 ± 0.001713\n",
      "\n",
      "  [12/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000021 (stopped at epoch 11)\n",
      "    Fold 1/3: Val Loss = 0.000021 (stopped at epoch 11)\n",
      "    Fold 2/3: Val Loss = 0.000018 (stopped at epoch 20)\n",
      "    Fold 2/3: Val Loss = 0.000018 (stopped at epoch 20)\n",
      "    Fold 3/3: Val Loss = 0.000018 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000019 ± 0.000001\n",
      "\n",
      "  [13/25] Testing: {'latent_dim': 64, 'hidden_dims': [256, 128], 'lr': 0.001, 'dropout': 0.1, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000018 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000019 ± 0.000001\n",
      "\n",
      "  [13/25] Testing: {'latent_dim': 64, 'hidden_dims': [256, 128], 'lr': 0.001, 'dropout': 0.1, 'activation': 'ELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000078 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000078 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000073 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000073 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000073 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000075 ± 0.000002\n",
      "\n",
      "  [14/25] Testing: {'latent_dim': 32, 'hidden_dims': [128, 64], 'lr': 0.0001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.000073 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000075 ± 0.000002\n",
      "\n",
      "  [14/25] Testing: {'latent_dim': 32, 'hidden_dims': [128, 64], 'lr': 0.0001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000000 ± 0.000000\n",
      "\n",
      "  [15/25] Testing: {'latent_dim': 32, 'hidden_dims': [64], 'lr': 0.01, 'dropout': 0.2, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000000 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000000 ± 0.000000\n",
      "\n",
      "  [15/25] Testing: {'latent_dim': 32, 'hidden_dims': [64], 'lr': 0.01, 'dropout': 0.2, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.000060 (stopped at epoch 10)\n",
      "    Fold 1/3: Val Loss = 0.000060 (stopped at epoch 10)\n",
      "    Fold 2/3: Val Loss = 0.000037 (stopped at epoch 18)\n",
      "    Fold 2/3: Val Loss = 0.000037 (stopped at epoch 18)\n",
      "    Fold 3/3: Val Loss = 0.000045 (stopped at epoch 13)\n",
      "    Average CV Score: 0.000047 ± 0.000009\n",
      "\n",
      "  [16/25] Testing: {'latent_dim': 128, 'hidden_dims': [], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000045 (stopped at epoch 13)\n",
      "    Average CV Score: 0.000047 ± 0.000009\n",
      "\n",
      "  [16/25] Testing: {'latent_dim': 128, 'hidden_dims': [], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.000551 (stopped at epoch 25)\n",
      "    Fold 1/3: Val Loss = 0.000551 (stopped at epoch 25)\n",
      "    Fold 2/3: Val Loss = 0.001176 (stopped at epoch 18)\n",
      "    Fold 2/3: Val Loss = 0.001176 (stopped at epoch 18)\n",
      "    Fold 3/3: Val Loss = 0.001133 (stopped at epoch 10)\n",
      "    Average CV Score: 0.000953 ± 0.000285\n",
      "\n",
      "  [17/25] Testing: {'latent_dim': 128, 'hidden_dims': [64, 32], 'lr': 0.0001, 'dropout': 0.1, 'activation': 'GELU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.001133 (stopped at epoch 10)\n",
      "    Average CV Score: 0.000953 ± 0.000285\n",
      "\n",
      "  [17/25] Testing: {'latent_dim': 128, 'hidden_dims': [64, 32], 'lr': 0.0001, 'dropout': 0.1, 'activation': 'GELU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000103 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.000103 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000085 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000085 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000036 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000075 ± 0.000028\n",
      "\n",
      "  [18/25] Testing: {'latent_dim': 64, 'hidden_dims': [], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000036 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000075 ± 0.000028\n",
      "\n",
      "  [18/25] Testing: {'latent_dim': 64, 'hidden_dims': [], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.001026 (stopped at epoch 14)\n",
      "    Fold 1/3: Val Loss = 0.001026 (stopped at epoch 14)\n",
      "    Fold 2/3: Val Loss = 0.000040 (stopped at epoch 24)\n",
      "    Fold 2/3: Val Loss = 0.000040 (stopped at epoch 24)\n",
      "    Fold 3/3: Val Loss = 0.001290 (stopped at epoch 8)\n",
      "    Average CV Score: 0.000786 ± 0.000538\n",
      "\n",
      "  [19/25] Testing: {'latent_dim': 32, 'hidden_dims': [64, 32], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'GELU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.001290 (stopped at epoch 8)\n",
      "    Average CV Score: 0.000786 ± 0.000538\n",
      "\n",
      "  [19/25] Testing: {'latent_dim': 32, 'hidden_dims': [64, 32], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'GELU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.014133 (stopped at epoch 20)\n",
      "    Fold 1/3: Val Loss = 0.014133 (stopped at epoch 20)\n",
      "    Fold 2/3: Val Loss = 0.002719 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.002719 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.013141 (stopped at epoch 16)\n",
      "    Average CV Score: 0.009998 ± 0.005163\n",
      "\n",
      "  [20/25] Testing: {'latent_dim': 64, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.013141 (stopped at epoch 16)\n",
      "    Average CV Score: 0.009998 ± 0.005163\n",
      "\n",
      "  [20/25] Testing: {'latent_dim': 64, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000314 (stopped at epoch 17)\n",
      "    Fold 1/3: Val Loss = 0.000314 (stopped at epoch 17)\n",
      "    Fold 2/3: Val Loss = 0.000206 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.000206 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.000339 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000286 ± 0.000058\n",
      "\n",
      "  [21/25] Testing: {'latent_dim': 128, 'hidden_dims': [128], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000339 (stopped at epoch 30)\n",
      "    Average CV Score: 0.000286 ± 0.000058\n",
      "\n",
      "  [21/25] Testing: {'latent_dim': 128, 'hidden_dims': [128], 'lr': 0.001, 'dropout': 0.2, 'activation': 'ReLU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.003555 (stopped at epoch 9)\n",
      "    Fold 1/3: Val Loss = 0.003555 (stopped at epoch 9)\n",
      "    Fold 2/3: Val Loss = 0.000259 (stopped at epoch 14)\n",
      "    Fold 2/3: Val Loss = 0.000259 (stopped at epoch 14)\n",
      "    Fold 3/3: Val Loss = 0.001886 (stopped at epoch 7)\n",
      "    Average CV Score: 0.001900 ± 0.001346\n",
      "\n",
      "  [22/25] Testing: {'latent_dim': 128, 'hidden_dims': [256, 128], 'lr': 0.001, 'dropout': 0.1, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 3/3: Val Loss = 0.001886 (stopped at epoch 7)\n",
      "    Average CV Score: 0.001900 ± 0.001346\n",
      "\n",
      "  [22/25] Testing: {'latent_dim': 128, 'hidden_dims': [256, 128], 'lr': 0.001, 'dropout': 0.1, 'activation': 'ReLU', 'batch_norm': False}\n",
      "    Fold 1/3: Val Loss = 0.000136 (stopped at epoch 14)\n",
      "    Fold 1/3: Val Loss = 0.000136 (stopped at epoch 14)\n",
      "    Fold 2/3: Val Loss = 0.000085 (stopped at epoch 15)\n",
      "    Fold 2/3: Val Loss = 0.000085 (stopped at epoch 15)\n",
      "    Fold 3/3: Val Loss = 0.000115 (stopped at epoch 22)\n",
      "    Average CV Score: 0.000112 ± 0.000021\n",
      "\n",
      "  [23/25] Testing: {'latent_dim': 32, 'hidden_dims': [256, 128], 'lr': 0.01, 'dropout': 0.1, 'activation': 'GELU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000115 (stopped at epoch 22)\n",
      "    Average CV Score: 0.000112 ± 0.000021\n",
      "\n",
      "  [23/25] Testing: {'latent_dim': 32, 'hidden_dims': [256, 128], 'lr': 0.01, 'dropout': 0.1, 'activation': 'GELU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.000070 (stopped at epoch 12)\n",
      "    Fold 1/3: Val Loss = 0.000070 (stopped at epoch 12)\n",
      "    Fold 2/3: Val Loss = 0.000113 (stopped at epoch 11)\n",
      "    Fold 2/3: Val Loss = 0.000113 (stopped at epoch 11)\n",
      "    Fold 3/3: Val Loss = 0.000064 (stopped at epoch 15)\n",
      "    Average CV Score: 0.000082 ± 0.000022\n",
      "\n",
      "  [24/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.01, 'dropout': 0.0, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000064 (stopped at epoch 15)\n",
      "    Average CV Score: 0.000082 ± 0.000022\n",
      "\n",
      "  [24/25] Testing: {'latent_dim': 128, 'hidden_dims': [64], 'lr': 0.01, 'dropout': 0.0, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.000068 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.000068 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.000010 (stopped at epoch 19)\n",
      "    Fold 2/3: Val Loss = 0.000010 (stopped at epoch 19)\n",
      "    Fold 3/3: Val Loss = 0.000100 (stopped at epoch 11)\n",
      "    Average CV Score: 0.000059 ± 0.000037\n",
      "\n",
      "  [25/25] Testing: {'latent_dim': 32, 'hidden_dims': [], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 3/3: Val Loss = 0.000100 (stopped at epoch 11)\n",
      "    Average CV Score: 0.000059 ± 0.000037\n",
      "\n",
      "  [25/25] Testing: {'latent_dim': 32, 'hidden_dims': [], 'lr': 0.0001, 'dropout': 0.2, 'activation': 'ELU', 'batch_norm': True}\n",
      "    Fold 1/3: Val Loss = 0.002667 (stopped at epoch 30)\n",
      "    Fold 1/3: Val Loss = 0.002667 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.003485 (stopped at epoch 30)\n",
      "    Fold 2/3: Val Loss = 0.003485 (stopped at epoch 30)\n",
      "    Fold 3/3: Val Loss = 0.002538 (stopped at epoch 30)\n",
      "    Average CV Score: 0.002897 ± 0.000420\n",
      "\n",
      "AutoEncoder Hyperparameter Search Complete!\n",
      "Best CV Score: 0.000000\n",
      "Best Hyperparameters: {'latent_dim': 128, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "\n",
      "Best AutoEncoder hyperparameters:\n",
      "  latent_dim: 128\n",
      "  hidden_dims: [128, 64]\n",
      "  lr: 0.001\n",
      "  dropout: 0.0\n",
      "  activation: ReLU\n",
      "  batch_norm: False\n",
      "Best CV score: 0.000000\n",
      "\n",
      "Step 2: Training AutoEncoder with best hyperparameters...\n",
      "Training with 33599 samples, validating with 8399 samples...\n",
      "    Fold 3/3: Val Loss = 0.002538 (stopped at epoch 30)\n",
      "    Average CV Score: 0.002897 ± 0.000420\n",
      "\n",
      "AutoEncoder Hyperparameter Search Complete!\n",
      "Best CV Score: 0.000000\n",
      "Best Hyperparameters: {'latent_dim': 128, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "\n",
      "Best AutoEncoder hyperparameters:\n",
      "  latent_dim: 128\n",
      "  hidden_dims: [128, 64]\n",
      "  lr: 0.001\n",
      "  dropout: 0.0\n",
      "  activation: ReLU\n",
      "  batch_norm: False\n",
      "Best CV score: 0.000000\n",
      "\n",
      "Step 2: Training AutoEncoder with best hyperparameters...\n",
      "Training with 33599 samples, validating with 8399 samples...\n",
      "  Early stopping at epoch 9 (best val loss: 0.000001)\n",
      "\n",
      "Step 3: Evaluating AutoEncoder...\n",
      "  Early stopping at epoch 9 (best val loss: 0.000001)\n",
      "\n",
      "Step 3: Evaluating AutoEncoder...\n",
      "\n",
      "AutoEncoder Final Results:\n",
      "  Best Hyperparams: {'latent_dim': 128, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "  HP Search CV Score: 0.000000\n",
      "  Train Time: 0.93s\n",
      "  Final Train Loss: nan\n",
      "  Final Val Loss: 0.000005\n",
      "  Avg MSE: 0.000005\n",
      "  Avg Reward: -60.576\n",
      "  Success Rate: 0.000\n",
      "  Model Parameters: 34,818\n",
      "\n",
      "\n",
      "AutoEncoder Final Results:\n",
      "  Best Hyperparams: {'latent_dim': 128, 'hidden_dims': [128, 64], 'lr': 0.001, 'dropout': 0.0, 'activation': 'ReLU', 'batch_norm': False}\n",
      "  HP Search CV Score: 0.000000\n",
      "  Train Time: 0.93s\n",
      "  Final Train Loss: nan\n",
      "  Final Val Loss: 0.000005\n",
      "  Avg MSE: 0.000005\n",
      "  Avg Reward: -60.576\n",
      "  Success Rate: 0.000\n",
      "  Model Parameters: 34,818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SIMPLIFIED TRAINING - ALL ALGORITHMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data into train/val\n",
    "n_samples = len(states)\n",
    "n_train = int(n_samples * (1 - CONFIG['val_ratio']))\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "# Split data\n",
    "train_states = states[train_indices]\n",
    "train_actions = actions[train_indices]\n",
    "train_goals = goals[train_indices]\n",
    "\n",
    "val_states = states[val_indices]\n",
    "val_actions = actions[val_indices]\n",
    "val_goals = goals[val_indices]\n",
    "\n",
    "print(f\"Data split: {len(train_states)} train, {len(val_states)} val\")\n",
    "print()\n",
    "\n",
    "# Dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "# Train each algorithm\n",
    "algorithms_to_train = {\n",
    "    'Linear': (LinearAgent, {}),\n",
    "    'AutoEncoder': (AutoEncoderAgent, MODEL_CONFIGS['AutoEncoder']),\n",
    "    'Transformer': (TransformerAgent, MODEL_CONFIGS['Transformer']),\n",
    "    'Bayesian': (BayesianAgent, MODEL_CONFIGS['Bayesian']),\n",
    "    'VAE': (VAEAgent, MODEL_CONFIGS['VAE'])\n",
    "}\n",
    "\n",
    "for algo_name, (AgentClass, model_config) in algorithms_to_train.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Config: {model_config}\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = AgentClass(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        goal_dim=GOAL_DIM,\n",
    "        lr=CONFIG['lr'],\n",
    "        device=CONFIG['device'],\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining for {CONFIG['num_epochs']} epochs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses, val_losses = train_agent_simple(\n",
    "        agent, \n",
    "        train_states, train_actions, train_goals,\n",
    "        val_states, val_actions, val_goals,\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        device=CONFIG['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    all_results[algo_name] = {\n",
    "        'agent': agent,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{algo_name} Training Complete!\")\n",
    "    print(f\"  Train time: {train_time:.2f}s\")\n",
    "    print(f\"  Final train loss: {train_losses[-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {val_losses[-1]:.6f}\")\n",
    "    print(f\"  Min val loss: {min(val_losses):.6f} (epoch {np.argmin(val_losses)})\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ffcf49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder model saved to risky_navigation/trained_models/autoencoder_model_20251122_230937.pt\n",
      "Hyperparameters saved to risky_navigation/trained_models/autoencoder_hyperparams_20251122_230937.json\n"
     ]
    }
   ],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for algo_name, results in all_results.items():\n",
    "    epochs = range(len(results['train_losses']))\n",
    "    axes[0].plot(epochs, results['train_losses'], label=algo_name, alpha=0.7)\n",
    "    axes[1].plot(epochs, results['val_losses'], label=algo_name, alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Loss Summary:\")\n",
    "print(f\"{'Algorithm':<15} {'Train Loss':<15} {'Val Loss':<15} {'Train Time (s)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for algo_name, results in all_results.items():\n",
    "    print(f\"{algo_name:<15} {results['train_losses'][-1]:<15.6f} {results['val_losses'][-1]:<15.6f} {results['train_time']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cee6b",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agents\n",
    "\n",
    "Now test the trained agents in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "76dfdccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BAYESIAN: HYPERPARAMETER TUNING & TRAINING\n",
      "============================================================\n",
      "Step 1: Hyperparameter Tuning\n",
      "\n",
      "Starting hyperparameter search for Bayesian...\n",
      "Search method: random\n",
      "Testing 25 hyperparameter combinations\n",
      "\n",
      "  [1/25] Testing: {'latent_dim': 64, 'lr': 0.001, 'kl_weight': 0.001, 'prior_std': 1.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000 ✓ NEW BEST!\n",
      "\n",
      "  [2/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.1, 'prior_std': 2.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000 ✓ NEW BEST!\n",
      "\n",
      "  [2/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.1, 'prior_std': 2.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [3/25] Testing: {'latent_dim': 64, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 2.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [3/25] Testing: {'latent_dim': 64, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 2.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [4/25] Testing: {'latent_dim': 32, 'lr': 0.0001, 'kl_weight': 0.1, 'prior_std': 1.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [4/25] Testing: {'latent_dim': 32, 'lr': 0.0001, 'kl_weight': 0.1, 'prior_std': 1.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [5/25] Testing: {'latent_dim': 128, 'lr': 0.0001, 'kl_weight': 0.01, 'prior_std': 2.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [5/25] Testing: {'latent_dim': 128, 'lr': 0.0001, 'kl_weight': 0.01, 'prior_std': 2.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [6/25] Testing: {'latent_dim': 128, 'lr': 0.001, 'kl_weight': 0.001, 'prior_std': 2.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [6/25] Testing: {'latent_dim': 128, 'lr': 0.001, 'kl_weight': 0.001, 'prior_std': 2.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [7/25] Testing: {'latent_dim': 128, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [7/25] Testing: {'latent_dim': 128, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [8/25] Testing: {'latent_dim': 32, 'lr': 0.0001, 'kl_weight': 0.01, 'prior_std': 2.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [8/25] Testing: {'latent_dim': 32, 'lr': 0.0001, 'kl_weight': 0.01, 'prior_std': 2.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [9/25] Testing: {'latent_dim': 64, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [9/25] Testing: {'latent_dim': 64, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [10/25] Testing: {'latent_dim': 32, 'lr': 0.0001, 'kl_weight': 0.01, 'prior_std': 2.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [10/25] Testing: {'latent_dim': 32, 'lr': 0.0001, 'kl_weight': 0.01, 'prior_std': 2.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [11/25] Testing: {'latent_dim': 64, 'lr': 0.01, 'kl_weight': 0.1, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [11/25] Testing: {'latent_dim': 64, 'lr': 0.01, 'kl_weight': 0.1, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [12/25] Testing: {'latent_dim': 128, 'lr': 0.001, 'kl_weight': 0.01, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [12/25] Testing: {'latent_dim': 128, 'lr': 0.001, 'kl_weight': 0.01, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [13/25] Testing: {'latent_dim': 128, 'lr': 0.01, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [13/25] Testing: {'latent_dim': 128, 'lr': 0.01, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [14/25] Testing: {'latent_dim': 64, 'lr': 0.01, 'kl_weight': 0.01, 'prior_std': 1.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [14/25] Testing: {'latent_dim': 64, 'lr': 0.01, 'kl_weight': 0.01, 'prior_std': 1.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [15/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.01, 'prior_std': 1.0}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [15/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.01, 'prior_std': 1.0}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [16/25] Testing: {'latent_dim': 128, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [16/25] Testing: {'latent_dim': 128, 'lr': 0.0001, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [17/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.1, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [17/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.1, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [18/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.01, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [18/25] Testing: {'latent_dim': 32, 'lr': 0.01, 'kl_weight': 0.01, 'prior_std': 0.5}\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 1/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 2/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [19/25] Testing: {'latent_dim': 64, 'lr': 0.01, 'kl_weight': 0.001, 'prior_std': 0.5}\n",
      "    Fold 3/3: Val Loss = 0.500000 (stopped at epoch 6)\n",
      "    Average CV Score: 0.500000 ± 0.000000\n",
      "\n",
      "  [19/25] Testing: {'latent_dim': 64, 'lr': 0.01, 'kl_weight': 0.001, 'prior_std': 0.5}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Step 1: Hyperparameter Tuning\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1: Hyperparameter Tuning\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m bayesian_best_hyperparams, bayesian_best_score, bayesian_search_results = \u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBayesian\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSTATE_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mACTION_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGOAL_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest Bayesian hyperparameters:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m bayesian_best_hyperparams.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mhyperparameter_search\u001b[39m\u001b[34m(algorithm_name, states, actions, rewards, next_states, dones, goals, state_dim, action_dim, goal_dim, device)\u001b[39m\n\u001b[32m    107\u001b[39m batch_goals_t = torch.tensor(train_goals[batch_indices], dtype=torch.float32, device=device)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Behavioral cloning: predict expert action from state+goal\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Pass None for unused action parameter (2nd arg)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m loss_result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_states_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_goals_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_expert_actions_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_result, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    113\u001b[39m     loss = loss_result[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/risky_navigation/src/algorithms/Bayesian/agent.py:162\u001b[39m, in \u001b[36mBayesianAgent.train_step\u001b[39m\u001b[34m(self, state, action, goal, expert_action)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    166\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: loss.item(),\n\u001b[32m    167\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnll\u001b[39m\u001b[33m'\u001b[39m: nll.item(),\n\u001b[32m    168\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mkl\u001b[39m\u001b[33m'\u001b[39m: kl.item()\n\u001b[32m    169\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING ALL AGENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_agent_simple(agent, env, num_episodes=50, max_steps=200):\n",
    "    \"\"\"Evaluate agent in environment.\"\"\"\n",
    "    results = {\n",
    "        'rewards': [],\n",
    "        'successes': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        goal = info.get('goal', np.zeros(GOAL_DIM))\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Predict action\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device=CONFIG['device'])\n",
    "            goal_t = torch.tensor(goal, dtype=torch.float32, device=CONFIG['device'])\n",
    "            action = agent.predict_action(state_t, goal_t)\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.cpu().numpy()\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        results['rewards'].append(episode_reward)\n",
    "        results['successes'].append(1 if info.get('success', False) else 0)\n",
    "        results['steps'].append(step + 1)\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(results['rewards']),\n",
    "        'std_reward': np.std(results['rewards']),\n",
    "        'success_rate': np.mean(results['successes']),\n",
    "        'avg_steps': np.mean(results['steps'])\n",
    "    }\n",
    "\n",
    "# Evaluate each agent\n",
    "eval_results = {}\n",
    "\n",
    "for algo_name, results_dict in all_results.items():\n",
    "    print(f\"\\nEvaluating {algo_name}...\")\n",
    "    agent = results_dict['agent']\n",
    "    \n",
    "    eval_res = evaluate_agent_simple(\n",
    "        agent, env, \n",
    "        num_episodes=CONFIG['num_test_episodes'],\n",
    "        max_steps=CONFIG['max_steps']\n",
    "    )\n",
    "    \n",
    "    eval_results[algo_name] = eval_res\n",
    "    \n",
    "    print(f\"  Avg Reward: {eval_res['avg_reward']:.3f} ± {eval_res['std_reward']:.3f}\")\n",
    "    print(f\"  Success Rate: {eval_res['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Steps: {eval_res['avg_steps']:.1f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "import pandas as pd\n",
    "\n",
    "results_data = []\n",
    "for algo_name in all_results.keys():\n",
    "    train_res = all_results[algo_name]\n",
    "    eval_res = eval_results[algo_name]\n",
    "    \n",
    "    results_data.append({\n",
    "        'Algorithm': algo_name,\n",
    "        'Train Loss': train_res['train_losses'][-1],\n",
    "        'Val Loss': train_res['val_losses'][-1],\n",
    "        'Train Time (s)': train_res['train_time'],\n",
    "        'Avg Reward': eval_res['avg_reward'],\n",
    "        'Success Rate': eval_res['success_rate'],\n",
    "        'Avg Steps': eval_res['avg_steps']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "df_results = df_results.sort_values('Val Loss')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Success Rate\n",
    "axes[0].bar(df_results['Algorithm'], df_results['Success Rate'])\n",
    "axes[0].set_ylabel('Success Rate')\n",
    "axes[0].set_title('Success Rate by Algorithm')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average Reward\n",
    "axes[1].bar(df_results['Algorithm'], df_results['Avg Reward'])\n",
    "axes[1].set_ylabel('Average Reward')\n",
    "axes[1].set_title('Average Reward by Algorithm')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "axes[2].bar(df_results['Algorithm'], df_results['Val Loss'])\n",
    "axes[2].set_ylabel('Validation Loss')\n",
    "axes[2].set_title('Validation Loss by Algorithm')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bdfe3",
   "metadata": {},
   "source": [
    "## Transformer: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRANSFORMER: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU cache before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "transformer_best_hyperparams, transformer_best_score, transformer_search_results = hyperparameter_search(\n",
    "    'Transformer', states, actions, rewards, next_states, dones, goals,\n",
    "    STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Transformer hyperparameters:\")\n",
    "for key, value in transformer_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {transformer_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training Transformer with best hyperparameters...\")\n",
    "transformer_agent = create_agent_with_hyperparams_rl(\n",
    "    'Transformer', transformer_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train Transformer with full epochs\n",
    "start_time = time.time()\n",
    "transformer_train_losses, transformer_val_losses = train_agent_rl(\n",
    "    transformer_agent, states, actions, rewards, next_states, dones, goals,\n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "transformer_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating Transformer...\")\n",
    "start_time = time.time()\n",
    "transformer_results = evaluate_agent(transformer_agent, env, CONFIG['num_test_episodes'])\n",
    "transformer_test_time = time.time() - start_time\n",
    "\n",
    "# Store results\n",
    "results['algorithm'].append('Transformer')\n",
    "results['train_time'].append(transformer_train_time)\n",
    "results['test_time'].append(transformer_test_time)\n",
    "results['final_train_loss'].append(transformer_train_losses[-1])\n",
    "results['final_val_loss'].append(transformer_val_losses[-1])\n",
    "results['avg_mse'].append(transformer_results['avg_mse'])\n",
    "results['avg_reward'].append(transformer_results['avg_reward'])\n",
    "results['success_rate'].append(transformer_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(transformer_agent))\n",
    "results['best_hyperparams'].append(transformer_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(transformer_best_score)\n",
    "\n",
    "print(f\"\\nTransformer Final Results:\")\n",
    "print(f\"  Best Hyperparams: {transformer_best_hyperparams}\")\n",
    "print(f\"  HP Search CV Score: {transformer_best_score:.6f}\")\n",
    "print(f\"  Train Time: {transformer_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {transformer_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {transformer_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {transformer_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {transformer_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {transformer_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(transformer_agent):,}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Bayesian model (robust universal method)\n",
    "# model_dir = \"trained_models\"\n",
    "model_dir = \"risky_navigation/trained_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, f\"transformer_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "\n",
    "# Use universal save function for all agent types\n",
    "save_agent_model(transformer_agent, model_path)\n",
    "\n",
    "# Also save hyperparameters for reproducibility\n",
    "hyperparams_path = os.path.join(model_dir, f\"transformer_hyperparams_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(hyperparams_path, 'w') as f:\n",
    "    json.dump(trans_best_hyperparams, f, indent=2)\n",
    "    print(f\"Hyperparameters saved to {hyperparams_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81606290",
   "metadata": {},
   "source": [
    "## Linear: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LINEAR: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU cache before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "linear_best_hyperparams, linear_best_score, linear_search_results = hyperparameter_search(\n",
    "    'Linear', states, actions, rewards, next_states, dones, goals,\n",
    "    STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Linear hyperparameters:\")\n",
    "for key, value in linear_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {linear_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training Linear with best hyperparameters...\")\n",
    "linear_agent = create_agent_with_hyperparams_rl(\n",
    "    'Linear', linear_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train Linear with full epochs\n",
    "start_time = time.time()\n",
    "linear_train_losses, linear_val_losses = train_agent_rl(\n",
    "    linear_agent, states, actions, rewards, next_states, dones, goals,\n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "linear_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating Linear...\")\n",
    "start_time = time.time()\n",
    "linear_results = evaluate_agent(linear_agent, env, CONFIG['num_test_episodes'])\n",
    "linear_test_time = time.time() - start_time\n",
    "\n",
    "# Store results\n",
    "results['algorithm'].append('Linear')\n",
    "results['train_time'].append(linear_train_time)\n",
    "results['test_time'].append(linear_test_time)\n",
    "results['final_train_loss'].append(linear_train_losses[-1])\n",
    "results['final_val_loss'].append(linear_val_losses[-1])\n",
    "results['avg_mse'].append(linear_results['avg_mse'])\n",
    "results['avg_reward'].append(linear_results['avg_reward'])\n",
    "results['success_rate'].append(linear_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(linear_agent))\n",
    "results['best_hyperparams'].append(linear_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(linear_best_score)\n",
    "\n",
    "print(f\"\\nLinear Final Results:\")\n",
    "print(f\"  Best Hyperparams: {linear_best_hyperparams}\")\n",
    "print(f\"  HP Search CV Score: {linear_best_score:.6f}\")\n",
    "print(f\"  Train Time: {linear_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {linear_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {linear_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {linear_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {linear_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {linear_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(linear_agent):,}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Bayesian model (robust universal method)\n",
    "# model_dir = \"trained_models\"\n",
    "model_dir = \"risky_navigation/trained_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, f\"linear_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "\n",
    "# Use universal save function for all agent types\n",
    "save_agent_model(linear_agent, model_path)\n",
    "\n",
    "# Also save hyperparameters for reproducibility\n",
    "hyperparams_path = os.path.join(model_dir, f\"linear_model_hyperparams_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(hyperparams_path, 'w') as f:\n",
    "    json.dump(lin_best_hyperparams, f, indent=2)\n",
    "    print(f\"Hyperparameters saved to {hyperparams_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1fb4d",
   "metadata": {},
   "source": [
    "## VAE: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db959410",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VAE: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU cache before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "vae_best_hyperparams, vae_best_score, vae_search_results = hyperparameter_search(\n",
    "    'VAE', states, actions, rewards, next_states, dones, goals,\n",
    "    STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest VAE hyperparameters:\")\n",
    "for key, value in vae_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {vae_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training VAE with best hyperparameters...\")\n",
    "vae_agent = create_agent_with_hyperparams_rl(\n",
    "    'VAE', vae_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train VAE with full epochs\n",
    "start_time = time.time()\n",
    "vae_train_losses, vae_val_losses = train_agent_rl(\n",
    "    vae_agent, states, actions, rewards, next_states, dones, goals,\n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "vae_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating VAE...\")\n",
    "start_time = time.time()\n",
    "vae_results = evaluate_agent(vae_agent, env, CONFIG['num_test_episodes'])\n",
    "vae_test_time = time.time() - start_time\n",
    "\n",
    "# Store results\n",
    "results['algorithm'].append('VAE')\n",
    "results['train_time'].append(vae_train_time)\n",
    "results['test_time'].append(vae_test_time)\n",
    "results['final_train_loss'].append(vae_train_losses[-1])\n",
    "results['final_val_loss'].append(vae_val_losses[-1])\n",
    "results['avg_mse'].append(vae_results['avg_mse'])\n",
    "results['avg_reward'].append(vae_results['avg_reward'])\n",
    "results['success_rate'].append(vae_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(vae_agent))\n",
    "results['best_hyperparams'].append(vae_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(vae_best_score)\n",
    "\n",
    "print(f\"\\nVAE Final Results:\")\n",
    "print(f\"  Best Hyperparams: {vae_best_hyperparams}\")\n",
    "print(f\"  HP Search CV Score: {vae_best_score:.6f}\")\n",
    "print(f\"  Train Time: {vae_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {vae_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {vae_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {vae_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {vae_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {vae_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(vae_agent):,}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Bayesian model (robust universal method)\n",
    "# model_dir = \"trained_models\"\n",
    "model_dir = \"risky_navigation/trained_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, f\"vae_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "\n",
    "# Use universal save function for all agent types\n",
    "save_agent_model(vae_agent, model_path)\n",
    "\n",
    "# Also save hyperparameters for reproducibility\n",
    "hyperparams_path = os.path.join(model_dir, f\"vae_model_hyperparams_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(hyperparams_path, 'w') as f:\n",
    "    json.dump(vae_best_hyperparams, f, indent=2)\n",
    "    print(f\"Hyperparameters saved to {hyperparams_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf1de7",
   "metadata": {},
   "source": [
    "## Final Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257b0ce",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning summary\n",
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all best hyperparameters and scores\n",
    "hp_summary = {\n",
    "    'Algorithm': [],\n",
    "    'Best_CV_Score': [],\n",
    "    'Best_Hyperparameters': []\n",
    "}\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('AutoEncoder')\n",
    "    hp_summary['Best_CV_Score'].append(ae_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(ae_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"AutoEncoder hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('Bayesian')\n",
    "    hp_summary['Best_CV_Score'].append(bay_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(bay_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"Bayesian hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('Transformer')\n",
    "    hp_summary['Best_CV_Score'].append(trans_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(trans_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"Transformer hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('Linear')\n",
    "    hp_summary['Best_CV_Score'].append(lin_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(lin_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"Linear hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('VAE')\n",
    "    hp_summary['Best_CV_Score'].append(vae_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(vae_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"VAE hyperparameter tuning not completed yet.\")\n",
    "\n",
    "if hp_summary['Algorithm']:\n",
    "    hp_df = pd.DataFrame(hp_summary)\n",
    "    \n",
    "    print(\"\\nBest Cross-Validation Scores from Hyperparameter Tuning:\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in hp_df.iterrows():\n",
    "        print(f\"{row['Algorithm']:>12}: {row['Best_CV_Score']:.6f}\")\n",
    "    \n",
    "    print(f\"\\nBest performing algorithm in hyperparameter search: {hp_df.loc[hp_df['Best_CV_Score'].idxmin(), 'Algorithm']}\")\n",
    "    \n",
    "    print(\"\\nDetailed Best Hyperparameters:\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in hp_df.iterrows():\n",
    "        print(f\"\\n{row['Algorithm']}:\")\n",
    "        for key, value in row['Best_Hyperparameters'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Create visualization of hyperparameter search results\n",
    "    if len(hp_summary['Algorithm']) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        bars = ax.bar(hp_df['Algorithm'], hp_df['Best_CV_Score'], \n",
    "                     color=sns.color_palette(\"husl\", len(hp_df)))\n",
    "        ax.set_title('Best Cross-Validation Scores from Hyperparameter Tuning', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('CV Score (lower is better)')\n",
    "        ax.set_xlabel('Algorithm')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, hp_df['Best_CV_Score']):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{score:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No hyperparameter tuning results available yet.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display results table\n",
    "print(\"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.round(6))\n",
    "print()\n",
    "\n",
    "# Create summary statistics\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Best Average Reward: {df_results['algorithm'][df_results['avg_reward'].idxmax()]} ({df_results['avg_reward'].max():.3f})\")\n",
    "print(f\"Lowest MSE: {df_results['algorithm'][df_results['avg_mse'].idxmin()]} ({df_results['avg_mse'].min():.6f})\")\n",
    "print(f\"Highest Success Rate: {df_results['algorithm'][df_results['success_rate'].idxmax()]} ({df_results['success_rate'].max():.3f})\")\n",
    "print(f\"Fastest Training: {df_results['algorithm'][df_results['train_time'].idxmin()]} ({df_results['train_time'].min():.2f}s)\")\n",
    "print(f\"Lowest Validation Loss: {df_results['algorithm'][df_results['final_val_loss'].idxmin()]} ({df_results['final_val_loss'].min():.6f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b4ea0",
   "metadata": {},
   "source": [
    "## Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aaa428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Algorithm Comparison Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Average Reward Comparison\n",
    "axes[0, 0].bar(df_results['algorithm'], df_results['avg_reward'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[0, 0].set_title('Average Reward per Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. MSE Comparison\n",
    "axes[0, 1].bar(df_results['algorithm'], df_results['avg_mse'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[0, 1].set_title('Average Mean Squared Error')\n",
    "axes[0, 1].set_ylabel('MSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].set_yscale('log')  # Log scale for better visibility\n",
    "\n",
    "# 3. Success Rate Comparison\n",
    "axes[0, 2].bar(df_results['algorithm'], df_results['success_rate'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[0, 2].set_title('Success Rate')\n",
    "axes[0, 2].set_ylabel('Success Rate')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "axes[0, 2].set_ylim(0, 1)\n",
    "\n",
    "# 4. Training Time Comparison\n",
    "axes[1, 0].bar(df_results['algorithm'], df_results['train_time'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[1, 0].set_title('Training Time')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Model Parameters Comparison\n",
    "axes[1, 1].bar(df_results['algorithm'], df_results['model_params'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[1, 1].set_title('Model Parameters')\n",
    "axes[1, 1].set_ylabel('Number of Parameters')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].set_yscale('log')  # Log scale for better visibility\n",
    "\n",
    "# 6. Final Validation Loss Comparison\n",
    "axes[1, 2].bar(df_results['algorithm'], df_results['final_val_loss'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[1, 2].set_title('Final Validation Loss')\n",
    "axes[1, 2].set_ylabel('Validation Loss')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ec4fc",
   "metadata": {},
   "source": [
    "## Detailed Performance Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7eee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for comprehensive comparison\n",
    "def create_radar_chart(df):\n",
    "    # Normalize metrics for radar chart (0-1 scale)\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # For metrics where lower is better (MSE, train_time, val_loss), invert them\n",
    "    normalized_df['norm_mse'] = 1 - (df['avg_mse'] - df['avg_mse'].min()) / (df['avg_mse'].max() - df['avg_mse'].min())\n",
    "    normalized_df['norm_train_time'] = 1 - (df['train_time'] - df['train_time'].min()) / (df['train_time'].max() - df['train_time'].min())\n",
    "    normalized_df['norm_val_loss'] = 1 - (df['final_val_loss'] - df['final_val_loss'].min()) / (df['final_val_loss'].max() - df['final_val_loss'].min())\n",
    "    \n",
    "    # For metrics where higher is better, normalize directly\n",
    "    normalized_df['norm_reward'] = (df['avg_reward'] - df['avg_reward'].min()) / (df['avg_reward'].max() - df['avg_reward'].min())\n",
    "    normalized_df['norm_success'] = df['success_rate']  # Already 0-1\n",
    "    \n",
    "    # Parameters normalized (smaller models get higher scores)\n",
    "    normalized_df['norm_params'] = 1 - (df['model_params'] - df['model_params'].min()) / (df['model_params'].max() - df['model_params'].min())\n",
    "    \n",
    "    # Metrics for radar chart\n",
    "    metrics = ['norm_reward', 'norm_mse', 'norm_success', 'norm_train_time', 'norm_val_loss', 'norm_params']\n",
    "    metric_labels = ['Reward', 'MSE (inv)', 'Success Rate', 'Train Time (inv)', 'Val Loss (inv)', 'Model Size (inv)']\n",
    "    \n",
    "    # Number of metrics\n",
    "    N = len(metrics)\n",
    "    \n",
    "    # Compute angles for each metric\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = sns.color_palette(\"husl\", len(normalized_df))\n",
    "    \n",
    "    for i, (idx, row) in enumerate(normalized_df.iterrows()):\n",
    "        values = [row[metric] for metric in metrics]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['algorithm'], color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Algorithm Performance Radar Chart\\n(Higher values = better performance)', size=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_radar_chart(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a7e67",
   "metadata": {},
   "source": [
    "## Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173589c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV with hyperparameter information\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"algorithm_comparison_results_{timestamp}.csv\"\n",
    "\n",
    "# Create a more detailed DataFrame for export\n",
    "export_df = pd.DataFrame(results)\n",
    "\n",
    "# Convert hyperparameters to string format for CSV\n",
    "export_df['best_hyperparams_str'] = export_df['best_hyperparams'].apply(lambda x: str(x) if x else 'N/A')\n",
    "\n",
    "# Save main results\n",
    "export_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to: {csv_filename}\")\n",
    "\n",
    "# Create a detailed summary report\n",
    "summary_report = f\"\"\"\n",
    "ALGORITHM COMPARISON SUMMARY REPORT (WITH HYPERPARAMETER TUNING)\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "{'='*80}\n",
    "\n",
    "EXPERIMENTAL SETUP:\n",
    "- Number of training episodes: {CONFIG['num_episodes']}\n",
    "- Training epochs per algorithm: {CONFIG['num_epochs']}\n",
    "- Batch size: {CONFIG['batch_size']}\n",
    "- Test episodes: {CONFIG['num_test_episodes']}\n",
    "- Device: {CONFIG['device']}\n",
    "\n",
    "HYPERPARAMETER TUNING SETUP:\n",
    "- Search method: {CONFIG['search_method']}\n",
    "- Number of trials per algorithm: {CONFIG['n_trials']}\n",
    "- Cross-validation folds: {CONFIG['cv_folds']}\n",
    "- Early stopping patience: {CONFIG['patience']}\n",
    "- HP search epochs: {CONFIG['hp_epochs']}\n",
    "\n",
    "RESULTS RANKING:\n",
    "\n",
    "1. Best Overall Performance (Average Reward):\n",
    "   {export_df.loc[export_df['avg_reward'].idxmax(), 'algorithm']} - {export_df['avg_reward'].max():.3f}\n",
    "   Best hyperparams: {export_df.loc[export_df['avg_reward'].idxmax(), 'best_hyperparams']}\n",
    "\n",
    "2. Most Accurate Predictions (Lowest MSE):\n",
    "   {export_df.loc[export_df['avg_mse'].idxmin(), 'algorithm']} - {export_df['avg_mse'].min():.6f}\n",
    "   Best hyperparams: {export_df.loc[export_df['avg_mse'].idxmin(), 'best_hyperparams']}\n",
    "\n",
    "3. Highest Success Rate:\n",
    "   {export_df.loc[export_df['success_rate'].idxmax(), 'algorithm']} - {export_df['success_rate'].max():.3f}\n",
    "   Best hyperparams: {export_df.loc[export_df['success_rate'].idxmax(), 'best_hyperparams']}\n",
    "\n",
    "4. Fastest Training:\n",
    "   {export_df.loc[export_df['train_time'].idxmin(), 'algorithm']} - {export_df['train_time'].min():.2f}s\n",
    "   Best hyperparams: {export_df.loc[export_df['train_time'].idxmin(), 'best_hyperparams']}\n",
    "\n",
    "5. Best Validation Performance:\n",
    "   {export_df.loc[export_df['final_val_loss'].idxmin(), 'algorithm']} - {export_df['final_val_loss'].min():.6f}\n",
    "   Best hyperparams: {export_df.loc[export_df['final_val_loss'].idxmin(), 'best_hyperparams']}\n",
    "\n",
    "6. Best Cross-Validation Score (from hyperparameter search):\n",
    "   {export_df.loc[export_df['hp_search_cv_score'].idxmin(), 'algorithm']} - {export_df['hp_search_cv_score'].min():.6f}\n",
    "\n",
    "DETAILED RESULTS:\n",
    "{export_df[['algorithm', 'avg_reward', 'avg_mse', 'success_rate', 'train_time', 'hp_search_cv_score']].to_string(index=False)}\n",
    "\n",
    "HYPERPARAMETER DETAILS:\n",
    "\"\"\"\n",
    "\n",
    "for _, row in export_df.iterrows():\n",
    "    summary_report += f\"\"\"\n",
    "{row['algorithm']}:\n",
    "  Best hyperparameters: {row['best_hyperparams']}\n",
    "  HP search CV score: {row['hp_search_cv_score']:.6f}\n",
    "  Final validation loss: {row['final_val_loss']:.6f}\n",
    "  Model parameters: {row['model_params']:,}\n",
    "\"\"\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "- For real-time applications: Choose the algorithm with fastest training/inference\n",
    "- For accuracy-critical tasks: Choose the algorithm with lowest MSE\n",
    "- For exploration tasks: Choose the algorithm with highest success rate\n",
    "- For resource-constrained environments: Choose the algorithm with fewest parameters\n",
    "- For robust performance: Consider the algorithm with best cross-validation score\n",
    "\n",
    "METHODOLOGY NOTES:\n",
    "- All algorithms underwent {CONFIG['cv_folds']}-fold cross-validation hyperparameter tuning\n",
    "- Best hyperparameters were selected based on validation loss minimization\n",
    "- Final models were trained with full epochs using the best hyperparameters\n",
    "- This ensures fair comparison and optimal performance for each algorithm\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "summary_filename = f\"algorithm_comparison_summary_{timestamp}.txt\"\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"Summary report saved to: {summary_filename}\")\n",
    "print(\"\\nSummary Report:\")\n",
    "print(summary_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
