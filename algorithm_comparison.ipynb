{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578ac5ae",
   "metadata": {},
   "source": [
    "# Algorithm Comparison for Behavioral Cloning\n",
    "\n",
    "This notebook provides a comparison of different neural network architectures for behavioral cloning in the risky navigation environment.\n",
    "\n",
    "**Algorithms Evaluated:**\n",
    "- **Linear**: Simple linear regression baseline\n",
    "- **AutoEncoder**: Neural network encoder-decoder architecture  \n",
    "- **Bayesian**: Bayesian neural network with uncertainty quantification\n",
    "- **Transformer**: Self-attention based model\n",
    "- **VAE**: Variational AutoEncoder with probabilistic latent representations\n",
    "\n",
    "**Workflow:**\n",
    "1. **Data Collection**: Load expert demonstrations from optimal visibility graph policy\n",
    "2. **Model Training**: Train each algorithm with simple, transparent training loop\n",
    "3. **Evaluation**: Test models in environment and compare performance\n",
    "4. **Analysis**: Visualize results and compare metrics\n",
    "\n",
    "This simplified approach prioritizes debuggability and clarity over automated hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077109",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn torch torchvision torchaudio gymnasium tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2cea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c632d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "Device available: CUDA\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/risky_navigation')\n",
    "\n",
    "from src.env.continuous_nav_env import ContinuousNavigationEnv\n",
    "from src.algorithms.AutoEncoder.agent import AutoEncoderAgent\n",
    "from src.algorithms.Bayesian.agent import BayesianAgent\n",
    "from src.algorithms.Transformer.agent import TransformerAgent\n",
    "from src.algorithms.Linear.agent import LinearAgent\n",
    "from src.algorithms.VAE.agent import VAEAgent\n",
    "from src.utils.file_management import save_pickle, load_pickle\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817ec8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a17c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 256\n",
      "Training epochs: 100\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'num_episodes': 1000,\n",
    "    'max_steps': 200,\n",
    "    'batch_size': 256,\n",
    "    'num_epochs': 100,\n",
    "    'val_ratio': 0.2,\n",
    "    'num_test_episodes': 50,\n",
    "    'lr': 1e-3,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "}\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'AutoEncoder': {'latent_dim': 32, 'hidden_dims': [128, 64]},\n",
    "    'Linear': {},\n",
    "    'Transformer': {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1},\n",
    "    'Bayesian': {'hidden_dim': 128, 'prior_std': 1.0},\n",
    "    'VAE': {'latent_dim': 32, 'hidden_dim': 128, 'beta': 1.0}\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Training epochs: {CONFIG['num_epochs']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55f165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU optimizations enabled for RTX 4090\n",
      "Batch size: 256\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU optimizations enabled for RTX 4090\")\n",
    "    print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Running on CPU will be very slow!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622191",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2b379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined!\n"
     ]
    }
   ],
   "source": [
    "def train_agent_simple(agent, train_states, train_expert_actions, train_goals,\n",
    "                       val_states, val_expert_actions, val_goals, \n",
    "                       num_epochs=100, batch_size=256, device='cpu', verbose=True):\n",
    "    \"\"\"Simple training loop with validation tracking.\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    n_train = len(train_states)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        indices = np.random.permutation(n_train)\n",
    "        \n",
    "        for start_idx in range(0, n_train, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_train)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            batch_states = torch.tensor(train_states[batch_indices], dtype=torch.float32, device=device)\n",
    "            batch_actions = torch.tensor(train_expert_actions[batch_indices], dtype=torch.float32, device=device)\n",
    "            \n",
    "            loss = agent.train_step(batch_states, None, None, batch_actions)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_states_t = torch.tensor(val_states, dtype=torch.float32, device=device)\n",
    "        val_actions_t = torch.tensor(val_expert_actions, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if hasattr(agent, 'model'):\n",
    "                agent.model.eval()\n",
    "                predictions = agent.model(val_states_t)\n",
    "            elif hasattr(agent, 'encoder'):  # VAE\n",
    "                agent.encoder.eval()\n",
    "                agent.decoder.eval()\n",
    "                mu, _ = agent.encoder(val_states_t)\n",
    "                predictions = agent.decoder(mu)\n",
    "            else:\n",
    "                predictions = agent.predict_action(val_states_t, None)\n",
    "            \n",
    "            val_loss = torch.nn.functional.mse_loss(predictions, val_actions_t).item()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"Epoch {epoch:3d}/{num_epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "            \n",
    "            if np.isnan(avg_train_loss) or np.isnan(val_loss):\n",
    "                print(f\"WARNING: NaN detected at epoch {epoch}!\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"Training function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652df29",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection function defined!\n"
     ]
    }
   ],
   "source": [
    "def collect_rl_experience(env, num_episodes=100, max_steps=200):\n",
    "    \"\"\"Collect RL training data using optimal policy from visibility graph.\"\"\"\n",
    "    data = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes), desc='Collecting RL experience'):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            current_pos = state[:2]\n",
    "            current_theta = state[2]\n",
    "            dist_to_goal = np.linalg.norm(current_pos - goal)\n",
    "            \n",
    "            try:\n",
    "                if hasattr(env, 'vgraph'):\n",
    "                    path = env.vgraph.shortest_path(current_pos, goal)\n",
    "                    \n",
    "                    if len(path) > 1:\n",
    "                        next_waypoint = path[1]\n",
    "                        direction = next_waypoint - current_pos\n",
    "                        desired_theta = np.arctan2(direction[1], direction[0])\n",
    "                        angle_diff = desired_theta - current_theta\n",
    "                        angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "                        steering = np.clip(angle_diff, env.action_space.low[1], env.action_space.high[1])\n",
    "                        \n",
    "                        # FIXED: Slow down near goal to satisfy velocity constraint\n",
    "                        if dist_to_goal < env.goal_radius * 2:\n",
    "                            # Slow down when close to goal\n",
    "                            throttle = env.action_space.high[0] * 0.2  # 20% throttle near goal\n",
    "                        else:\n",
    "                            throttle = env.action_space.high[0]  # Full throttle otherwise\n",
    "                        \n",
    "                        action = np.array([throttle, steering])\n",
    "                    else:\n",
    "                        # At goal, stop\n",
    "                        action = np.array([0.0, 0.0])\n",
    "                else:\n",
    "                    direction = goal - current_pos\n",
    "                    desired_theta = np.arctan2(direction[1], direction[0])\n",
    "                    angle_diff = desired_theta - current_theta\n",
    "                    angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "                    steering = np.clip(angle_diff, env.action_space.low[1], env.action_space.high[1])\n",
    "                    \n",
    "                    # FIXED: Slow down near goal\n",
    "                    if dist_to_goal < env.goal_radius * 2:\n",
    "                        throttle = env.action_space.high[0] * 0.2\n",
    "                    else:\n",
    "                        throttle = env.action_space.high[0]\n",
    "                    \n",
    "                    action = np.array([throttle, steering])\n",
    "                    \n",
    "            except Exception:\n",
    "                direction = goal - current_pos\n",
    "                desired_theta = np.arctan2(direction[1], direction[0])\n",
    "                angle_diff = desired_theta - current_theta\n",
    "                angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "                steering = np.clip(angle_diff, env.action_space.low[1], env.action_space.high[1])\n",
    "                \n",
    "                # FIXED: Slow down near goal\n",
    "                if dist_to_goal < env.goal_radius * 2:\n",
    "                    throttle = env.action_space.high[0] * 0.2\n",
    "                else:\n",
    "                    throttle = env.action_space.high[0]\n",
    "                \n",
    "                action = np.array([throttle, steering])\n",
    "            \n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            data.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action.copy(),\n",
    "                'reward': reward,\n",
    "                'next_state': next_state.copy(),\n",
    "                'done': done,\n",
    "                'goal': goal.copy()\n",
    "            })\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    successful_episodes += 1\n",
    "                break\n",
    "    \n",
    "    print(f\"Collected {len(data)} transitions from {num_episodes} episodes\")\n",
    "    print(f\"Success rate: {successful_episodes/num_episodes:.2%}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"Data collection function defined!\")\n",
    "print(\"FIXED: Expert now slows down near goal to satisfy velocity constraint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a347d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting RL experience: 100%|██████████| 1000/1000 [00:04<00:00, 202.75it/s]\n",
      "Collecting RL experience: 100%|██████████| 1000/1000 [00:04<00:00, 202.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 56010 transitions from 1000 episodes\n",
      "Success rate: 0.00%\n",
      "✓ Saved dataset to rl_experience_dataset.pickle\n",
      "\n",
      "Data extracted:\n",
      "  States shape: (56010, 8)\n",
      "  Actions shape: (56010, 2)\n",
      "  Goals shape: (56010, 2)\n",
      "  Rewards shape: (56010,)\n",
      "✓ Saved dataset to rl_experience_dataset.pickle\n",
      "\n",
      "Data extracted:\n",
      "  States shape: (56010, 8)\n",
      "  Actions shape: (56010, 2)\n",
      "  Goals shape: (56010, 2)\n",
      "  Rewards shape: (56010,)\n"
     ]
    }
   ],
   "source": [
    "# Load or collect data\n",
    "dataset_path = 'rl_experience_dataset.pickle'\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Deleting old dataset (0% success rate)...\")\n",
    "    os.remove(dataset_path)\n",
    "\n",
    "print(f\"Collecting new dataset with fixed expert policy...\")\n",
    "env_collect = ContinuousNavigationEnv()\n",
    "data = collect_rl_experience(env_collect, num_episodes=CONFIG['num_episodes'], max_steps=CONFIG['max_steps'])\n",
    "save_pickle(data, dataset_path)\n",
    "print(f\"✓ Saved dataset to {dataset_path}\")\n",
    "\n",
    "# Extract states, actions, and goals from data\n",
    "states = np.array([d['state'] for d in data])\n",
    "actions = np.array([d['action'] for d in data])\n",
    "next_states = np.array([d['next_state'] for d in data])\n",
    "rewards = np.array([d['reward'] for d in data])\n",
    "dones = np.array([d['done'] for d in data])\n",
    "goals = np.array([d['goal'] for d in data])\n",
    "\n",
    "print(f\"\\nData extracted:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Actions shape: {actions.shape}\")\n",
    "print(f\"  Goals shape: {goals.shape}\")\n",
    "print(f\"  Rewards shape: {rewards.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions from the collected data\n",
    "STATE_DIM = states.shape[1]\n",
    "ACTION_DIM = actions.shape[1]\n",
    "GOAL_DIM = goals.shape[1]\n",
    "\n",
    "print(f\"Data dimensions:\")\n",
    "print(f\"  STATE_DIM = {STATE_DIM}\")\n",
    "print(f\"  ACTION_DIM = {ACTION_DIM}\")\n",
    "print(f\"  GOAL_DIM = {GOAL_DIM}\")\n",
    "print(f\"  Total samples = {len(states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment for evaluation\n",
    "env = ContinuousNavigationEnv()\n",
    "print(f\"Environment initialized: {env}\")\n",
    "print(f\"  State space: {env.observation_space.shape}\")\n",
    "print(f\"  Action space: {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49cc82",
   "metadata": {},
   "source": [
    "## Train All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING ALL ALGORITHMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data\n",
    "n_samples = len(states)\n",
    "n_train = int(n_samples * (1 - CONFIG['val_ratio']))\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "train_states = states[train_indices]\n",
    "train_actions = actions[train_indices]\n",
    "train_goals = goals[train_indices]\n",
    "\n",
    "val_states = states[val_indices]\n",
    "val_actions = actions[val_indices]\n",
    "val_goals = goals[val_indices]\n",
    "\n",
    "print(f\"Data split: {len(train_states)} train, {len(val_states)} val\\n\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "algorithms_to_train = {\n",
    "    'Linear': (LinearAgent, {}),\n",
    "    'AutoEncoder': (AutoEncoderAgent, MODEL_CONFIGS['AutoEncoder']),\n",
    "    'Transformer': (TransformerAgent, MODEL_CONFIGS['Transformer']),\n",
    "    'Bayesian': (BayesianAgent, MODEL_CONFIGS['Bayesian']),\n",
    "    'VAE': (VAEAgent, MODEL_CONFIGS['VAE'])\n",
    "}\n",
    "\n",
    "for algo_name, (AgentClass, model_config) in algorithms_to_train.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    agent = AgentClass(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        lr=CONFIG['lr'],\n",
    "        device=CONFIG['device'],\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_losses, val_losses = train_agent_simple(\n",
    "        agent, \n",
    "        train_states, train_actions, train_goals,\n",
    "        val_states, val_actions, val_goals,\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        device=CONFIG['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    all_results[algo_name] = {\n",
    "        'agent': agent,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{algo_name} Complete! Time: {train_time:.2f}s\")\n",
    "    print(f\"Final train loss: {train_losses[-1]:.6f}, Val loss: {val_losses[-1]:.6f}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for algo_name, results in all_results.items():\n",
    "    epochs = range(len(results['train_losses']))\n",
    "    axes[0].plot(epochs, results['train_losses'], label=algo_name, alpha=0.7)\n",
    "    axes[1].plot(epochs, results['val_losses'], label=algo_name, alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Loss Summary:\")\n",
    "print(f\"{'Algorithm':<15} {'Train Loss':<15} {'Val Loss':<15} {'Train Time (s)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for algo_name, results in all_results.items():\n",
    "    print(f\"{algo_name:<15} {results['train_losses'][-1]:<15.6f} {results['val_losses'][-1]:<15.6f} {results['train_time']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cee6b",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agents\n",
    "\n",
    "Now test the trained agents in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING ALL AGENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_agent_simple(agent, env, num_episodes=50, max_steps=200):\n",
    "    \"\"\"Evaluate agent in environment.\"\"\"\n",
    "    results = {'rewards': [], 'successes': [], 'steps': []}\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device=CONFIG['device'])\n",
    "            action = agent.predict_action(state_t, None)\n",
    "            \n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.cpu().numpy()\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        results['rewards'].append(episode_reward)\n",
    "        success = info.get('reason', '') == 'goal_reached' if done else False\n",
    "        results['successes'].append(1 if success else 0)\n",
    "        results['steps'].append(step + 1)\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(results['rewards']),\n",
    "        'std_reward': np.std(results['rewards']),\n",
    "        'success_rate': np.mean(results['successes']),\n",
    "        'avg_steps': np.mean(results['steps'])\n",
    "    }\n",
    "\n",
    "eval_results = {}\n",
    "\n",
    "for algo_name, results_dict in all_results.items():\n",
    "    print(f\"\\nEvaluating {algo_name}...\")\n",
    "    agent = results_dict['agent']\n",
    "    \n",
    "    eval_res = evaluate_agent_simple(\n",
    "        agent, env, \n",
    "        num_episodes=CONFIG['num_test_episodes'],\n",
    "        max_steps=CONFIG['max_steps']\n",
    "    )\n",
    "    \n",
    "    eval_results[algo_name] = eval_res\n",
    "    \n",
    "    print(f\"  Avg Reward: {eval_res['avg_reward']:.3f} ± {eval_res['std_reward']:.3f}\")\n",
    "    print(f\"  Success Rate: {eval_res['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Steps: {eval_res['avg_steps']:.1f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "import pandas as pd\n",
    "\n",
    "results_data = []\n",
    "for algo_name in all_results.keys():\n",
    "    train_res = all_results[algo_name]\n",
    "    eval_res = eval_results[algo_name]\n",
    "    \n",
    "    results_data.append({\n",
    "        'Algorithm': algo_name,\n",
    "        'Train Loss': train_res['train_losses'][-1],\n",
    "        'Val Loss': train_res['val_losses'][-1],\n",
    "        'Train Time (s)': train_res['train_time'],\n",
    "        'Avg Reward': eval_res['avg_reward'],\n",
    "        'Success Rate': eval_res['success_rate'],\n",
    "        'Avg Steps': eval_res['avg_steps']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "df_results = df_results.sort_values('Val Loss')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Success Rate\n",
    "axes[0].bar(df_results['Algorithm'], df_results['Success Rate'])\n",
    "axes[0].set_ylabel('Success Rate')\n",
    "axes[0].set_title('Success Rate by Algorithm')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average Reward\n",
    "axes[1].bar(df_results['Algorithm'], df_results['Avg Reward'])\n",
    "axes[1].set_ylabel('Average Reward')\n",
    "axes[1].set_title('Average Reward by Algorithm')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "axes[2].bar(df_results['Algorithm'], df_results['Val Loss'])\n",
    "axes[2].set_ylabel('Validation Loss')\n",
    "axes[2].set_title('Validation Loss by Algorithm')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be17733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEBUGGING: Action and State Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "agent = all_results['Linear']['agent']\n",
    "state = env.reset()\n",
    "goal = env.goal.copy()\n",
    "\n",
    "print(f\"\\nInitial State: {state}\")\n",
    "print(f\"Goal Position: {goal}\")\n",
    "print(f\"Distance to Goal: {np.linalg.norm(state[:2] - goal):.3f}\")\n",
    "print(f\"\\nAction Space: [{env.action_space.low}, {env.action_space.high}]\")\n",
    "\n",
    "print(f\"\\n{'Step':<6} {'Action':<20} {'State[:2]':<20} {'Distance':<10} {'Reward':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in range(5):\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=CONFIG['device'])\n",
    "    action = agent.predict_action(state_t, None)\n",
    "    \n",
    "    if isinstance(action, torch.Tensor):\n",
    "        action = action.cpu().numpy()\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    distance = np.linalg.norm(next_state[:2] - goal)\n",
    "    \n",
    "    print(f\"{step:<6} {str(action):<20} {str(next_state[:2]):<20} {distance:<10.3f} {reward:<8.3f}\")\n",
    "    \n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(f\"\\nEpisode ended: {info.get('reason', 'unknown')}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training Data Statistics:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Expert actions - Mean: {actions.mean(axis=0)}\")\n",
    "print(f\"Expert actions - Std:  {actions.std(axis=0)}\")\n",
    "print(f\"Expert actions - Min:  {actions.min(axis=0)}\")\n",
    "print(f\"Expert actions - Max:  {actions.max(axis=0)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
