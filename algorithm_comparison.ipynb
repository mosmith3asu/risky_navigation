{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578ac5ae",
   "metadata": {},
   "source": [
    "# Operator Action Prediction for Teleoperation with Communication Delays\n",
    "\n",
    "Comparing neural network architectures for predicting operator actions during communication delays in space teleoperation.\n",
    "\n",
    "## Problem\n",
    "\n",
    "Space teleoperation faces critical communication delays (Mars: 22 min, Moon: 1.25 sec). Operators cannot respond to obstacles in real-time.\n",
    "\n",
    "## Approach\n",
    "\n",
    "Learn models that predict operator actions based on current state, previous action, and goal position.\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "1. **Linear** - Simple baseline\n",
    "2. **Bayesian** - Uncertainty-aware predictions\n",
    "3. **VAE** - Latent action distributions\n",
    "4. **Transformer** - Temporal sequence modeling\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Expert demonstrations from visibility graph policy\n",
    "2. Train models with grid search\n",
    "3. Evaluate prediction accuracy and success rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077109",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec3f6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (0.62.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba) (0.45.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba) (0.45.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn torch torchvision torchaudio gymnasium tqdm shapely numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d615b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'risky_navigation'...\n",
      "remote: Enumerating objects: 793, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Enumerating objects: 793, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 793 (delta 0), reused 1 (delta 0), pack-reused 790 (from 1)\u001b[K\n",
      "Receiving objects: 100% (793/793), 19.89 MiB | 33.18 MiB/s, done.\n",
      "remote: Total 793 (delta 0), reused 1 (delta 0), pack-reused 790 (from 1)\u001b[K\n",
      "Receiving objects: 100% (793/793), 19.89 MiB | 33.18 MiB/s, done.\n",
      "Resolving deltas: 100% (440/440), done.\n",
      "Resolving deltas: 100% (440/440), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf risky_navigation\n",
    "!git clone https://github.com/mosmith3asu/risky_navigation.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93444736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGC-DL-CONTAINER-LICENSE  jupyter.log\t     proc\t\t start.sh\n",
      "bin\t\t\t  lib\t\t     risky_navigation\t sys\n",
      "bin.usr-is-merged\t  lib.usr-is-merged  root\t\t tmp\n",
      "boot\t\t\t  lib64\t\t     run\t\t usr\n",
      "dev\t\t\t  media\t\t     sbin\t\t var\n",
      "etc\t\t\t  mnt\t\t     sbin.usr-is-merged  workspace\n",
      "home\t\t\t  opt\t\t     srv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe2cea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c632d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "Device available: CUDA\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./risky_navigation')\n",
    "\n",
    "from src.env.continuous_nav_env import ContinuousNavigationEnv\n",
    "from src.env.layouts import read_layout_dict\n",
    "from src.algorithms.Bayesian.agent import BayesianAgent\n",
    "from src.algorithms.Transformer.agent import TransformerAgent\n",
    "from src.algorithms.Linear.agent import LinearAgent\n",
    "from src.algorithms.VAE.agent import VAEAgent\n",
    "from src.utils.file_management import save_pickle, load_pickle\n",
    "from src.utils.visibility_graph import VisibilityGraph\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817ec8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69a17c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 512 (optimized for RTX 4090)\n",
      "Max epochs: 200 (with early stopping)\n",
      "Grid search enabled for: ['Transformer', 'Bayesian', 'VAE', 'Linear']\n",
      "\n",
      "All algorithms will test sequence lengths: [1, 5, 10]\n",
      "  - sequence_len=1: Single previous action (baseline)\n",
      "  - sequence_len>1: Temporal sequences (history-based prediction)\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    'num_episodes': 1000,\n",
    "    'max_steps': 200,\n",
    "    'batch_size': 512,  # Increased for RTX 4090\n",
    "    'num_epochs': 200,\n",
    "    'val_ratio': 0.2,\n",
    "    'num_test_episodes': 50,\n",
    "    'lr': 1e-3,\n",
    "    'device': torch.device('cuda'),\n",
    "    'num_workers': 4,  # Parallel data loading\n",
    "    'prefetch_factor': 2,\n",
    "}\n",
    "\n",
    "# Grid search configurations for each algorithm\n",
    "GRID_SEARCH_CONFIGS = {\n",
    "    'Transformer': {\n",
    "        'd_model': [32, 64, 128],\n",
    "        'nhead': [4, 8],\n",
    "        'num_layers': [2, 3, 4],\n",
    "        'dropout': [0.0, 0.1, 0.2],\n",
    "        'sequence_len': [1, 5, 10],  # Temporal sequence length\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    },\n",
    "    'Bayesian': {\n",
    "        'hidden_dim': [64, 128, 256],\n",
    "        'prior_std': [0.5, 1.0, 2.0],\n",
    "        'kl_weight': [1e-5, 1e-4, 1e-3],\n",
    "        'sequence_len': [1, 5, 10],  # Added temporal sequences\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    },\n",
    "    'VAE': {\n",
    "        'latent_dim': [16, 32, 64],\n",
    "        'hidden_dim': [64, 128, 256],\n",
    "        'beta': [0.5, 1.0, 2.0],\n",
    "        'sequence_len': [1, 5, 10],  # Added temporal sequences\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    },\n",
    "    'Linear': {\n",
    "        'sequence_len': [1, 5, 10],  # Added temporal sequences\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Best baseline configs (for quick comparison)\n",
    "MODEL_CONFIGS = {\n",
    "    'Linear': {'sequence_len': 5},\n",
    "    'Transformer': {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1, 'sequence_len': 10},\n",
    "    'Bayesian': {'hidden_dim': 128, 'prior_std': 1.0, 'kl_weight': 1e-5, 'sequence_len': 5},\n",
    "    'VAE': {'latent_dim': 32, 'hidden_dim': 128, 'beta': 1.0, 'sequence_len': 5}\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']} (optimized for RTX 4090)\")\n",
    "print(f\"Max epochs: {CONFIG['num_epochs']} (with early stopping)\")\n",
    "print(f\"Grid search enabled for: {list(GRID_SEARCH_CONFIGS.keys())}\")\n",
    "print(f\"\\nAll algorithms will test sequence lengths: {GRID_SEARCH_CONFIGS['Linear']['sequence_len']}\")\n",
    "print(f\"  - sequence_len=1: Single previous action (baseline)\")\n",
    "print(f\"  - sequence_len>1: Temporal sequences (history-based prediction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a55f165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU OPTIMIZATIONS ENABLED FOR RTX 4090\n",
      "============================================================\n",
      "✓ Batch size: 512\n",
      "✓ TF32 matmul: Enabled\n",
      "✓ cuDNN benchmark: Enabled\n",
      "✓ Mixed precision (AMP): Enabled\n",
      "✓ CUDA version: 12.8\n",
      "✓ GPU: NVIDIA GeForce RTX 4090\n",
      "✓ VRAM: 23.5 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # RTX 4090 optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "    # Enable cuDNN autotuner for optimal convolution algorithms\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # Memory optimizations\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Mixed precision training setup\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    use_amp = True\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"GPU OPTIMIZATIONS ENABLED FOR RTX 4090\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✓ Batch size: {CONFIG['batch_size']}\")\n",
    "    print(f\"✓ TF32 matmul: Enabled\")\n",
    "    print(f\"✓ cuDNN benchmark: Enabled\")\n",
    "    print(f\"✓ Mixed precision (AMP): Enabled\")\n",
    "    print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✓ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    use_amp = False\n",
    "    scaler = None\n",
    "    print(\"WARNING: CUDA not available. Running on CPU will be very slow!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622191",
   "metadata": {},
   "source": [
    "## Training Functions with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a2b379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined!\n"
     ]
    }
   ],
   "source": [
    "def train_agent_optimized(agent, train_state_seqs, train_action_seqs, train_target_actions, train_goals,\n",
    "                          val_state_seqs, val_action_seqs, val_target_actions, val_goals, \n",
    "                          num_epochs=50, batch_size=64, patience=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Clean training function without fallback mechanisms.\n",
    "    All agents must implement train_step() and predict_action() methods.\n",
    "    \"\"\"\n",
    "    device = CONFIG['device']\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(agent.optimizer, 'min', patience=5, factor=0.5)\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        indices = torch.randperm(len(train_state_seqs))\n",
    "        for i in range(0, len(train_state_seqs), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            batch_state_seqs = torch.tensor(train_state_seqs[batch_idx], dtype=torch.float32, device=device)\n",
    "            batch_action_seqs = torch.tensor(train_action_seqs[batch_idx], dtype=torch.float32, device=device)\n",
    "            batch_target_actions = torch.tensor(train_target_actions[batch_idx], dtype=torch.float32, device=device)\n",
    "            batch_goals = torch.tensor(train_goals[batch_idx], dtype=torch.float32, device=device)\n",
    "            \n",
    "            loss = agent.train_step(batch_state_seqs, batch_action_seqs, batch_goals, batch_target_actions)\n",
    "            if isinstance(loss, torch.Tensor):\n",
    "                loss = loss.item()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase - use agent's predict_action method\n",
    "        val_state_seqs_t = torch.tensor(val_state_seqs, dtype=torch.float32, device=device)\n",
    "        val_action_seqs_t = torch.tensor(val_action_seqs, dtype=torch.float32, device=device)\n",
    "        val_target_actions_t = torch.tensor(val_target_actions, dtype=torch.float32, device=device)\n",
    "        val_goals_t = torch.tensor(val_goals, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Use agent's predict_action method (all agents implement this)\n",
    "            predictions = agent.predict_action(val_state_seqs_t, val_action_seqs_t, val_goals_t)\n",
    "            val_loss = torch.nn.functional.mse_loss(predictions, val_target_actions_t).item()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping with model checkpointing\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save best model state\n",
    "            agent.save(f'temp_best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stop at epoch {epoch}. Best val: {best_val_loss:.6f}\")\n",
    "                # Load best model\n",
    "                agent.load(f'temp_best_model.pth')\n",
    "                break\n",
    "        \n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Train={avg_train_loss:.6f}, Val={val_loss:.6f}\")\n",
    "    \n",
    "    # Clean up temp file\n",
    "    import os\n",
    "    if os.path.exists('temp_best_model.pth'):\n",
    "        os.remove('temp_best_model.pth')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"Training function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652df29",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e7f97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection function defined!\n"
     ]
    }
   ],
   "source": [
    "def collect_rl_experience(env, vgraph, num_episodes=100, max_steps=200, sequence_len=1):\n",
    "    \"\"\"\n",
    "    Collect expert demonstrations and create temporal sequences.\n",
    "    \n",
    "    Args:\n",
    "        env: Environment instance\n",
    "        vgraph: VisibilityGraph for expert policy\n",
    "        num_episodes: Number of episodes\n",
    "        max_steps: Max steps per episode\n",
    "        sequence_len: Length of temporal sequences\n",
    "    \n",
    "    Returns:\n",
    "        list: Samples with state_sequences, action_sequences, target_action, goal\n",
    "    \"\"\"\n",
    "    episodes = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes), desc='Collecting experience'):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy()\n",
    "        prev_action = np.zeros(2)\n",
    "        episode_data = []\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            current_pos = state[:2]\n",
    "            current_theta = state[2]\n",
    "            \n",
    "            # Fix: Use vgraph.shortest_path() instead of calling vgraph()\n",
    "            path = vgraph.shortest_path(current_pos, goal)\n",
    "            if path is not None and len(path) > 1:\n",
    "                target = np.array(path[1])\n",
    "            else:\n",
    "                target = goal\n",
    "            \n",
    "            direction = target - current_pos\n",
    "            desired_theta = np.arctan2(direction[1], direction[0])\n",
    "            angle_diff = desired_theta - current_theta\n",
    "            angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "            \n",
    "            steering = np.clip(angle_diff * 2.0, env.action_space.low[1], env.action_space.high[1])\n",
    "            dist_to_target = np.linalg.norm(direction)\n",
    "            \n",
    "            if dist_to_target < env.goal_radius * 3:\n",
    "                throttle = env.action_space.high[0] * 0.3\n",
    "            else:\n",
    "                throttle = env.action_space.high[0] * 0.8\n",
    "            \n",
    "            action = np.array([throttle, steering])\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_data.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action.copy(),\n",
    "            })\n",
    "            \n",
    "            prev_action = action\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    successful_episodes += 1\n",
    "                break\n",
    "        \n",
    "        if len(episode_data) > 0:\n",
    "            episodes.append({'transitions': episode_data, 'goal': goal})\n",
    "    \n",
    "    # Create sequences from episodes\n",
    "    data = []\n",
    "    for episode in episodes:\n",
    "        transitions = episode['transitions']\n",
    "        goal = episode['goal']\n",
    "        \n",
    "        for i in range(len(transitions)):\n",
    "            # Get sequence history\n",
    "            start_idx = max(0, i - sequence_len + 1)\n",
    "            seq_transitions = transitions[start_idx:i+1]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            while len(seq_transitions) < sequence_len:\n",
    "                seq_transitions.insert(0, {'state': np.zeros_like(transitions[0]['state']), \n",
    "                                           'action': np.zeros_like(transitions[0]['action'])})\n",
    "            \n",
    "            state_seq = np.array([t['state'] for t in seq_transitions[:-1]] + [transitions[i]['state']])\n",
    "            action_seq = np.array([t['action'] for t in seq_transitions[:-1]] + [np.zeros_like(transitions[i]['action'])])\n",
    "            target_action = transitions[i]['action']\n",
    "            \n",
    "            data.append({\n",
    "                'state_sequences': state_seq,\n",
    "                'action_sequences': action_seq,\n",
    "                'target_action': target_action,\n",
    "                'goal': goal,\n",
    "            })\n",
    "    \n",
    "    print(f\"Collected {len(data)} sequence samples from {num_episodes} episodes\")\n",
    "    print(f\"Success rate: {successful_episodes/num_episodes:.2%}\")\n",
    "    return data\n",
    "\n",
    "print(\"Data collection function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a347d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting new dataset with sequence_len=5...\n",
      "Precomuting visibility graph...\n",
      "\t| Recomputing...\n",
      "\t| Creating grid...\n",
      "\t| Building graph...\n",
      "\t| Computing distances grid...\n",
      "\t| Computing distances grid...\n",
      "\t| saving to cache...\n",
      "\t| finished.\n",
      "\t| saving to cache...\n",
      "\t| finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting experience:   7%|▋         | 73/1000 [03:37<46:08,  2.99s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mInvalid response: 404 Not Found. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset_path = 'expert_dataset_seq5.pickle'  # Using sequence_len=5 for dataset\n",
    "SEQUENCE_LEN = 5  # Default sequence length for data collection\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Loading existing dataset from {dataset_path}...\")\n",
    "    data = load_pickle(dataset_path)\n",
    "    print(f\"✓ Loaded {len(data)} sequence samples\")\n",
    "else:\n",
    "    print(f\"Collecting new dataset with sequence_len={SEQUENCE_LEN}...\")\n",
    "    layout_dict = read_layout_dict('example0')\n",
    "    \n",
    "    # Fix layout_dict - combine start_pos and start_heading into start tuple\n",
    "    if 'start_pos' in layout_dict and 'start_heading' in layout_dict:\n",
    "        start_pos = layout_dict.pop('start_pos')\n",
    "        start_heading = layout_dict.pop('start_heading')\n",
    "        layout_dict['start'] = (*start_pos, start_heading)\n",
    "    elif 'start_pos' in layout_dict:\n",
    "        layout_dict['start'] = layout_dict.pop('start_pos')\n",
    "    \n",
    "    # Remove any keys that aren't valid env parameters\n",
    "    if 'layout' in layout_dict:\n",
    "        layout_dict.pop('layout')\n",
    "    \n",
    "    env_collect = ContinuousNavigationEnv(**layout_dict)\n",
    "    vgraph = VisibilityGraph(env_collect.goal, env_collect.obstacles, env_collect.bounds, resolution=(20, 20))\n",
    "    data = collect_rl_experience(env_collect, vgraph, num_episodes=CONFIG['num_episodes'], \n",
    "                                  max_steps=CONFIG['max_steps'], sequence_len=SEQUENCE_LEN)\n",
    "    save_pickle(data, dataset_path)\n",
    "    print(f\"✓ Saved dataset to {dataset_path}\")\n",
    "\n",
    "state_sequences = np.array([d['state_sequences'] for d in data])\n",
    "action_sequences = np.array([d['action_sequences'] for d in data])\n",
    "target_actions = np.array([d['target_action'] for d in data])\n",
    "goals = np.array([d['goal'] for d in data])\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Total sequence samples: {len(data)}\")\n",
    "print(f\"  State sequences shape: {state_sequences.shape}  # (samples, seq_len, state_dim)\")\n",
    "print(f\"  Action sequences shape: {action_sequences.shape}  # (samples, seq_len, action_dim)\")\n",
    "print(f\"  Target actions shape: {target_actions.shape}  # (samples, action_dim)\")\n",
    "print(f\"  Goals shape: {goals.shape}  # (samples, 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DIM = state_sequences.shape[2]  # state dimension from (samples, seq_len, state_dim)\n",
    "ACTION_DIM = target_actions.shape[1]\n",
    "GOAL_DIM = 2\n",
    "layout_dict = read_layout_dict('example0')\n",
    "\n",
    "# Fix layout_dict - combine start_pos and start_heading into start tuple\n",
    "if 'start_pos' in layout_dict and 'start_heading' in layout_dict:\n",
    "    start_pos = layout_dict.pop('start_pos')\n",
    "    start_heading = layout_dict.pop('start_heading')\n",
    "    layout_dict['start'] = (*start_pos, start_heading)\n",
    "elif 'start_pos' in layout_dict:\n",
    "    layout_dict['start'] = layout_dict.pop('start_pos')\n",
    "\n",
    "# Remove any keys that aren't valid env parameters\n",
    "if 'layout' in layout_dict:\n",
    "    layout_dict.pop('layout')\n",
    "\n",
    "env = ContinuousNavigationEnv(**layout_dict)\n",
    "\n",
    "print(f\"Environment & Data Configuration:\")\n",
    "print(f\"  STATE_DIM = {STATE_DIM}\")\n",
    "print(f\"  ACTION_DIM = {ACTION_DIM}\")\n",
    "print(f\"  GOAL_DIM = {GOAL_DIM}\")\n",
    "print(f\"  Total samples = {len(state_sequences)}\")\n",
    "print(f\"  Sequence length used = {state_sequences.shape[1]}\")\n",
    "print(f\"  State space: {env.observation_space.shape}\")\n",
    "print(f\"  Action space: {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49cc82",
   "metadata": {},
   "source": [
    "## Grid Search Training - All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GRID SEARCH TRAINING - ALL ALGORITHMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "GOAL_DIM = 2\n",
    "n_samples = len(state_sequences)\n",
    "n_train = int(n_samples * (1 - CONFIG['val_ratio']))\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "train_state_seqs = state_sequences[train_indices]\n",
    "train_action_seqs = action_sequences[train_indices]\n",
    "train_target_actions = target_actions[train_indices]\n",
    "train_goals = goals[train_indices]\n",
    "\n",
    "val_state_seqs = state_sequences[val_indices]\n",
    "val_action_seqs = action_sequences[val_indices]\n",
    "val_target_actions = target_actions[val_indices]\n",
    "val_goals = goals[val_indices]\n",
    "\n",
    "print(f\"Data split: {len(train_state_seqs)} train, {len(val_state_seqs)} val\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max epochs: {CONFIG['num_epochs']}\\n\")\n",
    "\n",
    "grid_search_results = {}\n",
    "algorithms_with_grid = {\n",
    "    'Linear': (LinearAgent, GRID_SEARCH_CONFIGS['Linear']),\n",
    "    'Transformer': (TransformerAgent, GRID_SEARCH_CONFIGS['Transformer']),\n",
    "    'Bayesian': (BayesianAgent, GRID_SEARCH_CONFIGS['Bayesian']),\n",
    "    'VAE': (VAEAgent, GRID_SEARCH_CONFIGS['VAE'])\n",
    "}\n",
    "\n",
    "print(\"Note: Data collected with sequence_len=5\")\n",
    "print(\"Grid search will test different sequence_len by slicing/padding sequences\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg_name, (AgentClass, param_grid) in algorithms_with_grid.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Grid Search: {alg_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    from itertools import product\n",
    "    configs = list(product(*param_values))\n",
    "    total_configs = len(configs)\n",
    "    \n",
    "    print(f\"Testing {total_configs} configurations for {alg_name}...\")\n",
    "    \n",
    "    best_config = None\n",
    "    best_val_loss = float('inf')\n",
    "    best_agent = None\n",
    "    best_train_time = 0\n",
    "    \n",
    "    for config_idx, config_vals in enumerate(configs):\n",
    "        config = dict(zip(param_names, config_vals))\n",
    "        \n",
    "        print(f\"  [{config_idx+1}/{total_configs}] Testing {config}...\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data for this sequence_len\n",
    "            config_seq_len = config.get('sequence_len', 1)\n",
    "            data_seq_len = train_state_seqs.shape[1]\n",
    "            \n",
    "            # Slice or pad sequences to match config\n",
    "            if config_seq_len <= data_seq_len:\n",
    "                train_seqs_adj = train_state_seqs[:, -config_seq_len:, :]\n",
    "                train_acts_adj = train_action_seqs[:, -config_seq_len:, :]\n",
    "                val_seqs_adj = val_state_seqs[:, -config_seq_len:, :]\n",
    "                val_acts_adj = val_action_seqs[:, -config_seq_len:, :]\n",
    "            else:\n",
    "                # Pad if needed (shouldn't happen with current setup)\n",
    "                train_seqs_adj = train_state_seqs\n",
    "                train_acts_adj = train_action_seqs\n",
    "                val_seqs_adj = val_state_seqs\n",
    "                val_acts_adj = val_action_seqs\n",
    "            \n",
    "            agent = AgentClass(\n",
    "                state_dim=STATE_DIM,\n",
    "                action_dim=ACTION_DIM,\n",
    "                goal_dim=GOAL_DIM,\n",
    "                **config,\n",
    "                device=CONFIG['device'],\n",
    "                action_low=env.action_space.low,\n",
    "                action_high=env.action_space.high\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            train_losses, val_losses = train_agent_optimized(\n",
    "                agent,\n",
    "                train_seqs_adj, train_acts_adj, train_target_actions, train_goals,\n",
    "                val_seqs_adj, val_acts_adj, val_target_actions, val_goals,\n",
    "                num_epochs=CONFIG['num_epochs'],\n",
    "                batch_size=CONFIG['batch_size'],\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            val_loss = min(val_losses) if val_losses else float('inf')\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_config = config\n",
    "                best_agent = agent\n",
    "                best_train_time = elapsed_time\n",
    "                print(f\"    ✓ New best: {val_loss:.6f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    grid_search_results[alg_name] = {\n",
    "        'best_config': best_config,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_agent': best_agent,\n",
    "        'train_time': best_train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{alg_name} Best Config: {best_config}\")\n",
    "    print(f\"{alg_name} Best Val Loss: {best_val_loss:.6f}\")\n",
    "    print(f\"{alg_name} Train Time: {best_train_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "for alg_name, result in grid_search_results.items():\n",
    "    print(f\"{alg_name:12} | Val Loss: {result['best_val_loss']:.6f} | Time: {result['train_time']:.2f}s\")\n",
    "    print(f\"             | Config: {result['best_config']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cee6b",
   "metadata": {},
   "source": [
    "## Evaluate Best Models from Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING BEST MODELS FROM GRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_agent_optimized(agent, env, num_episodes=50, max_steps=200, device='cpu'):\n",
    "    \"\"\"\n",
    "    Clean evaluation function without fallback mechanisms.\n",
    "    All agents must have sequence_len attribute.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'rewards': [], \n",
    "        'successes': [], \n",
    "        'steps': [],\n",
    "        'final_distances': [],\n",
    "        'final_velocities': []\n",
    "    }\n",
    "    \n",
    "    # All agents must have sequence_len attribute\n",
    "    sequence_len = agent.sequence_len\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy()  # Environment always has goal\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Initialize sequence history\n",
    "        state_history = [np.zeros_like(state) for _ in range(sequence_len)]\n",
    "        action_history = [np.zeros(ACTION_DIM) for _ in range(sequence_len)]\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Update history\n",
    "            state_history.append(state.copy())\n",
    "            state_history.pop(0)\n",
    "            \n",
    "            # Create sequences\n",
    "            state_seq = np.array(state_history)\n",
    "            action_seq = np.array(action_history)\n",
    "            \n",
    "            state_seq_t = torch.tensor(state_seq, dtype=torch.float32, device=device)\n",
    "            action_seq_t = torch.tensor(action_seq, dtype=torch.float32, device=device)\n",
    "            goal_t = torch.tensor(goal, dtype=torch.float32, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action = agent.predict_action(state_seq_t, action_seq_t, goal_t)\n",
    "            \n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.cpu().numpy()\n",
    "            \n",
    "            # Update action history\n",
    "            action_history.append(action.copy())\n",
    "            action_history.pop(0)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        results['rewards'].append(episode_reward)\n",
    "        success = info.get('reason', '') == 'goal_reached' if done else False\n",
    "        results['successes'].append(1 if success else 0)\n",
    "        results['steps'].append(step + 1)\n",
    "        results['final_distances'].append(np.linalg.norm(state[:2] - goal))\n",
    "        results['final_velocities'].append(state[3])\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(results['rewards']),\n",
    "        'success_rate': np.mean(results['successes']),\n",
    "        'avg_steps': np.mean(results['steps']),\n",
    "        'avg_final_dist': np.mean(results['final_distances']),\n",
    "        'avg_final_vel': np.mean(results['final_velocities'])\n",
    "    }\n",
    "\n",
    "eval_results = {}\n",
    "layout_dict_eval = read_layout_dict('example0')\n",
    "\n",
    "# Fix layout_dict - combine start_pos and start_heading into start tuple\n",
    "if 'start_pos' in layout_dict_eval and 'start_heading' in layout_dict_eval:\n",
    "    start_pos = layout_dict_eval.pop('start_pos')\n",
    "    start_heading = layout_dict_eval.pop('start_heading')\n",
    "    layout_dict_eval['start'] = (*start_pos, start_heading)\n",
    "elif 'start_pos' in layout_dict_eval:\n",
    "    layout_dict_eval['start'] = layout_dict_eval.pop('start_pos')\n",
    "\n",
    "# Remove any keys that aren't valid env parameters\n",
    "if 'layout' in layout_dict_eval:\n",
    "    layout_dict_eval.pop('layout')\n",
    "\n",
    "env_eval = ContinuousNavigationEnv(**layout_dict_eval)\n",
    "\n",
    "for alg_name, result in grid_search_results.items():\n",
    "    agent = result['best_agent']\n",
    "    print(f\"\\nEvaluating {alg_name}...\")\n",
    "    eval_res = evaluate_agent_optimized(agent, env_eval, num_episodes=CONFIG['num_test_episodes'], device=CONFIG['device'])\n",
    "    eval_results[alg_name] = eval_res\n",
    "    print(f\"{alg_name} - Success: {eval_res['success_rate']:.1%}, Avg Reward: {eval_res['avg_reward']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f2f13",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = []\n",
    "for alg_name, result in grid_search_results.items():\n",
    "    eval_res = eval_results[alg_name]\n",
    "    results_data.append({\n",
    "        'Algorithm': alg_name,\n",
    "        'Val Loss': result['best_val_loss'],\n",
    "        'Train Time (s)': result['train_time'],\n",
    "        'Success Rate': eval_res['success_rate'],\n",
    "        'Avg Reward': eval_res['avg_reward'],\n",
    "        'Avg Steps': eval_res['avg_steps'],\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "df_results = df_results.sort_values('Val Loss')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "axes[0, 0].bar(df_results['Algorithm'], df_results['Success Rate'], color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_ylabel('Success Rate')\n",
    "axes[0, 0].set_title('Success Rate by Algorithm')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].set_ylim([0, 1.1])\n",
    "\n",
    "axes[0, 1].bar(df_results['Algorithm'], df_results['Avg Reward'], color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Average Reward')\n",
    "axes[0, 1].set_title('Average Reward by Algorithm')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1, 0].bar(df_results['Algorithm'], df_results['Val Loss'], color='salmon', edgecolor='black')\n",
    "axes[1, 0].set_ylabel('Validation Loss')\n",
    "axes[1, 0].set_title('Validation Loss by Algorithm')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "axes[1, 1].bar(df_results['Algorithm'], df_results['Train Time (s)'], color='plum', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Training Time (s)')\n",
    "axes[1, 1].set_title('Training Time by Algorithm')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf01ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'trained_models/grid_search_best'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for alg_name, result in grid_search_results.items():\n",
    "    agent = result['best_agent']\n",
    "    model_path = os.path.join(save_dir, f'{alg_name}_best.pth')\n",
    "    \n",
    "    if hasattr(agent, 'model'):\n",
    "        torch.save(agent.model.state_dict(), model_path)\n",
    "    elif hasattr(agent, 'encoder'):\n",
    "        torch.save({'encoder': agent.encoder.state_dict(), 'decoder': agent.decoder.state_dict()}, model_path)\n",
    "    \n",
    "    config_path = os.path.join(save_dir, f'{alg_name}_config.pkl')\n",
    "    save_pickle({\n",
    "        'config': result['best_config'],\n",
    "        'val_loss': result['best_val_loss'],\n",
    "        'train_time': result['train_time'],\n",
    "        'eval_results': eval_results[alg_name]\n",
    "    }, config_path)\n",
    "\n",
    "print(f\"Models and configs saved to {save_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "algorithms = list(grid_search_results.keys())\n",
    "val_losses = [grid_search_results[alg]['best_val_loss'] for alg in algorithms]\n",
    "train_times = [grid_search_results[alg]['train_time'] for alg in algorithms]\n",
    "success_rates = [eval_results[alg]['success_rate'] for alg in algorithms]\n",
    "avg_rewards = [eval_results[alg]['avg_reward'] for alg in algorithms]\n",
    "\n",
    "axes[0, 0].bar(algorithms, success_rates, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_ylabel('Success Rate', fontsize=12)\n",
    "axes[0, 0].set_title('Success Rate by Algorithm', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].set_ylim([0, 1.1])\n",
    "\n",
    "axes[0, 1].bar(algorithms, avg_rewards, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Average Reward', fontsize=12)\n",
    "axes[0, 1].set_title('Average Reward by Algorithm', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1, 0].bar(algorithms, val_losses, color='salmon', edgecolor='black')\n",
    "axes[1, 0].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[1, 0].set_title('Validation Loss by Algorithm', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "axes[1, 1].bar(algorithms, train_times, color='plum', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Training Time (s)', fontsize=12)\n",
    "axes[1, 1].set_title('Training Time by Algorithm', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for alg in algorithms:\n",
    "    print(f\"\\n{alg}:\")\n",
    "    print(f\"  Best Config: {grid_search_results[alg]['best_config']}\")\n",
    "    print(f\"  Validation Loss: {grid_search_results[alg]['best_val_loss']:.6f}\")\n",
    "    print(f\"  Training Time: {grid_search_results[alg]['train_time']:.2f}s\")\n",
    "    print(f\"  Success Rate: {eval_results[alg]['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Reward: {eval_results[alg]['avg_reward']:.2f}\")\n",
    "    print(f\"  Avg Steps: {eval_results[alg]['avg_steps']:.1f}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
