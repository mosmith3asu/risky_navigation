{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578ac5ae",
   "metadata": {},
   "source": [
    "# Algorithm Comparison for Next-Action Prediction (with Hyperparameter Tuning)\n",
    "\n",
    "This notebook provides a comprehensive comparison of different algorithms for next-action prediction in the risky navigation environment, following machine learning best practices with proper hyperparameter tuning:\n",
    "\n",
    "**Algorithms Evaluated:**\n",
    "- **A2C (Advantage Actor-Critic)**: Reinforcement learning approach\n",
    "- **AutoEncoder**: Neural network encoder-decoder architecture  \n",
    "- **Bayesian**: Bayesian neural network with uncertainty quantification\n",
    "- **Transformer**: Self-attention based model\n",
    "- **Linear**: Simple linear regression baseline\n",
    "- **VAE**: Variational AutoEncoder with probabilistic latent representations\n",
    "\n",
    "**Best Practices Workflow:**\n",
    "1. **Data Collection & Preparation**: Collect training data and split into train/validation sets\n",
    "2. **Hyperparameter Tuning**: For each algorithm, perform cross-validation hyperparameter search\n",
    "3. **Model Training**: Train each algorithm with optimal hyperparameters found\n",
    "4. **Evaluation**: Test final models on unseen data and compare performance\n",
    "5. **Analysis**: Comprehensive results analysis with visualizations\n",
    "\n",
    "This approach ensures fair comparison by optimizing each algorithm's hyperparameters before evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077109",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3f6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib64/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib64/python3.13/site-packages (3.8.3)\n",
      "Requirement already satisfied: seaborn in ./venv/lib64/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib64/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: torch in ./venv/lib64/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./venv/lib64/python3.13/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib64/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib64/python3.13/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib64/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib64/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib64/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib64/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib64/python3.13/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib64/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib64/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib64/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib64/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib64/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib64/python3.13/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib64/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib64/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib64/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib64/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib64/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./venv/lib64/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./venv/lib64/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./venv/lib64/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./venv/lib64/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./venv/lib64/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./venv/lib64/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./venv/lib64/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./venv/lib64/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./venv/lib64/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./venv/lib64/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./venv/lib64/python3.13/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./venv/lib64/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./venv/lib64/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./venv/lib64/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./venv/lib64/python3.13/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib64/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib64/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe2cea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "Device available: CUDA\n",
      "\n",
      "Testing AutoEncoder architecture:\n",
      "AutoEncoder architecture verification successful!\n"
     ]
    }
   ],
   "source": [
    "# Restart the kernel to reload updated modules\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear the module cache for AutoEncoder\n",
    "modules_to_reload = [\n",
    "    'src.algorithms.AutoEncoder.agent',\n",
    "    'src.algorithms.Bayesian.agent', \n",
    "    'src.algorithms.Transformer.agent',\n",
    "    'src.algorithms.Linear.agent',\n",
    "    'src.algorithms.VAE.agent'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import environment and algorithms\n",
    "import sys\n",
    "sys.path.append('/home/ash/research-labs/RISE-LAB/risky_navigation')\n",
    "\n",
    "from src.env.continuous_nav_env import ContinuousNavigationEnv\n",
    "from src.algorithms.AutoEncoder.agent import AutoEncoderAgent\n",
    "from src.algorithms.Bayesian.agent import BayesianAgent\n",
    "from src.algorithms.Transformer.agent import TransformerAgent\n",
    "from src.algorithms.Linear.agent import LinearAgent\n",
    "from src.algorithms.VAE.agent import VAEAgent\n",
    "from src.utils.file_management import save_pickle, load_pickle\n",
    "from src.utils.logger import Logger\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Test AutoEncoder architecture to verify fix\n",
    "print(\"\\nTesting AutoEncoder architecture:\")\n",
    "test_model = AutoEncoderAgent(state_dim=4, action_dim=2, goal_dim=2, \n",
    "                             latent_dim=32, hidden_dims=[128, 64])\n",
    "print(\"AutoEncoder architecture verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817ec8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69a17c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment dimensions:\n",
      "  State dimension: 8\n",
      "  Action dimension: 2\n",
      "  Goal dimension: 2\n",
      "  Device: cuda\n",
      "Hyperparameter tuning configuration loaded:\n",
      "  Search method: grid\n",
      "  Number of trials: 10\n",
      "  CV folds: 3\n",
      "  HP search epochs: 50\n",
      "  Algorithms configured: ['AutoEncoder', 'Bayesian', 'Transformer', 'Linear', 'VAE']\n"
     ]
    }
   ],
   "source": [
    "# Configuration for experiments\n",
    "CONFIG = {\n",
    "    'num_episodes': 1000,          # Episodes for data collection\n",
    "    'max_steps': 200,             # Max steps per episode\n",
    "    'batch_size': 128,             # Training batch size\n",
    "    'num_epochs': 700,             # Training epochs (reduced for faster comparison)\n",
    "    'val_ratio': 0.2,             # Validation set ratio\n",
    "    'num_test_episodes': 50,      # Episodes for testing\n",
    "    'lr': 1e-3,                   # Learning rate\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'search_method': 'grid',  # 'grid' or 'random'\n",
    "    'n_trials': 10,             # For random search\n",
    "    'cv_folds': 3,              # Cross-validation folds\n",
    "    'patience': 3,              # Early stopping patience\n",
    "    'hp_epochs': 50,            # Epochs for hyperparameter search (reduced for speed)\n",
    "}\n",
    "\n",
    "# Algorithms to compare\n",
    "ALGORITHMS = ['AutoEncoder', 'Bayesian', 'Transformer', 'Linear', 'VAE']\n",
    "\n",
    "# Initialize environment\n",
    "env = ContinuousNavigationEnv()\n",
    "dummy_state = env.reset()\n",
    "\n",
    "# Get dimensions\n",
    "STATE_DIM = dummy_state.shape[0]\n",
    "ACTION_DIM = env.action_space.shape[0]\n",
    "GOAL_DIM = env.goal.shape[0] if hasattr(env, 'goal') else 2\n",
    "\n",
    "print(f\"Environment dimensions:\")\n",
    "print(f\"  State dimension: {STATE_DIM}\")\n",
    "print(f\"  Action dimension: {ACTION_DIM}\")\n",
    "print(f\"  Goal dimension: {GOAL_DIM}\")\n",
    "print(f\"  Device: {CONFIG['device']}\")\n",
    "\n",
    "# Results storage - updated to include hyperparameter information\n",
    "results = {\n",
    "    'algorithm': [],\n",
    "    'train_time': [],\n",
    "    'test_time': [],\n",
    "    'final_train_loss': [],\n",
    "    'final_val_loss': [],\n",
    "    'avg_mse': [],\n",
    "    'avg_reward': [],\n",
    "    'success_rate': [],\n",
    "    'model_params': [],\n",
    "    'best_hyperparams': [],        # New field for best hyperparameters\n",
    "    'hp_search_cv_score': []       # New field for CV score from hyperparameter search\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define hyperparameter search spaces for each algorithm\n",
    "HYPERPARAMETER_SPACES = {\n",
    "    'AutoEncoder': {\n",
    "        'latent_dim': [32, 64, 128],\n",
    "        'hidden_dims': [[], [64], [128], [64, 32], [128, 64], [256, 128]],  # Intermediate layers only\n",
    "        'lr': [1e-4, 1e-3, 1e-2],\n",
    "        'dropout': [0.0, 0.1, 0.2],\n",
    "        'activation': ['ReLU', 'ELU', 'GELU'],\n",
    "        'batch_norm': [True, False]\n",
    "    },\n",
    "    'Bayesian': {\n",
    "        'latent_dim': [32, 64, 128],  # Fixed: BayesianAgent uses latent_dim, not hidden_dim\n",
    "        'lr': [1e-4, 1e-3, 1e-2],\n",
    "        'kl_weight': [0.001, 0.01, 0.1],  # Fixed: Bayesian specific parameter\n",
    "        'prior_std': [0.5, 1.0, 2.0]  # Fixed: prior_variance -> prior_std\n",
    "    },\n",
    "    'Transformer': {\n",
    "        'd_model': [32, 64, 128],\n",
    "        'nhead': [2, 4, 8],\n",
    "        'num_layers': [1, 2, 3],\n",
    "        'dropout': [0.0, 0.1, 0.2],\n",
    "        'lr': [1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "    'Linear': {\n",
    "        'lr': [1e-4, 1e-3, 1e-2],\n",
    "        'weight_decay': [0.0, 1e-5, 1e-4]\n",
    "    },\n",
    "    'VAE': {\n",
    "        'latent_dim': [16, 32, 64],\n",
    "        'hidden_dim': [64, 128, 256],\n",
    "        'lr': [1e-4, 1e-3, 1e-2],\n",
    "        'beta': [0.5, 1.0, 2.0],\n",
    "        'dropout': [0.0, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter tuning configuration loaded:\")\n",
    "print(f\"  Search method: {CONFIG['search_method']}\")\n",
    "print(f\"  Number of trials: {CONFIG['n_trials']}\")\n",
    "print(f\"  CV folds: {CONFIG['cv_folds']}\")\n",
    "print(f\"  HP search epochs: {CONFIG['hp_epochs']}\")\n",
    "print(f\"  Algorithms configured: {list(HYPERPARAMETER_SPACES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622191",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a2b379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def create_k_fold_splits(data, k=3):\n",
    "    \"\"\"Create k-fold cross-validation splits.\"\"\"\n",
    "    states, actions, goals, next_actions = data\n",
    "    n_samples = len(states)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    fold_size = n_samples // k\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        start_idx = i * fold_size\n",
    "        end_idx = start_idx + fold_size if i < k-1 else n_samples\n",
    "        \n",
    "        val_indices = indices[start_idx:end_idx]\n",
    "        train_indices = np.concatenate([indices[:start_idx], indices[end_idx:]])\n",
    "        \n",
    "        train_fold = (states[train_indices], actions[train_indices], \n",
    "                      goals[train_indices], next_actions[train_indices])\n",
    "        val_fold = (states[val_indices], actions[val_indices], \n",
    "                    goals[val_indices], next_actions[val_indices])\n",
    "        \n",
    "        folds.append((train_fold, val_fold))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def generate_hyperparameter_combinations(algorithm_name, search_method='random', n_trials=10):\n",
    "    \"\"\"Generate hyperparameter combinations for tuning.\"\"\"\n",
    "    param_space = HYPERPARAMETER_SPACES[algorithm_name]\n",
    "    \n",
    "    if search_method == 'grid':\n",
    "        # Grid search - all combinations\n",
    "        combinations = list(ParameterGrid(param_space))\n",
    "    else:\n",
    "        # Random search - sample n_trials combinations\n",
    "        combinations = []\n",
    "        keys = list(param_space.keys())\n",
    "        \n",
    "        for _ in range(n_trials):\n",
    "            combination = {}\n",
    "            for key in keys:\n",
    "                combination[key] = random.choice(param_space[key])\n",
    "            combinations.append(combination)\n",
    "    \n",
    "    return combinations\n",
    "\n",
    "def add_missing_methods(agent):\n",
    "    \"\"\"Add missing methods to agents to ensure compatibility.\"\"\"\n",
    "    # Add predict_next_action method if missing\n",
    "    if not hasattr(agent, 'predict_next_action'):\n",
    "        def predict_next_action(state, action, goal):\n",
    "            \"\"\"Predict next action given current state, action, and goal.\"\"\"\n",
    "            agent.model.eval()\n",
    "            with torch.no_grad():\n",
    "                if isinstance(state, np.ndarray):\n",
    "                    state = torch.tensor(state, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "                if isinstance(action, np.ndarray):\n",
    "                    action = torch.tensor(action, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "                if isinstance(goal, np.ndarray):\n",
    "                    goal = torch.tensor(goal, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "                \n",
    "                if hasattr(agent.model, 'forward'):\n",
    "                    prediction = agent.model(state, action, goal)\n",
    "                elif hasattr(agent, 'model'):\n",
    "                    prediction = agent.model(state, action, goal)\n",
    "                else:\n",
    "                    # Fallback for different model structures\n",
    "                    inputs = torch.cat([state, action, goal], dim=-1)\n",
    "                    prediction = agent.model(inputs)\n",
    "                \n",
    "                return prediction.squeeze().cpu().numpy()\n",
    "        \n",
    "        agent.predict_next_action = predict_next_action\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def create_agent_with_hyperparams(algorithm_name, hyperparams, state_dim, action_dim, goal_dim, device):\n",
    "    \"\"\"Create an agent instance with specific hyperparameters.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if algorithm_name == 'AutoEncoder':\n",
    "            agent = AutoEncoderAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                goal_dim=goal_dim,\n",
    "                latent_dim=hyperparams.get('latent_dim', 64),\n",
    "                hidden_dims=hyperparams.get('hidden_dims', [128]),  # Fixed: intermediate layers only\n",
    "                lr=hyperparams.get('lr', 1e-3),\n",
    "                dropout=hyperparams.get('dropout', 0.0),\n",
    "                activation=hyperparams.get('activation', 'ReLU'),\n",
    "                batch_norm=hyperparams.get('batch_norm', False),\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        elif algorithm_name == 'Bayesian':\n",
    "            agent = BayesianAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                goal_dim=goal_dim,\n",
    "                latent_dim=hyperparams.get('latent_dim', 64),  # Fixed: use latent_dim\n",
    "                lr=hyperparams.get('lr', 1e-3),\n",
    "                kl_weight=hyperparams.get('kl_weight', 0.01),  # Fixed: use kl_weight\n",
    "                prior_std=hyperparams.get('prior_std', 1.0),  # Fixed: use prior_std\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        elif algorithm_name == 'Transformer':\n",
    "            agent = TransformerAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                goal_dim=goal_dim,\n",
    "                d_model=hyperparams.get('d_model', 64),\n",
    "                nhead=hyperparams.get('nhead', 4),\n",
    "                num_layers=hyperparams.get('num_layers', 2),\n",
    "                dropout=hyperparams.get('dropout', 0.1),\n",
    "                lr=hyperparams.get('lr', 1e-3),\n",
    "                device=device,\n",
    "            )\n",
    "        \n",
    "        elif algorithm_name == 'Linear':\n",
    "            agent = LinearAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                goal_dim=goal_dim,\n",
    "                lr=hyperparams.get('lr', 1e-3),\n",
    "                weight_decay=hyperparams.get('weight_decay', 0.0),\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        elif algorithm_name == 'VAE':\n",
    "            agent = VAEAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                goal_dim=goal_dim,\n",
    "                latent_dim=hyperparams.get('latent_dim', 32),\n",
    "                hidden_dim=hyperparams.get('hidden_dim', 128),\n",
    "                lr=hyperparams.get('lr', 1e-3),\n",
    "                beta=hyperparams.get('beta', 1.0),\n",
    "                dropout=hyperparams.get('dropout', 0.0),\n",
    "                device=device\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {algorithm_name}\")\n",
    "        \n",
    "        # Add missing methods for compatibility\n",
    "        agent = add_missing_methods(agent)\n",
    "        return agent\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating agent {algorithm_name} with hyperparams {hyperparams}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# --- Universal Agent Save Function ---\n",
    "def save_agent_model(agent, path):\n",
    "    \"\"\"Save model for any agent type.\"\"\"\n",
    "    try:\n",
    "        if hasattr(agent, 'save'):\n",
    "            agent.save(path)\n",
    "        elif hasattr(agent, 'model'):\n",
    "            torch.save(agent.model.state_dict(), path)\n",
    "        elif hasattr(agent, 'encoder') and hasattr(agent, 'decoder'):\n",
    "            torch.save({\n",
    "                'encoder': agent.encoder.state_dict(),\n",
    "                'decoder': agent.decoder.state_dict()\n",
    "            }, path)\n",
    "        else:\n",
    "            raise AttributeError(\"No model found to save.\")\n",
    "        print(f\"Model saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652df29",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e7f97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset from comparison_dataset.pickle\n",
      "Training samples: 9372\n",
      "Validation samples: 2342\n",
      "Training samples: 9372\n",
      "Validation samples: 2342\n"
     ]
    }
   ],
   "source": [
    "def collect_data(env, num_episodes=100, max_steps=200):\n",
    "    \"\"\"Collect training data from random trajectories.\"\"\"\n",
    "    data = []\n",
    "    for ep in tqdm(range(num_episodes), desc='Collecting data'):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        for t in range(max_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_action = env.action_space.sample() if not done else np.zeros_like(action)\n",
    "            data.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action.copy(),\n",
    "                'goal': goal.copy(),\n",
    "                'next_action': next_action.copy(),\n",
    "            })\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def prepare_data(data, val_ratio=0.2):\n",
    "    \"\"\"Convert data to arrays and split into train/val sets.\"\"\"\n",
    "    states = np.stack([d['state'] for d in data])\n",
    "    actions = np.stack([d['action'] for d in data])\n",
    "    goals = np.stack([d['goal'] for d in data])\n",
    "    next_actions = np.stack([d['next_action'] for d in data])\n",
    "    \n",
    "    # Split data\n",
    "    n = states.shape[0]\n",
    "    indices = np.random.permutation(n)\n",
    "    val_size = int(n * val_ratio)\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "    \n",
    "    train_data = (states[train_indices], actions[train_indices], \n",
    "                  goals[train_indices], next_actions[train_indices])\n",
    "    val_data = (states[val_indices], actions[val_indices], \n",
    "                goals[val_indices], next_actions[val_indices])\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# Collect or load data\n",
    "dataset_path = 'comparison_dataset.pickle'\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Loading existing dataset from {dataset_path}\")\n",
    "    data = load_pickle(dataset_path)\n",
    "else:\n",
    "    print(\"Collecting new dataset...\")\n",
    "    data = collect_data(env, CONFIG['num_episodes'], CONFIG['max_steps'])\n",
    "    save_pickle(data, dataset_path)\n",
    "    print(f\"Dataset saved to {dataset_path}\")\n",
    "\n",
    "# Prepare data\n",
    "train_data, val_data = prepare_data(data, CONFIG['val_ratio'])\n",
    "print(f\"Training samples: {len(train_data[0])}\")\n",
    "print(f\"Validation samples: {len(val_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f3f2d",
   "metadata": {},
   "source": [
    "## Helper Functions for Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f50e8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, train_data, val_data, num_epochs, batch_size):\n",
    "    \"\"\"Train an agent and return training metrics.\"\"\"\n",
    "    train_states, train_actions, train_goals, train_next_actions = train_data\n",
    "    val_states, val_actions, val_goals, val_next_actions = val_data\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    num_batches = len(train_states) // batch_size\n",
    "    \n",
    "    print(f\"Training for {num_epochs} epochs with {num_batches} batches per epoch...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(len(train_states))\n",
    "        train_states_shuffled = train_states[indices]\n",
    "        train_actions_shuffled = train_actions[indices]\n",
    "        train_goals_shuffled = train_goals[indices]\n",
    "        train_next_actions_shuffled = train_next_actions[indices]\n",
    "        \n",
    "        # Train for one epoch\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            batch_states = train_states_shuffled[start_idx:end_idx]\n",
    "            batch_actions = train_actions_shuffled[start_idx:end_idx]\n",
    "            batch_goals = train_goals_shuffled[start_idx:end_idx]\n",
    "            batch_next_actions = train_next_actions_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Convert to tensors and move to device\n",
    "            device = getattr(agent, 'device', 'cpu')\n",
    "            batch_states = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "            batch_actions = torch.tensor(batch_actions, dtype=torch.float32, device=device)\n",
    "            batch_goals = torch.tensor(batch_goals, dtype=torch.float32, device=device)\n",
    "            batch_next_actions = torch.tensor(batch_next_actions, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Handle different return types from train_step\n",
    "            loss_result = agent.train_step(batch_states, batch_actions, batch_goals, batch_next_actions)\n",
    "            if isinstance(loss_result, dict):\n",
    "                loss = loss_result['loss']\n",
    "            else:\n",
    "                loss = loss_result\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        # Compute losses\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        \n",
    "        # Validate - compute validation loss manually for all agents\n",
    "        val_loss = compute_validation_loss(agent, val_states, val_actions, val_goals, val_next_actions)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}: Train Loss={avg_train_loss:.6f}, Val Loss={val_loss:.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def compute_validation_loss(agent, val_states, val_actions, val_goals, val_next_actions):\n",
    "    \"\"\"Compute validation loss for all agent types.\"\"\"\n",
    "    # Set models to eval mode\n",
    "    if hasattr(agent, 'model'):\n",
    "        agent.model.eval()\n",
    "    if hasattr(agent, 'encoder'):\n",
    "        agent.encoder.eval()\n",
    "    if hasattr(agent, 'decoder'):\n",
    "        agent.decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert to tensors and move to device\n",
    "        device = getattr(agent, 'device', 'cpu')\n",
    "        states = torch.tensor(val_states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(val_actions, dtype=torch.float32, device=device)\n",
    "        goals = torch.tensor(val_goals, dtype=torch.float32, device=device)\n",
    "        next_actions = torch.tensor(val_next_actions, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # For VAE agent (has compute_loss method)\n",
    "        if hasattr(agent, 'compute_loss'):\n",
    "            loss_dict = agent.compute_loss(states, actions, goals, next_actions)\n",
    "            val_loss = loss_dict['loss'].item()\n",
    "        \n",
    "        # For Bayesian agent (has encoder and decoder, returns tuple from predict)\n",
    "        elif hasattr(agent, 'encoder') and hasattr(agent, 'decoder') and hasattr(agent, 'predict_next_action'):\n",
    "            # Use a subset of data for validation (to speed up)\n",
    "            subset_size = min(100, len(val_states))\n",
    "            val_loss_sum = 0.0\n",
    "            for i in range(subset_size):\n",
    "                pred_result = agent.predict_next_action(val_states[i], val_actions[i], val_goals[i])\n",
    "                if isinstance(pred_result, tuple):\n",
    "                    pred_mean = pred_result[0]  # Take mean, ignore std\n",
    "                else:\n",
    "                    pred_mean = pred_result\n",
    "                \n",
    "                # Calculate MSE for this sample\n",
    "                if isinstance(pred_mean, np.ndarray):\n",
    "                    mse = np.mean((pred_mean - val_next_actions[i])**2)\n",
    "                else:\n",
    "                    mse = float(torch.mean((pred_mean - next_actions[i])**2))\n",
    "                val_loss_sum += mse\n",
    "            val_loss = val_loss_sum / subset_size\n",
    "        \n",
    "        # For AutoEncoder, Transformer, Linear agents (have model attribute)\n",
    "        elif hasattr(agent, 'model'):\n",
    "            predictions = agent.model(states, actions, goals)\n",
    "            val_loss = torch.mean((predictions - next_actions)**2).item()\n",
    "        \n",
    "        else:\n",
    "            # Fallback: use agent's built-in validation if available\n",
    "            if hasattr(agent, 'validate'):\n",
    "                val_loss = agent.validate(val_states, val_actions, val_goals, val_next_actions)\n",
    "            else:\n",
    "                # Last resort: simple MSE calculation\n",
    "                pred_result = agent.predict_next_action(val_states[0], val_actions[0], val_goals[0])\n",
    "                if isinstance(pred_result, tuple):\n",
    "                    pred = pred_result[0]\n",
    "                else:\n",
    "                    pred = pred_result\n",
    "                val_loss = np.mean((pred - val_next_actions[0])**2)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes=10, max_steps=200):\n",
    "    \"\"\"Evaluate agent performance in the environment.\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_mses = []\n",
    "    success_count = 0\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        ep_reward = 0.0\n",
    "        ep_mses = []\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            if t == 0:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Handle different prediction return types\n",
    "                pred_result = agent.predict_next_action(state, prev_action, goal)\n",
    "                if isinstance(pred_result, tuple):\n",
    "                    action = pred_result[0]  # Take mean, ignore std\n",
    "                else:\n",
    "                    action = pred_result\n",
    "                \n",
    "                # Ensure action is the right shape and type\n",
    "                if isinstance(action, torch.Tensor):\n",
    "                    action = action.detach().cpu().numpy()\n",
    "                if action.ndim > 1:\n",
    "                    action = action.flatten()\n",
    "                if len(action) == 1 and env.action_space.shape[0] == 2:\n",
    "                    # If we get a scalar but need 2D action, duplicate it\n",
    "                    action = np.array([action[0], action[0]])\n",
    "                elif len(action) > env.action_space.shape[0]:\n",
    "                    # If we get too many dimensions, take the first N\n",
    "                    action = action[:env.action_space.shape[0]]\n",
    "                \n",
    "                action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Calculate MSE with a random next action as baseline\n",
    "            next_action_gt = env.action_space.sample() if not done else np.zeros_like(action)\n",
    "            pred_result = agent.predict_next_action(state, action, goal)\n",
    "            if isinstance(pred_result, tuple):\n",
    "                next_action_pred = pred_result[0]  # Take mean, ignore std\n",
    "            else:\n",
    "                next_action_pred = pred_result\n",
    "            \n",
    "            # Ensure prediction is the right shape\n",
    "            if isinstance(next_action_pred, torch.Tensor):\n",
    "                next_action_pred = next_action_pred.detach().cpu().numpy()\n",
    "            if next_action_pred.ndim > 1:\n",
    "                next_action_pred = next_action_pred.flatten()\n",
    "            if len(next_action_pred) == 1 and len(next_action_gt) == 2:\n",
    "                next_action_pred = np.array([next_action_pred[0], next_action_pred[0]])\n",
    "            elif len(next_action_pred) > len(next_action_gt):\n",
    "                next_action_pred = next_action_pred[:len(next_action_gt)]\n",
    "            \n",
    "            mse = np.mean((next_action_pred - next_action_gt)**2)\n",
    "            ep_mses.append(mse)\n",
    "            \n",
    "            ep_reward += reward\n",
    "            prev_action = action\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    success_count += 1\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(ep_reward)\n",
    "        episode_mses.extend(ep_mses)\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(episode_rewards),\n",
    "        'avg_mse': np.mean(episode_mses),\n",
    "        'success_rate': success_count / num_episodes\n",
    "    }\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model.\"\"\"\n",
    "    if hasattr(model, 'model'):\n",
    "        return sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "    elif hasattr(model, 'encoder') and hasattr(model, 'decoder'):\n",
    "        return sum(p.numel() for p in model.encoder.parameters() if p.requires_grad) + \\\n",
    "               sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49cc82",
   "metadata": {},
   "source": [
    "## AutoEncoder: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUTOENCODER: HYPERPARAMETER TUNING & TRAINING\n",
      "============================================================\n",
      "Step 1: Hyperparameter Tuning\n",
      "Starting hyperparameter search for AutoEncoder...\n",
      "  Evaluating 972 hyperparameter combinations...\n",
      "  Too many combinations (972), using random sampling of 20\n",
      "    Trial 1/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.2, 'hidden_dims': [256, 128], 'latent_dim': 128, 'lr': 0.01}\n",
      "      CV Score: 0.218475 (±0.003486)\n",
      "    Trial 2/20: {'activation': 'ReLU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.218475 (±0.003486)\n",
      "    Trial 2/20: {'activation': 'ReLU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.221531 (±0.003387)\n",
      "    Trial 3/20: {'activation': 'ELU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [128], 'latent_dim': 128, 'lr': 0.001}\n",
      "      CV Score: 0.221531 (±0.003387)\n",
      "    Trial 3/20: {'activation': 'ELU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [128], 'latent_dim': 128, 'lr': 0.001}\n",
      "      CV Score: 0.219150 (±0.004141)\n",
      "    Trial 4/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [128], 'latent_dim': 128, 'lr': 0.01}\n",
      "      CV Score: 0.219150 (±0.004141)\n",
      "    Trial 4/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [128], 'latent_dim': 128, 'lr': 0.01}\n",
      "      CV Score: 0.220413 (±0.005122)\n",
      "    Trial 5/20: {'activation': 'ELU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.220413 (±0.005122)\n",
      "    Trial 5/20: {'activation': 'ELU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.218588 (±0.004070)\n",
      "    Trial 6/20: {'activation': 'GELU', 'batch_norm': True, 'dropout': 0.2, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.218588 (±0.004070)\n",
      "    Trial 6/20: {'activation': 'GELU', 'batch_norm': True, 'dropout': 0.2, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.219312 (±0.004257)\n",
      "    Trial 7/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [256, 128], 'latent_dim': 32, 'lr': 0.001}\n",
      "      CV Score: 0.219312 (±0.004257)\n",
      "    Trial 7/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [256, 128], 'latent_dim': 32, 'lr': 0.001}\n",
      "      CV Score: 0.220192 (±0.004693)\n",
      "    Trial 8/20: {'activation': 'GELU', 'batch_norm': True, 'dropout': 0.2, 'hidden_dims': [], 'latent_dim': 128, 'lr': 0.0001}\n",
      "      CV Score: 0.220192 (±0.004693)\n",
      "    Trial 8/20: {'activation': 'GELU', 'batch_norm': True, 'dropout': 0.2, 'hidden_dims': [], 'latent_dim': 128, 'lr': 0.0001}\n",
      "      CV Score: 0.222390 (±0.002902)\n",
      "    Trial 9/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.222390 (±0.002902)\n",
      "    Trial 9/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.218733 (±0.003780)\n",
      "    Trial 10/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [64, 32], 'latent_dim': 32, 'lr': 0.0001}\n",
      "      CV Score: 0.218733 (±0.003780)\n",
      "    Trial 10/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.2, 'hidden_dims': [64, 32], 'latent_dim': 32, 'lr': 0.0001}\n",
      "      CV Score: 0.218693 (±0.003875)\n",
      "    Trial 11/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [64, 32], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.218693 (±0.003875)\n",
      "    Trial 11/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [64, 32], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.224009 (±0.001664)\n",
      "    Trial 12/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [64, 32], 'latent_dim': 128, 'lr': 0.001}\n",
      "      CV Score: 0.224009 (±0.001664)\n",
      "    Trial 12/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [64, 32], 'latent_dim': 128, 'lr': 0.001}\n",
      "      CV Score: 0.222233 (±0.003123)\n",
      "    Trial 13/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.1, 'hidden_dims': [], 'latent_dim': 128, 'lr': 0.0001}\n",
      "      CV Score: 0.222233 (±0.003123)\n",
      "    Trial 13/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.1, 'hidden_dims': [], 'latent_dim': 128, 'lr': 0.0001}\n",
      "      CV Score: 0.222667 (±0.003635)\n",
      "    Trial 14/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.222667 (±0.003635)\n",
      "    Trial 14/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.218080 (±0.003985)\n",
      "    Trial 15/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.1, 'hidden_dims': [256, 128], 'latent_dim': 128, 'lr': 0.001}\n",
      "      CV Score: 0.218080 (±0.003985)\n",
      "    Trial 15/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.1, 'hidden_dims': [256, 128], 'latent_dim': 128, 'lr': 0.001}\n",
      "      CV Score: 0.219100 (±0.003532)\n",
      "    Trial 16/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.1, 'hidden_dims': [256, 128], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.219100 (±0.003532)\n",
      "    Trial 16/20: {'activation': 'ELU', 'batch_norm': True, 'dropout': 0.1, 'hidden_dims': [256, 128], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.222155 (±0.004509)\n",
      "    Trial 17/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.2, 'hidden_dims': [256, 128], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.222155 (±0.004509)\n",
      "    Trial 17/20: {'activation': 'ReLU', 'batch_norm': True, 'dropout': 0.2, 'hidden_dims': [256, 128], 'latent_dim': 64, 'lr': 0.001}\n",
      "      CV Score: 0.222002 (±0.001894)\n",
      "    Trial 18/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.222002 (±0.001894)\n",
      "    Trial 18/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.221891 (±0.004833)\n",
      "    Trial 19/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.1, 'hidden_dims': [128], 'latent_dim': 64, 'lr': 0.01}\n",
      "      CV Score: 0.221891 (±0.004833)\n",
      "    Trial 19/20: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.1, 'hidden_dims': [128], 'latent_dim': 64, 'lr': 0.01}\n",
      "      CV Score: 0.218576 (±0.003907)\n",
      "    Trial 20/20: {'activation': 'GELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [64, 32], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.218576 (±0.003907)\n",
      "    Trial 20/20: {'activation': 'GELU', 'batch_norm': True, 'dropout': 0.0, 'hidden_dims': [64, 32], 'latent_dim': 64, 'lr': 0.0001}\n",
      "      CV Score: 0.225472 (±0.002605)\n",
      "  Best hyperparameters found with CV score: 0.218080\n",
      "  Best hyperparams: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "\n",
      "Best AutoEncoder hyperparameters:\n",
      "  activation: GELU\n",
      "  batch_norm: False\n",
      "  dropout: 0.0\n",
      "  hidden_dims: [128, 64]\n",
      "  latent_dim: 64\n",
      "  lr: 0.001\n",
      "Best CV score: 0.218080\n",
      "\n",
      "Step 2: Training AutoEncoder with best hyperparameters...\n",
      "Training for 700 epochs with 73 batches per epoch...\n",
      "      CV Score: 0.225472 (±0.002605)\n",
      "  Best hyperparameters found with CV score: 0.218080\n",
      "  Best hyperparams: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "\n",
      "Best AutoEncoder hyperparameters:\n",
      "  activation: GELU\n",
      "  batch_norm: False\n",
      "  dropout: 0.0\n",
      "  hidden_dims: [128, 64]\n",
      "  latent_dim: 64\n",
      "  lr: 0.001\n",
      "Best CV score: 0.218080\n",
      "\n",
      "Step 2: Training AutoEncoder with best hyperparameters...\n",
      "Training for 700 epochs with 73 batches per epoch...\n",
      "  Epoch 5/700: Train Loss=0.218821, Val Loss=0.214085\n",
      "  Epoch 5/700: Train Loss=0.218821, Val Loss=0.214085\n",
      "  Epoch 10/700: Train Loss=0.218433, Val Loss=0.214117\n",
      "  Epoch 10/700: Train Loss=0.218433, Val Loss=0.214117\n",
      "  Epoch 15/700: Train Loss=0.217667, Val Loss=0.214150\n",
      "  Epoch 15/700: Train Loss=0.217667, Val Loss=0.214150\n",
      "  Epoch 20/700: Train Loss=0.217956, Val Loss=0.214278\n",
      "  Epoch 20/700: Train Loss=0.217956, Val Loss=0.214278\n",
      "  Epoch 25/700: Train Loss=0.218067, Val Loss=0.213808\n",
      "  Epoch 25/700: Train Loss=0.218067, Val Loss=0.213808\n",
      "  Epoch 30/700: Train Loss=0.217451, Val Loss=0.215064\n",
      "  Epoch 30/700: Train Loss=0.217451, Val Loss=0.215064\n",
      "  Epoch 35/700: Train Loss=0.217343, Val Loss=0.213784\n",
      "  Epoch 35/700: Train Loss=0.217343, Val Loss=0.213784\n",
      "  Epoch 40/700: Train Loss=0.217537, Val Loss=0.214763\n",
      "  Epoch 40/700: Train Loss=0.217537, Val Loss=0.214763\n",
      "  Epoch 45/700: Train Loss=0.217260, Val Loss=0.213266\n",
      "  Epoch 45/700: Train Loss=0.217260, Val Loss=0.213266\n",
      "  Epoch 50/700: Train Loss=0.216429, Val Loss=0.214122\n",
      "  Epoch 50/700: Train Loss=0.216429, Val Loss=0.214122\n",
      "  Epoch 55/700: Train Loss=0.217115, Val Loss=0.213520\n",
      "  Epoch 55/700: Train Loss=0.217115, Val Loss=0.213520\n",
      "  Epoch 60/700: Train Loss=0.216142, Val Loss=0.215325\n",
      "  Epoch 60/700: Train Loss=0.216142, Val Loss=0.215325\n",
      "  Epoch 65/700: Train Loss=0.216209, Val Loss=0.213231\n",
      "  Epoch 65/700: Train Loss=0.216209, Val Loss=0.213231\n",
      "  Epoch 70/700: Train Loss=0.215783, Val Loss=0.213452\n",
      "  Epoch 70/700: Train Loss=0.215783, Val Loss=0.213452\n",
      "  Epoch 75/700: Train Loss=0.216137, Val Loss=0.215073\n",
      "  Epoch 75/700: Train Loss=0.216137, Val Loss=0.215073\n",
      "  Epoch 80/700: Train Loss=0.215629, Val Loss=0.213205\n",
      "  Epoch 80/700: Train Loss=0.215629, Val Loss=0.213205\n",
      "  Epoch 85/700: Train Loss=0.215111, Val Loss=0.214420\n",
      "  Epoch 85/700: Train Loss=0.215111, Val Loss=0.214420\n",
      "  Epoch 90/700: Train Loss=0.214821, Val Loss=0.214791\n",
      "  Epoch 90/700: Train Loss=0.214821, Val Loss=0.214791\n",
      "  Epoch 95/700: Train Loss=0.214804, Val Loss=0.214303\n",
      "  Epoch 95/700: Train Loss=0.214804, Val Loss=0.214303\n",
      "  Epoch 100/700: Train Loss=0.214392, Val Loss=0.213089\n",
      "  Epoch 100/700: Train Loss=0.214392, Val Loss=0.213089\n",
      "  Epoch 105/700: Train Loss=0.214186, Val Loss=0.214291\n",
      "  Epoch 105/700: Train Loss=0.214186, Val Loss=0.214291\n",
      "  Epoch 110/700: Train Loss=0.213503, Val Loss=0.213614\n",
      "  Epoch 110/700: Train Loss=0.213503, Val Loss=0.213614\n",
      "  Epoch 115/700: Train Loss=0.213132, Val Loss=0.216524\n",
      "  Epoch 115/700: Train Loss=0.213132, Val Loss=0.216524\n",
      "  Epoch 120/700: Train Loss=0.212991, Val Loss=0.215440\n",
      "  Epoch 120/700: Train Loss=0.212991, Val Loss=0.215440\n",
      "  Epoch 125/700: Train Loss=0.211403, Val Loss=0.216869\n",
      "  Epoch 125/700: Train Loss=0.211403, Val Loss=0.216869\n",
      "  Epoch 130/700: Train Loss=0.211114, Val Loss=0.217121\n",
      "  Epoch 130/700: Train Loss=0.211114, Val Loss=0.217121\n",
      "  Epoch 135/700: Train Loss=0.208918, Val Loss=0.219940\n",
      "  Epoch 135/700: Train Loss=0.208918, Val Loss=0.219940\n",
      "  Epoch 140/700: Train Loss=0.208501, Val Loss=0.217370\n",
      "  Epoch 140/700: Train Loss=0.208501, Val Loss=0.217370\n",
      "  Epoch 145/700: Train Loss=0.207362, Val Loss=0.220208\n",
      "  Epoch 145/700: Train Loss=0.207362, Val Loss=0.220208\n",
      "  Epoch 150/700: Train Loss=0.205267, Val Loss=0.224387\n",
      "  Epoch 150/700: Train Loss=0.205267, Val Loss=0.224387\n",
      "  Epoch 155/700: Train Loss=0.204331, Val Loss=0.222653\n",
      "  Epoch 155/700: Train Loss=0.204331, Val Loss=0.222653\n",
      "  Epoch 160/700: Train Loss=0.201307, Val Loss=0.226707\n",
      "  Epoch 160/700: Train Loss=0.201307, Val Loss=0.226707\n",
      "  Epoch 165/700: Train Loss=0.199435, Val Loss=0.230086\n",
      "  Epoch 165/700: Train Loss=0.199435, Val Loss=0.230086\n",
      "  Epoch 170/700: Train Loss=0.196100, Val Loss=0.233855\n",
      "  Epoch 170/700: Train Loss=0.196100, Val Loss=0.233855\n",
      "  Epoch 175/700: Train Loss=0.195856, Val Loss=0.231777\n",
      "  Epoch 175/700: Train Loss=0.195856, Val Loss=0.231777\n",
      "  Epoch 180/700: Train Loss=0.192393, Val Loss=0.235991\n",
      "  Epoch 180/700: Train Loss=0.192393, Val Loss=0.235991\n",
      "  Epoch 185/700: Train Loss=0.189022, Val Loss=0.237950\n",
      "  Epoch 185/700: Train Loss=0.189022, Val Loss=0.237950\n",
      "  Epoch 190/700: Train Loss=0.186762, Val Loss=0.243038\n",
      "  Epoch 190/700: Train Loss=0.186762, Val Loss=0.243038\n",
      "  Epoch 195/700: Train Loss=0.183093, Val Loss=0.246521\n",
      "  Epoch 195/700: Train Loss=0.183093, Val Loss=0.246521\n",
      "  Epoch 200/700: Train Loss=0.180276, Val Loss=0.247540\n",
      "  Epoch 200/700: Train Loss=0.180276, Val Loss=0.247540\n",
      "  Epoch 205/700: Train Loss=0.177407, Val Loss=0.252589\n",
      "  Epoch 205/700: Train Loss=0.177407, Val Loss=0.252589\n",
      "  Epoch 210/700: Train Loss=0.173804, Val Loss=0.253550\n",
      "  Epoch 210/700: Train Loss=0.173804, Val Loss=0.253550\n",
      "  Epoch 215/700: Train Loss=0.168688, Val Loss=0.260697\n",
      "  Epoch 215/700: Train Loss=0.168688, Val Loss=0.260697\n",
      "  Epoch 220/700: Train Loss=0.165856, Val Loss=0.267458\n",
      "  Epoch 220/700: Train Loss=0.165856, Val Loss=0.267458\n",
      "  Epoch 225/700: Train Loss=0.161687, Val Loss=0.268688\n",
      "  Epoch 225/700: Train Loss=0.161687, Val Loss=0.268688\n",
      "  Epoch 230/700: Train Loss=0.157328, Val Loss=0.270142\n",
      "  Epoch 230/700: Train Loss=0.157328, Val Loss=0.270142\n",
      "  Epoch 235/700: Train Loss=0.154543, Val Loss=0.271800\n",
      "  Epoch 235/700: Train Loss=0.154543, Val Loss=0.271800\n",
      "  Epoch 240/700: Train Loss=0.150900, Val Loss=0.281372\n",
      "  Epoch 240/700: Train Loss=0.150900, Val Loss=0.281372\n",
      "  Epoch 245/700: Train Loss=0.146652, Val Loss=0.287911\n",
      "  Epoch 245/700: Train Loss=0.146652, Val Loss=0.287911\n",
      "  Epoch 250/700: Train Loss=0.142127, Val Loss=0.291016\n",
      "  Epoch 250/700: Train Loss=0.142127, Val Loss=0.291016\n",
      "  Epoch 255/700: Train Loss=0.137433, Val Loss=0.298219\n",
      "  Epoch 255/700: Train Loss=0.137433, Val Loss=0.298219\n",
      "  Epoch 260/700: Train Loss=0.133689, Val Loss=0.303299\n",
      "  Epoch 260/700: Train Loss=0.133689, Val Loss=0.303299\n",
      "  Epoch 265/700: Train Loss=0.130829, Val Loss=0.310552\n",
      "  Epoch 265/700: Train Loss=0.130829, Val Loss=0.310552\n",
      "  Epoch 270/700: Train Loss=0.127954, Val Loss=0.313013\n",
      "  Epoch 270/700: Train Loss=0.127954, Val Loss=0.313013\n",
      "  Epoch 275/700: Train Loss=0.121648, Val Loss=0.319300\n",
      "  Epoch 275/700: Train Loss=0.121648, Val Loss=0.319300\n",
      "  Epoch 280/700: Train Loss=0.119616, Val Loss=0.326002\n",
      "  Epoch 280/700: Train Loss=0.119616, Val Loss=0.326002\n",
      "  Epoch 285/700: Train Loss=0.114977, Val Loss=0.329343\n",
      "  Epoch 285/700: Train Loss=0.114977, Val Loss=0.329343\n",
      "  Epoch 290/700: Train Loss=0.112493, Val Loss=0.339334\n",
      "  Epoch 290/700: Train Loss=0.112493, Val Loss=0.339334\n",
      "  Epoch 295/700: Train Loss=0.108221, Val Loss=0.342669\n",
      "  Epoch 295/700: Train Loss=0.108221, Val Loss=0.342669\n",
      "  Epoch 300/700: Train Loss=0.105224, Val Loss=0.355186\n",
      "  Epoch 300/700: Train Loss=0.105224, Val Loss=0.355186\n",
      "  Epoch 305/700: Train Loss=0.101982, Val Loss=0.352431\n",
      "  Epoch 305/700: Train Loss=0.101982, Val Loss=0.352431\n",
      "  Epoch 310/700: Train Loss=0.098400, Val Loss=0.357231\n",
      "  Epoch 310/700: Train Loss=0.098400, Val Loss=0.357231\n",
      "  Epoch 315/700: Train Loss=0.095359, Val Loss=0.358331\n",
      "  Epoch 315/700: Train Loss=0.095359, Val Loss=0.358331\n",
      "  Epoch 320/700: Train Loss=0.092706, Val Loss=0.365736\n",
      "  Epoch 320/700: Train Loss=0.092706, Val Loss=0.365736\n",
      "  Epoch 325/700: Train Loss=0.090183, Val Loss=0.364916\n",
      "  Epoch 325/700: Train Loss=0.090183, Val Loss=0.364916\n",
      "  Epoch 330/700: Train Loss=0.087794, Val Loss=0.369068\n",
      "  Epoch 330/700: Train Loss=0.087794, Val Loss=0.369068\n",
      "  Epoch 335/700: Train Loss=0.085856, Val Loss=0.380407\n",
      "  Epoch 335/700: Train Loss=0.085856, Val Loss=0.380407\n",
      "  Epoch 340/700: Train Loss=0.082769, Val Loss=0.380877\n",
      "  Epoch 340/700: Train Loss=0.082769, Val Loss=0.380877\n",
      "  Epoch 345/700: Train Loss=0.082010, Val Loss=0.387830\n",
      "  Epoch 345/700: Train Loss=0.082010, Val Loss=0.387830\n",
      "  Epoch 350/700: Train Loss=0.079157, Val Loss=0.385005\n",
      "  Epoch 350/700: Train Loss=0.079157, Val Loss=0.385005\n",
      "  Epoch 355/700: Train Loss=0.074969, Val Loss=0.394236\n",
      "  Epoch 355/700: Train Loss=0.074969, Val Loss=0.394236\n",
      "  Epoch 360/700: Train Loss=0.074489, Val Loss=0.394966\n",
      "  Epoch 360/700: Train Loss=0.074489, Val Loss=0.394966\n",
      "  Epoch 365/700: Train Loss=0.073056, Val Loss=0.396469\n",
      "  Epoch 365/700: Train Loss=0.073056, Val Loss=0.396469\n",
      "  Epoch 370/700: Train Loss=0.070911, Val Loss=0.402706\n",
      "  Epoch 370/700: Train Loss=0.070911, Val Loss=0.402706\n",
      "  Epoch 375/700: Train Loss=0.067762, Val Loss=0.413060\n",
      "  Epoch 375/700: Train Loss=0.067762, Val Loss=0.413060\n",
      "  Epoch 380/700: Train Loss=0.067819, Val Loss=0.407534\n",
      "  Epoch 380/700: Train Loss=0.067819, Val Loss=0.407534\n",
      "  Epoch 385/700: Train Loss=0.065432, Val Loss=0.417829\n",
      "  Epoch 385/700: Train Loss=0.065432, Val Loss=0.417829\n",
      "  Epoch 390/700: Train Loss=0.063873, Val Loss=0.417456\n",
      "  Epoch 390/700: Train Loss=0.063873, Val Loss=0.417456\n",
      "  Epoch 395/700: Train Loss=0.062334, Val Loss=0.422640\n",
      "  Epoch 395/700: Train Loss=0.062334, Val Loss=0.422640\n",
      "  Epoch 400/700: Train Loss=0.061474, Val Loss=0.420066\n",
      "  Epoch 400/700: Train Loss=0.061474, Val Loss=0.420066\n",
      "  Epoch 405/700: Train Loss=0.059380, Val Loss=0.422855\n",
      "  Epoch 405/700: Train Loss=0.059380, Val Loss=0.422855\n",
      "  Epoch 410/700: Train Loss=0.058695, Val Loss=0.426794\n",
      "  Epoch 410/700: Train Loss=0.058695, Val Loss=0.426794\n",
      "  Epoch 415/700: Train Loss=0.055999, Val Loss=0.424038\n",
      "  Epoch 415/700: Train Loss=0.055999, Val Loss=0.424038\n",
      "  Epoch 420/700: Train Loss=0.055622, Val Loss=0.424922\n",
      "  Epoch 420/700: Train Loss=0.055622, Val Loss=0.424922\n",
      "  Epoch 425/700: Train Loss=0.055409, Val Loss=0.428742\n",
      "  Epoch 425/700: Train Loss=0.055409, Val Loss=0.428742\n",
      "  Epoch 430/700: Train Loss=0.052678, Val Loss=0.434863\n",
      "  Epoch 430/700: Train Loss=0.052678, Val Loss=0.434863\n",
      "  Epoch 435/700: Train Loss=0.052928, Val Loss=0.436371\n",
      "  Epoch 435/700: Train Loss=0.052928, Val Loss=0.436371\n",
      "  Epoch 440/700: Train Loss=0.052182, Val Loss=0.434149\n",
      "  Epoch 440/700: Train Loss=0.052182, Val Loss=0.434149\n",
      "  Epoch 445/700: Train Loss=0.049976, Val Loss=0.432839\n",
      "  Epoch 445/700: Train Loss=0.049976, Val Loss=0.432839\n",
      "  Epoch 450/700: Train Loss=0.050058, Val Loss=0.440643\n",
      "  Epoch 450/700: Train Loss=0.050058, Val Loss=0.440643\n",
      "  Epoch 455/700: Train Loss=0.048623, Val Loss=0.440247\n",
      "  Epoch 455/700: Train Loss=0.048623, Val Loss=0.440247\n",
      "  Epoch 460/700: Train Loss=0.046557, Val Loss=0.441013\n",
      "  Epoch 460/700: Train Loss=0.046557, Val Loss=0.441013\n",
      "  Epoch 465/700: Train Loss=0.046469, Val Loss=0.439331\n",
      "  Epoch 465/700: Train Loss=0.046469, Val Loss=0.439331\n",
      "  Epoch 470/700: Train Loss=0.046153, Val Loss=0.446323\n",
      "  Epoch 470/700: Train Loss=0.046153, Val Loss=0.446323\n",
      "  Epoch 475/700: Train Loss=0.044960, Val Loss=0.445030\n",
      "  Epoch 475/700: Train Loss=0.044960, Val Loss=0.445030\n",
      "  Epoch 480/700: Train Loss=0.043440, Val Loss=0.433219\n",
      "  Epoch 480/700: Train Loss=0.043440, Val Loss=0.433219\n",
      "  Epoch 485/700: Train Loss=0.043183, Val Loss=0.448054\n",
      "  Epoch 485/700: Train Loss=0.043183, Val Loss=0.448054\n",
      "  Epoch 490/700: Train Loss=0.043518, Val Loss=0.440516\n",
      "  Epoch 490/700: Train Loss=0.043518, Val Loss=0.440516\n",
      "  Epoch 495/700: Train Loss=0.042423, Val Loss=0.437411\n",
      "  Epoch 495/700: Train Loss=0.042423, Val Loss=0.437411\n",
      "  Epoch 500/700: Train Loss=0.041732, Val Loss=0.451831\n",
      "  Epoch 500/700: Train Loss=0.041732, Val Loss=0.451831\n",
      "  Epoch 505/700: Train Loss=0.041339, Val Loss=0.449805\n",
      "  Epoch 505/700: Train Loss=0.041339, Val Loss=0.449805\n",
      "  Epoch 510/700: Train Loss=0.040669, Val Loss=0.450579\n",
      "  Epoch 510/700: Train Loss=0.040669, Val Loss=0.450579\n",
      "  Epoch 515/700: Train Loss=0.040192, Val Loss=0.449730\n",
      "  Epoch 515/700: Train Loss=0.040192, Val Loss=0.449730\n",
      "  Epoch 520/700: Train Loss=0.039426, Val Loss=0.446468\n",
      "  Epoch 520/700: Train Loss=0.039426, Val Loss=0.446468\n",
      "  Epoch 525/700: Train Loss=0.037572, Val Loss=0.454774\n",
      "  Epoch 525/700: Train Loss=0.037572, Val Loss=0.454774\n",
      "  Epoch 530/700: Train Loss=0.038424, Val Loss=0.450886\n",
      "  Epoch 530/700: Train Loss=0.038424, Val Loss=0.450886\n",
      "  Epoch 535/700: Train Loss=0.038045, Val Loss=0.446592\n",
      "  Epoch 535/700: Train Loss=0.038045, Val Loss=0.446592\n",
      "  Epoch 540/700: Train Loss=0.037767, Val Loss=0.457409\n",
      "  Epoch 540/700: Train Loss=0.037767, Val Loss=0.457409\n",
      "  Epoch 545/700: Train Loss=0.037086, Val Loss=0.458258\n",
      "  Epoch 545/700: Train Loss=0.037086, Val Loss=0.458258\n",
      "  Epoch 550/700: Train Loss=0.034366, Val Loss=0.461983\n",
      "  Epoch 550/700: Train Loss=0.034366, Val Loss=0.461983\n",
      "  Epoch 555/700: Train Loss=0.034927, Val Loss=0.456648\n",
      "  Epoch 555/700: Train Loss=0.034927, Val Loss=0.456648\n",
      "  Epoch 560/700: Train Loss=0.034244, Val Loss=0.464118\n",
      "  Epoch 560/700: Train Loss=0.034244, Val Loss=0.464118\n",
      "  Epoch 565/700: Train Loss=0.034963, Val Loss=0.460329\n",
      "  Epoch 565/700: Train Loss=0.034963, Val Loss=0.460329\n",
      "  Epoch 570/700: Train Loss=0.035746, Val Loss=0.452513\n",
      "  Epoch 570/700: Train Loss=0.035746, Val Loss=0.452513\n",
      "  Epoch 575/700: Train Loss=0.033720, Val Loss=0.462236\n",
      "  Epoch 575/700: Train Loss=0.033720, Val Loss=0.462236\n",
      "  Epoch 580/700: Train Loss=0.034713, Val Loss=0.461394\n",
      "  Epoch 580/700: Train Loss=0.034713, Val Loss=0.461394\n",
      "  Epoch 585/700: Train Loss=0.032777, Val Loss=0.462675\n",
      "  Epoch 585/700: Train Loss=0.032777, Val Loss=0.462675\n",
      "  Epoch 590/700: Train Loss=0.032806, Val Loss=0.459650\n",
      "  Epoch 590/700: Train Loss=0.032806, Val Loss=0.459650\n",
      "  Epoch 595/700: Train Loss=0.032354, Val Loss=0.462457\n",
      "  Epoch 595/700: Train Loss=0.032354, Val Loss=0.462457\n",
      "  Epoch 600/700: Train Loss=0.031965, Val Loss=0.469650\n",
      "  Epoch 600/700: Train Loss=0.031965, Val Loss=0.469650\n",
      "  Epoch 605/700: Train Loss=0.030502, Val Loss=0.457125\n",
      "  Epoch 605/700: Train Loss=0.030502, Val Loss=0.457125\n",
      "  Epoch 610/700: Train Loss=0.030591, Val Loss=0.467758\n",
      "  Epoch 610/700: Train Loss=0.030591, Val Loss=0.467758\n",
      "  Epoch 615/700: Train Loss=0.031317, Val Loss=0.464682\n",
      "  Epoch 615/700: Train Loss=0.031317, Val Loss=0.464682\n",
      "  Epoch 620/700: Train Loss=0.029417, Val Loss=0.464699\n",
      "  Epoch 620/700: Train Loss=0.029417, Val Loss=0.464699\n",
      "  Epoch 625/700: Train Loss=0.030529, Val Loss=0.472836\n",
      "  Epoch 625/700: Train Loss=0.030529, Val Loss=0.472836\n",
      "  Epoch 630/700: Train Loss=0.029496, Val Loss=0.467075\n",
      "  Epoch 630/700: Train Loss=0.029496, Val Loss=0.467075\n",
      "  Epoch 635/700: Train Loss=0.028993, Val Loss=0.470904\n",
      "  Epoch 635/700: Train Loss=0.028993, Val Loss=0.470904\n",
      "  Epoch 640/700: Train Loss=0.028347, Val Loss=0.470253\n",
      "  Epoch 640/700: Train Loss=0.028347, Val Loss=0.470253\n",
      "  Epoch 645/700: Train Loss=0.028827, Val Loss=0.466853\n",
      "  Epoch 645/700: Train Loss=0.028827, Val Loss=0.466853\n",
      "  Epoch 650/700: Train Loss=0.026939, Val Loss=0.468870\n",
      "  Epoch 650/700: Train Loss=0.026939, Val Loss=0.468870\n",
      "  Epoch 655/700: Train Loss=0.027885, Val Loss=0.471993\n",
      "  Epoch 655/700: Train Loss=0.027885, Val Loss=0.471993\n",
      "  Epoch 660/700: Train Loss=0.027371, Val Loss=0.478117\n",
      "  Epoch 660/700: Train Loss=0.027371, Val Loss=0.478117\n",
      "  Epoch 665/700: Train Loss=0.027183, Val Loss=0.474822\n",
      "  Epoch 665/700: Train Loss=0.027183, Val Loss=0.474822\n",
      "  Epoch 670/700: Train Loss=0.025776, Val Loss=0.474468\n",
      "  Epoch 670/700: Train Loss=0.025776, Val Loss=0.474468\n",
      "  Epoch 675/700: Train Loss=0.026105, Val Loss=0.474824\n",
      "  Epoch 675/700: Train Loss=0.026105, Val Loss=0.474824\n",
      "  Epoch 680/700: Train Loss=0.027022, Val Loss=0.472348\n",
      "  Epoch 680/700: Train Loss=0.027022, Val Loss=0.472348\n",
      "  Epoch 685/700: Train Loss=0.024622, Val Loss=0.476679\n",
      "  Epoch 685/700: Train Loss=0.024622, Val Loss=0.476679\n",
      "  Epoch 690/700: Train Loss=0.024914, Val Loss=0.471665\n",
      "  Epoch 690/700: Train Loss=0.024914, Val Loss=0.471665\n",
      "  Epoch 695/700: Train Loss=0.025902, Val Loss=0.468128\n",
      "  Epoch 695/700: Train Loss=0.025902, Val Loss=0.468128\n",
      "  Epoch 700/700: Train Loss=0.025383, Val Loss=0.471787\n",
      "\n",
      "Step 3: Evaluating AutoEncoder...\n",
      "  Epoch 700/700: Train Loss=0.025383, Val Loss=0.471787\n",
      "\n",
      "Step 3: Evaluating AutoEncoder...\n",
      "\n",
      "AutoEncoder Final Results:\n",
      "  Best Hyperparams: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "  HP Search CV Score: 0.218080\n",
      "  Train Time: 103.58s\n",
      "  Final Train Loss: 0.025383\n",
      "  Final Val Loss: 0.471787\n",
      "  Avg MSE: 0.577670\n",
      "  Avg Reward: -55.782\n",
      "  Success Rate: 0.000\n",
      "  Model Parameters: 26,818\n",
      "\n",
      "\n",
      "AutoEncoder Final Results:\n",
      "  Best Hyperparams: {'activation': 'GELU', 'batch_norm': False, 'dropout': 0.0, 'hidden_dims': [128, 64], 'latent_dim': 64, 'lr': 0.001}\n",
      "  HP Search CV Score: 0.218080\n",
      "  Train Time: 103.58s\n",
      "  Final Train Loss: 0.025383\n",
      "  Final Val Loss: 0.471787\n",
      "  Avg MSE: 0.577670\n",
      "  Avg Reward: -55.782\n",
      "  Success Rate: 0.000\n",
      "  Model Parameters: 26,818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUTOENCODER: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "ae_best_hyperparams, ae_best_score, ae_search_results = hyperparameter_search(\n",
    "    'AutoEncoder', train_data, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest AutoEncoder hyperparameters:\")\n",
    "for key, value in ae_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {ae_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training AutoEncoder with best hyperparameters...\")\n",
    "autoencoder_agent = create_agent_with_hyperparams(\n",
    "    'AutoEncoder', ae_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train AutoEncoder with full epochs\n",
    "start_time = time.time()\n",
    "ae_train_losses, ae_val_losses = train_agent(\n",
    "    autoencoder_agent, train_data, val_data, \n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "ae_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating AutoEncoder...\")\n",
    "start_time = time.time()\n",
    "ae_results = evaluate_agent(autoencoder_agent, env, CONFIG['num_test_episodes'])\n",
    "ae_test_time = time.time() - start_time\n",
    "\n",
    "# Store results with hyperparameter information\n",
    "results['algorithm'].append('AutoEncoder')\n",
    "results['train_time'].append(ae_train_time)\n",
    "results['test_time'].append(ae_test_time)\n",
    "results['final_train_loss'].append(ae_train_losses[-1])\n",
    "results['final_val_loss'].append(ae_val_losses[-1])\n",
    "results['avg_mse'].append(ae_results['avg_mse'])\n",
    "results['avg_reward'].append(ae_results['avg_reward'])\n",
    "results['success_rate'].append(ae_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(autoencoder_agent))\n",
    "results['best_hyperparams'].append(ae_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(ae_best_score)\n",
    "\n",
    "print(f\"\\nAutoEncoder Final Results:\")\n",
    "print(f\"  Best Hyperparams: {ae_best_hyperparams}\")\n",
    "print(f\"  HP Search CV Score: {ae_best_score:.6f}\")\n",
    "print(f\"  Train Time: {ae_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {ae_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {ae_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {ae_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {ae_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {ae_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(autoencoder_agent):,}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffcf49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder model saved to trained_models/autoencoder_model_20251010_201223.pt\n",
      "Hyperparameters saved to trained_models/autoencoder_hyperparams_20251010_201223.json\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "# Save the trained AutoEncoder model\n",
    "model_dir = \"trained_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, f\"autoencoder_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "\n",
    "# Save model state dictionary\n",
    "if hasattr(autoencoder_agent, 'model'):\n",
    "    torch.save(autoencoder_agent.model.state_dict(), model_path)\n",
    "    print(f\"AutoEncoder model saved to {model_path}\")\n",
    "else:\n",
    "    print(\"Could not save model: model attribute not found\")\n",
    "\n",
    "# Also save hyperparameters for reproducibility\n",
    "hyperparams_path = os.path.join(model_dir, f\"autoencoder_hyperparams_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(hyperparams_path, 'w') as f:\n",
    "    json.dump(ae_best_hyperparams, f, indent=2)\n",
    "    print(f\"Hyperparameters saved to {hyperparams_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cee6b",
   "metadata": {},
   "source": [
    "## Bayesian: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BAYESIAN: HYPERPARAMETER TUNING & TRAINING\n",
      "============================================================\n",
      "Step 1: Hyperparameter Tuning\n",
      "Starting hyperparameter search for Bayesian...\n",
      "  Evaluating 81 hyperparameter combinations...\n",
      "  Too many combinations (81), using random sampling of 20\n",
      "    Trial 1/20: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.01, 'prior_std': 0.5}\n",
      "      CV Score: 0.233280 (±0.018168)\n",
      "    Trial 2/20: {'kl_weight': 0.01, 'latent_dim': 128, 'lr': 0.01, 'prior_std': 1.0}\n",
      "      CV Score: 0.233280 (±0.018168)\n",
      "    Trial 2/20: {'kl_weight': 0.01, 'latent_dim': 128, 'lr': 0.01, 'prior_std': 1.0}\n",
      "      CV Score: 0.237726 (±0.017422)\n",
      "    Trial 3/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.237726 (±0.017422)\n",
      "    Trial 3/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.230081 (±0.018930)\n",
      "    Trial 4/20: {'kl_weight': 0.1, 'latent_dim': 128, 'lr': 0.001, 'prior_std': 0.5}\n",
      "      CV Score: 0.230081 (±0.018930)\n",
      "    Trial 4/20: {'kl_weight': 0.1, 'latent_dim': 128, 'lr': 0.001, 'prior_std': 0.5}\n",
      "      CV Score: 0.229387 (±0.020737)\n",
      "    Trial 5/20: {'kl_weight': 0.001, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.229387 (±0.020737)\n",
      "    Trial 5/20: {'kl_weight': 0.001, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.228810 (±0.019551)\n",
      "    Trial 6/20: {'kl_weight': 0.1, 'latent_dim': 32, 'lr': 0.001, 'prior_std': 2.0}\n",
      "      CV Score: 0.228810 (±0.019551)\n",
      "    Trial 6/20: {'kl_weight': 0.1, 'latent_dim': 32, 'lr': 0.001, 'prior_std': 2.0}\n",
      "      CV Score: 0.229200 (±0.020304)\n",
      "    Trial 7/20: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.229200 (±0.020304)\n",
      "    Trial 7/20: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.228442 (±0.019395)\n",
      "    Trial 8/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.01, 'prior_std': 2.0}\n",
      "      CV Score: 0.228442 (±0.019395)\n",
      "    Trial 8/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.01, 'prior_std': 2.0}\n",
      "      CV Score: 0.231174 (±0.021261)\n",
      "    Trial 9/20: {'kl_weight': 0.001, 'latent_dim': 32, 'lr': 0.001, 'prior_std': 2.0}\n",
      "      CV Score: 0.231174 (±0.021261)\n",
      "    Trial 9/20: {'kl_weight': 0.001, 'latent_dim': 32, 'lr': 0.001, 'prior_std': 2.0}\n",
      "      CV Score: 0.229571 (±0.021100)\n",
      "    Trial 10/20: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.229571 (±0.021100)\n",
      "    Trial 10/20: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.228951 (±0.018552)\n",
      "    Trial 11/20: {'kl_weight': 0.1, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.228951 (±0.018552)\n",
      "    Trial 11/20: {'kl_weight': 0.1, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.228556 (±0.020184)\n",
      "    Trial 12/20: {'kl_weight': 0.01, 'latent_dim': 128, 'lr': 0.01, 'prior_std': 0.5}\n",
      "      CV Score: 0.228556 (±0.020184)\n",
      "    Trial 12/20: {'kl_weight': 0.01, 'latent_dim': 128, 'lr': 0.01, 'prior_std': 0.5}\n",
      "      CV Score: 0.234945 (±0.018502)\n",
      "    Trial 13/20: {'kl_weight': 0.1, 'latent_dim': 128, 'lr': 0.0001, 'prior_std': 2.0}\n",
      "      CV Score: 0.234945 (±0.018502)\n",
      "    Trial 13/20: {'kl_weight': 0.1, 'latent_dim': 128, 'lr': 0.0001, 'prior_std': 2.0}\n",
      "      CV Score: 0.238749 (±0.013233)\n",
      "    Trial 14/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.001, 'prior_std': 0.5}\n",
      "      CV Score: 0.238749 (±0.013233)\n",
      "    Trial 14/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.001, 'prior_std': 0.5}\n",
      "      CV Score: 0.230643 (±0.019647)\n",
      "    Trial 15/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.230643 (±0.019647)\n",
      "    Trial 15/20: {'kl_weight': 0.01, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.229999 (±0.021233)\n",
      "    Trial 16/20: {'kl_weight': 0.1, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 2.0}\n",
      "      CV Score: 0.229999 (±0.021233)\n",
      "    Trial 16/20: {'kl_weight': 0.1, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 2.0}\n",
      "      CV Score: 0.228793 (±0.022237)\n",
      "    Trial 17/20: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.01, 'prior_std': 1.0}\n",
      "      CV Score: 0.228793 (±0.022237)\n",
      "    Trial 17/20: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.01, 'prior_std': 1.0}\n",
      "      CV Score: 0.232618 (±0.019896)\n",
      "    Trial 18/20: {'kl_weight': 0.1, 'latent_dim': 32, 'lr': 0.01, 'prior_std': 0.5}\n",
      "      CV Score: 0.232618 (±0.019896)\n",
      "    Trial 18/20: {'kl_weight': 0.1, 'latent_dim': 32, 'lr': 0.01, 'prior_std': 0.5}\n",
      "      CV Score: 0.229696 (±0.020117)\n",
      "    Trial 19/20: {'kl_weight': 0.001, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.229696 (±0.020117)\n",
      "    Trial 19/20: {'kl_weight': 0.001, 'latent_dim': 32, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "      CV Score: 0.228800 (±0.020460)\n",
      "    Trial 20/20: {'kl_weight': 0.001, 'latent_dim': 128, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.228800 (±0.020460)\n",
      "    Trial 20/20: {'kl_weight': 0.001, 'latent_dim': 128, 'lr': 0.0001, 'prior_std': 0.5}\n",
      "      CV Score: 0.243318 (±0.018108)\n",
      "  Best hyperparameters found with CV score: 0.228442\n",
      "  Best hyperparams: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "\n",
      "Best Bayesian hyperparameters:\n",
      "  kl_weight: 0.01\n",
      "  latent_dim: 64\n",
      "  lr: 0.0001\n",
      "  prior_std: 1.0\n",
      "Best CV score: 0.228442\n",
      "\n",
      "Step 2: Training Bayesian with best hyperparameters...\n",
      "Training for 700 epochs with 73 batches per epoch...\n",
      "      CV Score: 0.243318 (±0.018108)\n",
      "  Best hyperparameters found with CV score: 0.228442\n",
      "  Best hyperparams: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "\n",
      "Best Bayesian hyperparameters:\n",
      "  kl_weight: 0.01\n",
      "  latent_dim: 64\n",
      "  lr: 0.0001\n",
      "  prior_std: 1.0\n",
      "Best CV score: 0.228442\n",
      "\n",
      "Step 2: Training Bayesian with best hyperparameters...\n",
      "Training for 700 epochs with 73 batches per epoch...\n",
      "  Epoch 5/700: Train Loss=737.947896, Val Loss=0.198282\n",
      "  Epoch 5/700: Train Loss=737.947896, Val Loss=0.198282\n",
      "  Epoch 10/700: Train Loss=734.394667, Val Loss=0.201479\n",
      "  Epoch 10/700: Train Loss=734.394667, Val Loss=0.201479\n",
      "  Epoch 15/700: Train Loss=730.878227, Val Loss=0.201228\n",
      "  Epoch 15/700: Train Loss=730.878227, Val Loss=0.201228\n",
      "  Epoch 20/700: Train Loss=727.386791, Val Loss=0.204932\n",
      "  Epoch 20/700: Train Loss=727.386791, Val Loss=0.204932\n",
      "  Epoch 25/700: Train Loss=723.911471, Val Loss=0.199479\n",
      "  Epoch 25/700: Train Loss=723.911471, Val Loss=0.199479\n",
      "  Epoch 30/700: Train Loss=720.447170, Val Loss=0.196779\n",
      "  Epoch 30/700: Train Loss=720.447170, Val Loss=0.196779\n",
      "  Epoch 35/700: Train Loss=716.993419, Val Loss=0.197803\n",
      "  Epoch 35/700: Train Loss=716.993419, Val Loss=0.197803\n",
      "  Epoch 40/700: Train Loss=713.546626, Val Loss=0.201003\n",
      "  Epoch 40/700: Train Loss=713.546626, Val Loss=0.201003\n",
      "  Epoch 45/700: Train Loss=710.106140, Val Loss=0.197106\n",
      "  Epoch 45/700: Train Loss=710.106140, Val Loss=0.197106\n",
      "  Epoch 50/700: Train Loss=706.671089, Val Loss=0.199279\n",
      "  Epoch 50/700: Train Loss=706.671089, Val Loss=0.199279\n",
      "  Epoch 55/700: Train Loss=703.242288, Val Loss=0.200048\n",
      "  Epoch 55/700: Train Loss=703.242288, Val Loss=0.200048\n",
      "  Epoch 60/700: Train Loss=699.816293, Val Loss=0.199439\n",
      "  Epoch 60/700: Train Loss=699.816293, Val Loss=0.199439\n",
      "  Epoch 65/700: Train Loss=696.393340, Val Loss=0.198242\n",
      "  Epoch 65/700: Train Loss=696.393340, Val Loss=0.198242\n",
      "  Epoch 70/700: Train Loss=692.973614, Val Loss=0.197782\n",
      "  Epoch 70/700: Train Loss=692.973614, Val Loss=0.197782\n",
      "  Epoch 75/700: Train Loss=689.560579, Val Loss=0.199985\n",
      "  Epoch 75/700: Train Loss=689.560579, Val Loss=0.199985\n",
      "  Epoch 80/700: Train Loss=686.147413, Val Loss=0.198578\n",
      "  Epoch 80/700: Train Loss=686.147413, Val Loss=0.198578\n",
      "  Epoch 85/700: Train Loss=682.737683, Val Loss=0.200786\n",
      "  Epoch 85/700: Train Loss=682.737683, Val Loss=0.200786\n",
      "  Epoch 90/700: Train Loss=679.329609, Val Loss=0.200104\n",
      "  Epoch 90/700: Train Loss=679.329609, Val Loss=0.200104\n",
      "  Epoch 95/700: Train Loss=675.925648, Val Loss=0.199644\n",
      "  Epoch 95/700: Train Loss=675.925648, Val Loss=0.199644\n",
      "  Epoch 100/700: Train Loss=672.520627, Val Loss=0.201470\n",
      "  Epoch 100/700: Train Loss=672.520627, Val Loss=0.201470\n",
      "  Epoch 105/700: Train Loss=669.119170, Val Loss=0.200973\n",
      "  Epoch 105/700: Train Loss=669.119170, Val Loss=0.200973\n",
      "  Epoch 110/700: Train Loss=665.717839, Val Loss=0.198954\n",
      "  Epoch 110/700: Train Loss=665.717839, Val Loss=0.198954\n",
      "  Epoch 115/700: Train Loss=662.316365, Val Loss=0.199442\n",
      "  Epoch 115/700: Train Loss=662.316365, Val Loss=0.199442\n",
      "  Epoch 120/700: Train Loss=658.922467, Val Loss=0.202388\n",
      "  Epoch 120/700: Train Loss=658.922467, Val Loss=0.202388\n",
      "  Epoch 125/700: Train Loss=655.520720, Val Loss=0.197993\n",
      "  Epoch 125/700: Train Loss=655.520720, Val Loss=0.197993\n",
      "  Epoch 130/700: Train Loss=652.123181, Val Loss=0.199825\n",
      "  Epoch 130/700: Train Loss=652.123181, Val Loss=0.199825\n",
      "  Epoch 135/700: Train Loss=648.731080, Val Loss=0.198290\n",
      "  Epoch 135/700: Train Loss=648.731080, Val Loss=0.198290\n",
      "  Epoch 140/700: Train Loss=645.337003, Val Loss=0.202195\n",
      "  Epoch 140/700: Train Loss=645.337003, Val Loss=0.202195\n",
      "  Epoch 145/700: Train Loss=641.941666, Val Loss=0.201472\n",
      "  Epoch 145/700: Train Loss=641.941666, Val Loss=0.201472\n",
      "  Epoch 150/700: Train Loss=638.546687, Val Loss=0.201682\n",
      "  Epoch 150/700: Train Loss=638.546687, Val Loss=0.201682\n",
      "  Epoch 155/700: Train Loss=635.153530, Val Loss=0.200460\n",
      "  Epoch 155/700: Train Loss=635.153530, Val Loss=0.200460\n",
      "  Epoch 160/700: Train Loss=631.766667, Val Loss=0.201485\n",
      "  Epoch 160/700: Train Loss=631.766667, Val Loss=0.201485\n",
      "  Epoch 165/700: Train Loss=628.383240, Val Loss=0.202125\n",
      "  Epoch 165/700: Train Loss=628.383240, Val Loss=0.202125\n",
      "  Epoch 170/700: Train Loss=624.985086, Val Loss=0.201402\n",
      "  Epoch 170/700: Train Loss=624.985086, Val Loss=0.201402\n",
      "  Epoch 175/700: Train Loss=621.599329, Val Loss=0.199732\n",
      "  Epoch 175/700: Train Loss=621.599329, Val Loss=0.199732\n",
      "  Epoch 180/700: Train Loss=618.208284, Val Loss=0.200216\n",
      "  Epoch 180/700: Train Loss=618.208284, Val Loss=0.200216\n",
      "  Epoch 185/700: Train Loss=614.818894, Val Loss=0.201990\n",
      "  Epoch 185/700: Train Loss=614.818894, Val Loss=0.201990\n",
      "  Epoch 190/700: Train Loss=611.428332, Val Loss=0.199526\n",
      "  Epoch 190/700: Train Loss=611.428332, Val Loss=0.199526\n",
      "  Epoch 195/700: Train Loss=608.049055, Val Loss=0.200634\n",
      "  Epoch 195/700: Train Loss=608.049055, Val Loss=0.200634\n",
      "  Epoch 200/700: Train Loss=604.655980, Val Loss=0.204468\n",
      "  Epoch 200/700: Train Loss=604.655980, Val Loss=0.204468\n",
      "  Epoch 205/700: Train Loss=601.273663, Val Loss=0.201392\n",
      "  Epoch 205/700: Train Loss=601.273663, Val Loss=0.201392\n",
      "  Epoch 210/700: Train Loss=597.886728, Val Loss=0.199213\n",
      "  Epoch 210/700: Train Loss=597.886728, Val Loss=0.199213\n",
      "  Epoch 215/700: Train Loss=594.499318, Val Loss=0.202149\n",
      "  Epoch 215/700: Train Loss=594.499318, Val Loss=0.202149\n",
      "  Epoch 220/700: Train Loss=591.115354, Val Loss=0.200843\n",
      "  Epoch 220/700: Train Loss=591.115354, Val Loss=0.200843\n",
      "  Epoch 225/700: Train Loss=587.727386, Val Loss=0.199125\n",
      "  Epoch 225/700: Train Loss=587.727386, Val Loss=0.199125\n",
      "  Epoch 230/700: Train Loss=584.345140, Val Loss=0.199718\n",
      "  Epoch 230/700: Train Loss=584.345140, Val Loss=0.199718\n",
      "  Epoch 235/700: Train Loss=580.963466, Val Loss=0.199656\n",
      "  Epoch 235/700: Train Loss=580.963466, Val Loss=0.199656\n",
      "  Epoch 240/700: Train Loss=577.583098, Val Loss=0.201197\n",
      "  Epoch 240/700: Train Loss=577.583098, Val Loss=0.201197\n",
      "  Epoch 245/700: Train Loss=574.194950, Val Loss=0.201996\n",
      "  Epoch 245/700: Train Loss=574.194950, Val Loss=0.201996\n",
      "  Epoch 250/700: Train Loss=570.814017, Val Loss=0.199642\n",
      "  Epoch 250/700: Train Loss=570.814017, Val Loss=0.199642\n",
      "  Epoch 255/700: Train Loss=567.433069, Val Loss=0.199735\n",
      "  Epoch 255/700: Train Loss=567.433069, Val Loss=0.199735\n",
      "  Epoch 260/700: Train Loss=564.053739, Val Loss=0.201169\n",
      "  Epoch 260/700: Train Loss=564.053739, Val Loss=0.201169\n",
      "  Epoch 265/700: Train Loss=560.671991, Val Loss=0.201162\n",
      "  Epoch 265/700: Train Loss=560.671991, Val Loss=0.201162\n",
      "  Epoch 270/700: Train Loss=557.288386, Val Loss=0.200964\n",
      "  Epoch 270/700: Train Loss=557.288386, Val Loss=0.200964\n",
      "  Epoch 275/700: Train Loss=553.913378, Val Loss=0.199568\n",
      "  Epoch 275/700: Train Loss=553.913378, Val Loss=0.199568\n",
      "  Epoch 280/700: Train Loss=550.535526, Val Loss=0.201035\n",
      "  Epoch 280/700: Train Loss=550.535526, Val Loss=0.201035\n",
      "  Epoch 285/700: Train Loss=547.162785, Val Loss=0.202989\n",
      "  Epoch 285/700: Train Loss=547.162785, Val Loss=0.202989\n",
      "  Epoch 290/700: Train Loss=543.781172, Val Loss=0.203227\n",
      "  Epoch 290/700: Train Loss=543.781172, Val Loss=0.203227\n",
      "  Epoch 295/700: Train Loss=540.406244, Val Loss=0.201622\n",
      "  Epoch 295/700: Train Loss=540.406244, Val Loss=0.201622\n",
      "  Epoch 300/700: Train Loss=537.024792, Val Loss=0.201850\n",
      "  Epoch 300/700: Train Loss=537.024792, Val Loss=0.201850\n",
      "  Epoch 305/700: Train Loss=533.657777, Val Loss=0.200830\n",
      "  Epoch 305/700: Train Loss=533.657777, Val Loss=0.200830\n",
      "  Epoch 310/700: Train Loss=530.281151, Val Loss=0.203520\n",
      "  Epoch 310/700: Train Loss=530.281151, Val Loss=0.203520\n",
      "  Epoch 315/700: Train Loss=526.910646, Val Loss=0.201346\n",
      "  Epoch 315/700: Train Loss=526.910646, Val Loss=0.201346\n",
      "  Epoch 320/700: Train Loss=523.533452, Val Loss=0.201479\n",
      "  Epoch 320/700: Train Loss=523.533452, Val Loss=0.201479\n",
      "  Epoch 325/700: Train Loss=520.158587, Val Loss=0.202753\n",
      "  Epoch 325/700: Train Loss=520.158587, Val Loss=0.202753\n",
      "  Epoch 330/700: Train Loss=516.786758, Val Loss=0.202331\n",
      "  Epoch 330/700: Train Loss=516.786758, Val Loss=0.202331\n",
      "  Epoch 335/700: Train Loss=513.417661, Val Loss=0.201508\n",
      "  Epoch 335/700: Train Loss=513.417661, Val Loss=0.201508\n",
      "  Epoch 340/700: Train Loss=510.049152, Val Loss=0.201078\n",
      "  Epoch 340/700: Train Loss=510.049152, Val Loss=0.201078\n",
      "  Epoch 345/700: Train Loss=506.680101, Val Loss=0.204089\n",
      "  Epoch 345/700: Train Loss=506.680101, Val Loss=0.204089\n",
      "  Epoch 350/700: Train Loss=503.300151, Val Loss=0.201784\n",
      "  Epoch 350/700: Train Loss=503.300151, Val Loss=0.201784\n",
      "  Epoch 355/700: Train Loss=499.936480, Val Loss=0.201361\n",
      "  Epoch 355/700: Train Loss=499.936480, Val Loss=0.201361\n",
      "  Epoch 360/700: Train Loss=496.566706, Val Loss=0.201199\n",
      "  Epoch 360/700: Train Loss=496.566706, Val Loss=0.201199\n",
      "  Epoch 365/700: Train Loss=493.210743, Val Loss=0.202573\n",
      "  Epoch 365/700: Train Loss=493.210743, Val Loss=0.202573\n",
      "  Epoch 370/700: Train Loss=489.831880, Val Loss=0.202382\n",
      "  Epoch 370/700: Train Loss=489.831880, Val Loss=0.202382\n",
      "  Epoch 375/700: Train Loss=486.463433, Val Loss=0.203812\n",
      "  Epoch 375/700: Train Loss=486.463433, Val Loss=0.203812\n",
      "  Epoch 380/700: Train Loss=483.095144, Val Loss=0.199673\n",
      "  Epoch 380/700: Train Loss=483.095144, Val Loss=0.199673\n",
      "  Epoch 385/700: Train Loss=479.729065, Val Loss=0.202793\n",
      "  Epoch 385/700: Train Loss=479.729065, Val Loss=0.202793\n",
      "  Epoch 390/700: Train Loss=476.372200, Val Loss=0.200003\n",
      "  Epoch 390/700: Train Loss=476.372200, Val Loss=0.200003\n",
      "  Epoch 395/700: Train Loss=473.003025, Val Loss=0.202270\n",
      "  Epoch 395/700: Train Loss=473.003025, Val Loss=0.202270\n",
      "  Epoch 400/700: Train Loss=469.633513, Val Loss=0.202247\n",
      "  Epoch 400/700: Train Loss=469.633513, Val Loss=0.202247\n",
      "  Epoch 405/700: Train Loss=466.277593, Val Loss=0.200369\n",
      "  Epoch 405/700: Train Loss=466.277593, Val Loss=0.200369\n",
      "  Epoch 410/700: Train Loss=462.914596, Val Loss=0.201842\n",
      "  Epoch 410/700: Train Loss=462.914596, Val Loss=0.201842\n",
      "  Epoch 415/700: Train Loss=459.550850, Val Loss=0.202243\n",
      "  Epoch 415/700: Train Loss=459.550850, Val Loss=0.202243\n",
      "  Epoch 420/700: Train Loss=456.179499, Val Loss=0.205371\n",
      "  Epoch 420/700: Train Loss=456.179499, Val Loss=0.205371\n",
      "  Epoch 425/700: Train Loss=452.822634, Val Loss=0.204515\n",
      "  Epoch 425/700: Train Loss=452.822634, Val Loss=0.204515\n",
      "  Epoch 430/700: Train Loss=449.459251, Val Loss=0.205063\n",
      "  Epoch 430/700: Train Loss=449.459251, Val Loss=0.205063\n",
      "  Epoch 435/700: Train Loss=446.098354, Val Loss=0.205587\n",
      "  Epoch 435/700: Train Loss=446.098354, Val Loss=0.205587\n",
      "  Epoch 440/700: Train Loss=442.734700, Val Loss=0.202067\n",
      "  Epoch 440/700: Train Loss=442.734700, Val Loss=0.202067\n",
      "  Epoch 445/700: Train Loss=439.384923, Val Loss=0.202134\n",
      "  Epoch 445/700: Train Loss=439.384923, Val Loss=0.202134\n",
      "  Epoch 450/700: Train Loss=436.019962, Val Loss=0.204270\n",
      "  Epoch 450/700: Train Loss=436.019962, Val Loss=0.204270\n",
      "  Epoch 455/700: Train Loss=432.668763, Val Loss=0.203057\n",
      "  Epoch 455/700: Train Loss=432.668763, Val Loss=0.203057\n",
      "  Epoch 460/700: Train Loss=429.323917, Val Loss=0.205786\n",
      "  Epoch 460/700: Train Loss=429.323917, Val Loss=0.205786\n",
      "  Epoch 465/700: Train Loss=425.953264, Val Loss=0.202966\n",
      "  Epoch 465/700: Train Loss=425.953264, Val Loss=0.202966\n",
      "  Epoch 470/700: Train Loss=422.607566, Val Loss=0.207642\n",
      "  Epoch 470/700: Train Loss=422.607566, Val Loss=0.207642\n",
      "  Epoch 475/700: Train Loss=419.244528, Val Loss=0.202982\n",
      "  Epoch 475/700: Train Loss=419.244528, Val Loss=0.202982\n",
      "  Epoch 480/700: Train Loss=415.904333, Val Loss=0.207991\n",
      "  Epoch 480/700: Train Loss=415.904333, Val Loss=0.207991\n",
      "  Epoch 485/700: Train Loss=412.552683, Val Loss=0.207569\n",
      "  Epoch 485/700: Train Loss=412.552683, Val Loss=0.207569\n",
      "  Epoch 490/700: Train Loss=409.200775, Val Loss=0.203986\n",
      "  Epoch 490/700: Train Loss=409.200775, Val Loss=0.203986\n",
      "  Epoch 495/700: Train Loss=405.853102, Val Loss=0.207223\n",
      "  Epoch 495/700: Train Loss=405.853102, Val Loss=0.207223\n",
      "  Epoch 500/700: Train Loss=402.530790, Val Loss=0.203541\n",
      "  Epoch 500/700: Train Loss=402.530790, Val Loss=0.203541\n",
      "  Epoch 505/700: Train Loss=399.155253, Val Loss=0.208267\n",
      "  Epoch 505/700: Train Loss=399.155253, Val Loss=0.208267\n",
      "  Epoch 510/700: Train Loss=395.820468, Val Loss=0.203922\n",
      "  Epoch 510/700: Train Loss=395.820468, Val Loss=0.203922\n",
      "  Epoch 515/700: Train Loss=392.482079, Val Loss=0.204101\n",
      "  Epoch 515/700: Train Loss=392.482079, Val Loss=0.204101\n",
      "  Epoch 520/700: Train Loss=389.136140, Val Loss=0.205210\n",
      "  Epoch 520/700: Train Loss=389.136140, Val Loss=0.205210\n",
      "  Epoch 525/700: Train Loss=385.805783, Val Loss=0.207236\n",
      "  Epoch 525/700: Train Loss=385.805783, Val Loss=0.207236\n",
      "  Epoch 530/700: Train Loss=382.483635, Val Loss=0.208386\n",
      "  Epoch 530/700: Train Loss=382.483635, Val Loss=0.208386\n",
      "  Epoch 535/700: Train Loss=379.145335, Val Loss=0.201898\n",
      "  Epoch 535/700: Train Loss=379.145335, Val Loss=0.201898\n",
      "  Epoch 540/700: Train Loss=375.802627, Val Loss=0.205895\n",
      "  Epoch 540/700: Train Loss=375.802627, Val Loss=0.205895\n",
      "  Epoch 545/700: Train Loss=372.508748, Val Loss=0.204589\n",
      "  Epoch 545/700: Train Loss=372.508748, Val Loss=0.204589\n",
      "  Epoch 550/700: Train Loss=369.165734, Val Loss=0.213085\n",
      "  Epoch 550/700: Train Loss=369.165734, Val Loss=0.213085\n",
      "  Epoch 555/700: Train Loss=365.841814, Val Loss=0.208869\n",
      "  Epoch 555/700: Train Loss=365.841814, Val Loss=0.208869\n",
      "  Epoch 560/700: Train Loss=362.544415, Val Loss=0.203934\n",
      "  Epoch 560/700: Train Loss=362.544415, Val Loss=0.203934\n",
      "  Epoch 565/700: Train Loss=359.218221, Val Loss=0.210827\n",
      "  Epoch 565/700: Train Loss=359.218221, Val Loss=0.210827\n",
      "  Epoch 570/700: Train Loss=355.918700, Val Loss=0.203532\n",
      "  Epoch 570/700: Train Loss=355.918700, Val Loss=0.203532\n",
      "  Epoch 575/700: Train Loss=352.616916, Val Loss=0.209229\n",
      "  Epoch 575/700: Train Loss=352.616916, Val Loss=0.209229\n",
      "  Epoch 580/700: Train Loss=349.334607, Val Loss=0.210627\n",
      "  Epoch 580/700: Train Loss=349.334607, Val Loss=0.210627\n",
      "  Epoch 585/700: Train Loss=346.049324, Val Loss=0.206422\n",
      "  Epoch 585/700: Train Loss=346.049324, Val Loss=0.206422\n",
      "  Epoch 590/700: Train Loss=342.731925, Val Loss=0.211640\n",
      "  Epoch 590/700: Train Loss=342.731925, Val Loss=0.211640\n",
      "  Epoch 595/700: Train Loss=339.460271, Val Loss=0.211724\n",
      "  Epoch 595/700: Train Loss=339.460271, Val Loss=0.211724\n",
      "  Epoch 600/700: Train Loss=336.191420, Val Loss=0.213138\n",
      "  Epoch 600/700: Train Loss=336.191420, Val Loss=0.213138\n",
      "  Epoch 605/700: Train Loss=332.925027, Val Loss=0.212250\n",
      "  Epoch 605/700: Train Loss=332.925027, Val Loss=0.212250\n",
      "  Epoch 610/700: Train Loss=329.689556, Val Loss=0.214866\n",
      "  Epoch 610/700: Train Loss=329.689556, Val Loss=0.214866\n",
      "  Epoch 615/700: Train Loss=326.431220, Val Loss=0.216442\n",
      "  Epoch 615/700: Train Loss=326.431220, Val Loss=0.216442\n",
      "  Epoch 620/700: Train Loss=323.164414, Val Loss=0.206330\n",
      "  Epoch 620/700: Train Loss=323.164414, Val Loss=0.206330\n",
      "  Epoch 625/700: Train Loss=319.944469, Val Loss=0.211768\n",
      "  Epoch 625/700: Train Loss=319.944469, Val Loss=0.211768\n",
      "  Epoch 630/700: Train Loss=316.706983, Val Loss=0.214640\n",
      "  Epoch 630/700: Train Loss=316.706983, Val Loss=0.214640\n",
      "  Epoch 635/700: Train Loss=313.485129, Val Loss=0.211427\n",
      "  Epoch 635/700: Train Loss=313.485129, Val Loss=0.211427\n",
      "  Epoch 640/700: Train Loss=310.289296, Val Loss=0.212241\n",
      "  Epoch 640/700: Train Loss=310.289296, Val Loss=0.212241\n",
      "  Epoch 645/700: Train Loss=307.047487, Val Loss=0.204612\n",
      "  Epoch 645/700: Train Loss=307.047487, Val Loss=0.204612\n",
      "  Epoch 650/700: Train Loss=303.869458, Val Loss=0.209489\n",
      "  Epoch 650/700: Train Loss=303.869458, Val Loss=0.209489\n",
      "  Epoch 655/700: Train Loss=300.638057, Val Loss=0.216577\n",
      "  Epoch 655/700: Train Loss=300.638057, Val Loss=0.216577\n",
      "  Epoch 660/700: Train Loss=297.485020, Val Loss=0.204845\n",
      "  Epoch 660/700: Train Loss=297.485020, Val Loss=0.204845\n",
      "  Epoch 665/700: Train Loss=294.355658, Val Loss=0.210420\n",
      "  Epoch 665/700: Train Loss=294.355658, Val Loss=0.210420\n",
      "  Epoch 670/700: Train Loss=291.156839, Val Loss=0.212518\n",
      "  Epoch 670/700: Train Loss=291.156839, Val Loss=0.212518\n",
      "  Epoch 675/700: Train Loss=287.986853, Val Loss=0.210509\n",
      "  Epoch 675/700: Train Loss=287.986853, Val Loss=0.210509\n",
      "  Epoch 680/700: Train Loss=284.844469, Val Loss=0.208031\n",
      "  Epoch 680/700: Train Loss=284.844469, Val Loss=0.208031\n",
      "  Epoch 685/700: Train Loss=281.760490, Val Loss=0.215383\n",
      "  Epoch 685/700: Train Loss=281.760490, Val Loss=0.215383\n",
      "  Epoch 690/700: Train Loss=278.657480, Val Loss=0.209394\n",
      "  Epoch 690/700: Train Loss=278.657480, Val Loss=0.209394\n",
      "  Epoch 695/700: Train Loss=275.565671, Val Loss=0.211714\n",
      "  Epoch 695/700: Train Loss=275.565671, Val Loss=0.211714\n",
      "  Epoch 700/700: Train Loss=272.481440, Val Loss=0.217698\n",
      "\n",
      "Step 3: Evaluating Bayesian...\n",
      "  Epoch 700/700: Train Loss=272.481440, Val Loss=0.217698\n",
      "\n",
      "Step 3: Evaluating Bayesian...\n",
      "\n",
      "Bayesian Final Results:\n",
      "  Best Hyperparams: {'kl_weight': 0.01, 'latent_dim': 64, 'lr': 0.0001, 'prior_std': 1.0}\n",
      "  HP Search CV Score: 0.228442\n",
      "  Train Time: 1837.25s\n",
      "  Final Train Loss: 272.481440\n",
      "  Final Val Loss: 0.217698\n",
      "  Avg MSE: 0.221728\n",
      "  Avg Reward: -56.688\n",
      "  Success Rate: 0.000\n",
      "  Model Parameters: 36,998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BAYESIAN: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "bay_best_hyperparams, bay_best_score, bay_search_results = hyperparameter_search(\n",
    "    'Bayesian', train_data, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Bayesian hyperparameters:\")\n",
    "for key, value in bay_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {bay_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training Bayesian with best hyperparameters...\")\n",
    "bayesian_agent = create_agent_with_hyperparams(\n",
    "    'Bayesian', bay_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train Bayesian with full epochs\n",
    "start_time = time.time()\n",
    "bay_train_losses, bay_val_losses = train_agent(\n",
    "    bayesian_agent, train_data, val_data,\n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "bay_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating Bayesian...\")\n",
    "start_time = time.time()\n",
    "bay_results = evaluate_agent(bayesian_agent, env, CONFIG['num_test_episodes'])\n",
    "bay_test_time = time.time() - start_time\n",
    "\n",
    "# Store results with hyperparameter information\n",
    "results['algorithm'].append('Bayesian')\n",
    "results['train_time'].append(bay_train_time)\n",
    "results['test_time'].append(bay_test_time)\n",
    "results['final_train_loss'].append(bay_train_losses[-1])\n",
    "results['final_val_loss'].append(bay_val_losses[-1])\n",
    "results['avg_mse'].append(bay_results['avg_mse'])\n",
    "results['avg_reward'].append(bay_results['avg_reward'])\n",
    "results['success_rate'].append(bay_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(bayesian_agent))\n",
    "results['best_hyperparams'].append(bay_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(bay_best_score)\n",
    "\n",
    "print(f\"\\nBayesian Final Results:\")\n",
    "print(f\"  Best Hyperparams: {bay_best_hyperparams}\")1\n",
    "print(f\"  HP Search CV Score: {bay_best_score:.6f}\")\n",
    "print(f\"  Train Time: {bay_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {bay_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {bay_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {bay_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {bay_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {bay_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(bayesian_agent):,}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd3cc541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_models/bayesian_model_20251010_201336.pt\n",
      "Hyperparameters saved to trained_models/bayesian_hyperparams_20251010_201336.json\n"
     ]
    }
   ],
   "source": [
    "# Save the trained Bayesian model (robust universal method)\n",
    "model_dir = \"trained_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, f\"bayesian_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "\n",
    "# Use universal save function for all agent types\n",
    "save_agent_model(bayesian_agent, model_path)\n",
    "\n",
    "# Also save hyperparameters for reproducibility\n",
    "hyperparams_path = os.path.join(model_dir, f\"bayesian_hyperparams_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(hyperparams_path, 'w') as f:\n",
    "    json.dump(bay_best_hyperparams, f, indent=2)\n",
    "    print(f\"Hyperparameters saved to {hyperparams_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bdfe3",
   "metadata": {},
   "source": [
    "## Transformer: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a66e295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRANSFORMER: HYPERPARAMETER TUNING & TRAINING\n",
      "============================================================\n",
      "Step 1: Hyperparameter Tuning\n",
      "Starting hyperparameter search for Transformer...\n",
      "  Evaluating 243 hyperparameter combinations...\n",
      "  Too many combinations (243), using random sampling of 20\n",
      "    Trial 1/20: {'d_model': 64, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 2}\n",
      "      CV Score: 0.218675 (±0.002479)\n",
      "    Trial 2/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.218675 (±0.002479)\n",
      "    Trial 2/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.218649 (±0.002319)\n",
      "    Trial 3/20: {'d_model': 64, 'dropout': 0.0, 'lr': 0.0001, 'nhead': 2, 'num_layers': 3}\n",
      "      CV Score: 0.218649 (±0.002319)\n",
      "    Trial 3/20: {'d_model': 64, 'dropout': 0.0, 'lr': 0.0001, 'nhead': 2, 'num_layers': 3}\n",
      "      CV Score: 0.218548 (±0.001716)\n",
      "    Trial 4/20: {'d_model': 64, 'dropout': 0.2, 'lr': 0.01, 'nhead': 4, 'num_layers': 1}\n",
      "      CV Score: 0.218548 (±0.001716)\n",
      "    Trial 4/20: {'d_model': 64, 'dropout': 0.2, 'lr': 0.01, 'nhead': 4, 'num_layers': 1}\n",
      "      CV Score: 0.220093 (±0.003249)\n",
      "    Trial 5/20: {'d_model': 128, 'dropout': 0.2, 'lr': 0.01, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.220093 (±0.003249)\n",
      "    Trial 5/20: {'d_model': 128, 'dropout': 0.2, 'lr': 0.01, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.220252 (±0.001669)\n",
      "    Trial 6/20: {'d_model': 64, 'dropout': 0.1, 'lr': 0.001, 'nhead': 2, 'num_layers': 3}\n",
      "      CV Score: 0.220252 (±0.001669)\n",
      "    Trial 6/20: {'d_model': 64, 'dropout': 0.1, 'lr': 0.001, 'nhead': 2, 'num_layers': 3}\n",
      "      CV Score: 0.218835 (±0.002619)\n",
      "    Trial 7/20: {'d_model': 32, 'dropout': 0.1, 'lr': 0.001, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218835 (±0.002619)\n",
      "    Trial 7/20: {'d_model': 32, 'dropout': 0.1, 'lr': 0.001, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218297 (±0.002193)\n",
      "    Trial 8/20: {'d_model': 64, 'dropout': 0.2, 'lr': 0.01, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218297 (±0.002193)\n",
      "    Trial 8/20: {'d_model': 64, 'dropout': 0.2, 'lr': 0.01, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218843 (±0.001859)\n",
      "    Trial 9/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 2, 'num_layers': 1}\n",
      "      CV Score: 0.218843 (±0.001859)\n",
      "    Trial 9/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 2, 'num_layers': 1}\n",
      "      CV Score: 0.218997 (±0.002357)\n",
      "    Trial 10/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218997 (±0.002357)\n",
      "    Trial 10/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218685 (±0.002115)\n",
      "    Trial 11/20: {'d_model': 128, 'dropout': 0.0, 'lr': 0.01, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.218685 (±0.002115)\n",
      "    Trial 11/20: {'d_model': 128, 'dropout': 0.0, 'lr': 0.01, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.218689 (±0.001535)\n",
      "    Trial 12/20: {'d_model': 32, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 2}\n",
      "      CV Score: 0.218689 (±0.001535)\n",
      "    Trial 12/20: {'d_model': 32, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 2}\n",
      "      CV Score: 0.218258 (±0.001986)\n",
      "    Trial 13/20: {'d_model': 32, 'dropout': 0.1, 'lr': 0.01, 'nhead': 2, 'num_layers': 2}\n",
      "      CV Score: 0.218258 (±0.001986)\n",
      "    Trial 13/20: {'d_model': 32, 'dropout': 0.1, 'lr': 0.01, 'nhead': 2, 'num_layers': 2}\n",
      "      CV Score: 0.218393 (±0.002170)\n",
      "    Trial 14/20: {'d_model': 128, 'dropout': 0.0, 'lr': 0.0001, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.218393 (±0.002170)\n",
      "    Trial 14/20: {'d_model': 128, 'dropout': 0.0, 'lr': 0.0001, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.219110 (±0.002506)\n",
      "    Trial 15/20: {'d_model': 64, 'dropout': 0.2, 'lr': 0.01, 'nhead': 2, 'num_layers': 3}\n",
      "      CV Score: 0.219110 (±0.002506)\n",
      "    Trial 15/20: {'d_model': 64, 'dropout': 0.2, 'lr': 0.01, 'nhead': 2, 'num_layers': 3}\n",
      "      CV Score: 0.224071 (±0.009200)\n",
      "    Trial 16/20: {'d_model': 128, 'dropout': 0.2, 'lr': 0.001, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.224071 (±0.009200)\n",
      "    Trial 16/20: {'d_model': 128, 'dropout': 0.2, 'lr': 0.001, 'nhead': 4, 'num_layers': 3}\n",
      "      CV Score: 0.222094 (±0.005194)\n",
      "    Trial 17/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 8, 'num_layers': 2}\n",
      "      CV Score: 0.222094 (±0.005194)\n",
      "    Trial 17/20: {'d_model': 128, 'dropout': 0.1, 'lr': 0.001, 'nhead': 8, 'num_layers': 2}\n",
      "      CV Score: 0.219336 (±0.001730)\n",
      "    Trial 18/20: {'d_model': 64, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.219336 (±0.001730)\n",
      "    Trial 18/20: {'d_model': 64, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218595 (±0.001872)\n",
      "    Trial 19/20: {'d_model': 32, 'dropout': 0.2, 'lr': 0.0001, 'nhead': 4, 'num_layers': 1}\n",
      "      CV Score: 0.218595 (±0.001872)\n",
      "    Trial 19/20: {'d_model': 32, 'dropout': 0.2, 'lr': 0.0001, 'nhead': 4, 'num_layers': 1}\n",
      "      CV Score: 0.218576 (±0.002074)\n",
      "    Trial 20/20: {'d_model': 64, 'dropout': 0.1, 'lr': 0.01, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218576 (±0.002074)\n",
      "    Trial 20/20: {'d_model': 64, 'dropout': 0.1, 'lr': 0.01, 'nhead': 8, 'num_layers': 3}\n",
      "      CV Score: 0.218732 (±0.001975)\n",
      "  Best hyperparameters found with CV score: 0.218258\n",
      "  Best hyperparams: {'d_model': 32, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 2}\n",
      "\n",
      "Best Transformer hyperparameters:\n",
      "  d_model: 32\n",
      "  dropout: 0.1\n",
      "  lr: 0.0001\n",
      "  nhead: 8\n",
      "  num_layers: 2\n",
      "Best CV score: 0.218258\n",
      "\n",
      "Step 2: Training Transformer with best hyperparameters...\n",
      "Training for 700 epochs with 73 batches per epoch...\n",
      "      CV Score: 0.218732 (±0.001975)\n",
      "  Best hyperparameters found with CV score: 0.218258\n",
      "  Best hyperparams: {'d_model': 32, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 2}\n",
      "\n",
      "Best Transformer hyperparameters:\n",
      "  d_model: 32\n",
      "  dropout: 0.1\n",
      "  lr: 0.0001\n",
      "  nhead: 8\n",
      "  num_layers: 2\n",
      "Best CV score: 0.218258\n",
      "\n",
      "Step 2: Training Transformer with best hyperparameters...\n",
      "Training for 700 epochs with 73 batches per epoch...\n",
      "  Epoch 5/700: Train Loss=0.221280, Val Loss=0.215688\n",
      "  Epoch 5/700: Train Loss=0.221280, Val Loss=0.215688\n",
      "  Epoch 10/700: Train Loss=0.219551, Val Loss=0.215448\n",
      "  Epoch 10/700: Train Loss=0.219551, Val Loss=0.215448\n",
      "  Epoch 15/700: Train Loss=0.219472, Val Loss=0.216683\n",
      "  Epoch 15/700: Train Loss=0.219472, Val Loss=0.216683\n",
      "  Epoch 20/700: Train Loss=0.219315, Val Loss=0.216012\n",
      "  Epoch 20/700: Train Loss=0.219315, Val Loss=0.216012\n",
      "  Epoch 25/700: Train Loss=0.218382, Val Loss=0.215328\n",
      "  Epoch 25/700: Train Loss=0.218382, Val Loss=0.215328\n",
      "  Epoch 30/700: Train Loss=0.218600, Val Loss=0.214830\n",
      "  Epoch 30/700: Train Loss=0.218600, Val Loss=0.214830\n",
      "  Epoch 35/700: Train Loss=0.218831, Val Loss=0.214764\n",
      "  Epoch 35/700: Train Loss=0.218831, Val Loss=0.214764\n",
      "  Epoch 40/700: Train Loss=0.218319, Val Loss=0.214062\n",
      "  Epoch 40/700: Train Loss=0.218319, Val Loss=0.214062\n",
      "  Epoch 45/700: Train Loss=0.217860, Val Loss=0.214211\n",
      "  Epoch 45/700: Train Loss=0.217860, Val Loss=0.214211\n",
      "  Epoch 50/700: Train Loss=0.217659, Val Loss=0.213944\n",
      "  Epoch 50/700: Train Loss=0.217659, Val Loss=0.213944\n",
      "  Epoch 55/700: Train Loss=0.217639, Val Loss=0.214033\n",
      "  Epoch 55/700: Train Loss=0.217639, Val Loss=0.214033\n",
      "  Epoch 60/700: Train Loss=0.217001, Val Loss=0.213722\n",
      "  Epoch 60/700: Train Loss=0.217001, Val Loss=0.213722\n",
      "  Epoch 65/700: Train Loss=0.217123, Val Loss=0.214235\n",
      "  Epoch 65/700: Train Loss=0.217123, Val Loss=0.214235\n",
      "  Epoch 70/700: Train Loss=0.217515, Val Loss=0.215439\n",
      "  Epoch 70/700: Train Loss=0.217515, Val Loss=0.215439\n",
      "  Epoch 75/700: Train Loss=0.217614, Val Loss=0.214560\n",
      "  Epoch 75/700: Train Loss=0.217614, Val Loss=0.214560\n",
      "  Epoch 80/700: Train Loss=0.217113, Val Loss=0.215880\n",
      "  Epoch 80/700: Train Loss=0.217113, Val Loss=0.215880\n",
      "  Epoch 85/700: Train Loss=0.216818, Val Loss=0.213618\n",
      "  Epoch 85/700: Train Loss=0.216818, Val Loss=0.213618\n",
      "  Epoch 90/700: Train Loss=0.216585, Val Loss=0.214774\n",
      "  Epoch 90/700: Train Loss=0.216585, Val Loss=0.214774\n",
      "  Epoch 95/700: Train Loss=0.216305, Val Loss=0.214150\n",
      "  Epoch 95/700: Train Loss=0.216305, Val Loss=0.214150\n",
      "  Epoch 100/700: Train Loss=0.216780, Val Loss=0.213602\n",
      "  Epoch 100/700: Train Loss=0.216780, Val Loss=0.213602\n",
      "  Epoch 105/700: Train Loss=0.216351, Val Loss=0.213407\n",
      "  Epoch 105/700: Train Loss=0.216351, Val Loss=0.213407\n",
      "  Epoch 110/700: Train Loss=0.216356, Val Loss=0.214327\n",
      "  Epoch 110/700: Train Loss=0.216356, Val Loss=0.214327\n",
      "  Epoch 115/700: Train Loss=0.216174, Val Loss=0.214761\n",
      "  Epoch 115/700: Train Loss=0.216174, Val Loss=0.214761\n",
      "  Epoch 120/700: Train Loss=0.216508, Val Loss=0.214119\n",
      "  Epoch 120/700: Train Loss=0.216508, Val Loss=0.214119\n",
      "  Epoch 125/700: Train Loss=0.216159, Val Loss=0.213801\n",
      "  Epoch 125/700: Train Loss=0.216159, Val Loss=0.213801\n",
      "  Epoch 130/700: Train Loss=0.216075, Val Loss=0.213708\n",
      "  Epoch 130/700: Train Loss=0.216075, Val Loss=0.213708\n",
      "  Epoch 135/700: Train Loss=0.215760, Val Loss=0.213362\n",
      "  Epoch 135/700: Train Loss=0.215760, Val Loss=0.213362\n",
      "  Epoch 140/700: Train Loss=0.216008, Val Loss=0.213664\n",
      "  Epoch 140/700: Train Loss=0.216008, Val Loss=0.213664\n",
      "  Epoch 145/700: Train Loss=0.215777, Val Loss=0.214651\n",
      "  Epoch 145/700: Train Loss=0.215777, Val Loss=0.214651\n",
      "  Epoch 150/700: Train Loss=0.216119, Val Loss=0.213926\n",
      "  Epoch 150/700: Train Loss=0.216119, Val Loss=0.213926\n",
      "  Epoch 155/700: Train Loss=0.215432, Val Loss=0.214085\n",
      "  Epoch 155/700: Train Loss=0.215432, Val Loss=0.214085\n",
      "  Epoch 160/700: Train Loss=0.215473, Val Loss=0.214243\n",
      "  Epoch 160/700: Train Loss=0.215473, Val Loss=0.214243\n",
      "  Epoch 165/700: Train Loss=0.215485, Val Loss=0.214764\n",
      "  Epoch 165/700: Train Loss=0.215485, Val Loss=0.214764\n",
      "  Epoch 170/700: Train Loss=0.215535, Val Loss=0.213943\n",
      "  Epoch 170/700: Train Loss=0.215535, Val Loss=0.213943\n",
      "  Epoch 175/700: Train Loss=0.215508, Val Loss=0.215088\n",
      "  Epoch 175/700: Train Loss=0.215508, Val Loss=0.215088\n",
      "  Epoch 180/700: Train Loss=0.215758, Val Loss=0.213674\n",
      "  Epoch 180/700: Train Loss=0.215758, Val Loss=0.213674\n",
      "  Epoch 185/700: Train Loss=0.215238, Val Loss=0.214021\n",
      "  Epoch 185/700: Train Loss=0.215238, Val Loss=0.214021\n",
      "  Epoch 190/700: Train Loss=0.215416, Val Loss=0.214434\n",
      "  Epoch 190/700: Train Loss=0.215416, Val Loss=0.214434\n",
      "  Epoch 195/700: Train Loss=0.215703, Val Loss=0.213363\n",
      "  Epoch 195/700: Train Loss=0.215703, Val Loss=0.213363\n",
      "  Epoch 200/700: Train Loss=0.215911, Val Loss=0.214140\n",
      "  Epoch 200/700: Train Loss=0.215911, Val Loss=0.214140\n",
      "  Epoch 205/700: Train Loss=0.215090, Val Loss=0.214860\n",
      "  Epoch 205/700: Train Loss=0.215090, Val Loss=0.214860\n",
      "  Epoch 210/700: Train Loss=0.215383, Val Loss=0.213497\n",
      "  Epoch 210/700: Train Loss=0.215383, Val Loss=0.213497\n",
      "  Epoch 215/700: Train Loss=0.215230, Val Loss=0.213207\n",
      "  Epoch 215/700: Train Loss=0.215230, Val Loss=0.213207\n",
      "  Epoch 220/700: Train Loss=0.215356, Val Loss=0.213643\n",
      "  Epoch 220/700: Train Loss=0.215356, Val Loss=0.213643\n",
      "  Epoch 225/700: Train Loss=0.215249, Val Loss=0.213985\n",
      "  Epoch 225/700: Train Loss=0.215249, Val Loss=0.213985\n",
      "  Epoch 230/700: Train Loss=0.214573, Val Loss=0.213369\n",
      "  Epoch 230/700: Train Loss=0.214573, Val Loss=0.213369\n",
      "  Epoch 235/700: Train Loss=0.215249, Val Loss=0.214391\n",
      "  Epoch 235/700: Train Loss=0.215249, Val Loss=0.214391\n",
      "  Epoch 240/700: Train Loss=0.214792, Val Loss=0.215343\n",
      "  Epoch 240/700: Train Loss=0.214792, Val Loss=0.215343\n",
      "  Epoch 245/700: Train Loss=0.214666, Val Loss=0.213699\n",
      "  Epoch 245/700: Train Loss=0.214666, Val Loss=0.213699\n",
      "  Epoch 250/700: Train Loss=0.214512, Val Loss=0.214461\n",
      "  Epoch 250/700: Train Loss=0.214512, Val Loss=0.214461\n",
      "  Epoch 255/700: Train Loss=0.214382, Val Loss=0.213579\n",
      "  Epoch 255/700: Train Loss=0.214382, Val Loss=0.213579\n",
      "  Epoch 260/700: Train Loss=0.214281, Val Loss=0.213116\n",
      "  Epoch 260/700: Train Loss=0.214281, Val Loss=0.213116\n",
      "  Epoch 265/700: Train Loss=0.214962, Val Loss=0.214346\n",
      "  Epoch 265/700: Train Loss=0.214962, Val Loss=0.214346\n",
      "  Epoch 270/700: Train Loss=0.214806, Val Loss=0.213408\n",
      "  Epoch 270/700: Train Loss=0.214806, Val Loss=0.213408\n",
      "  Epoch 275/700: Train Loss=0.214326, Val Loss=0.213523\n",
      "  Epoch 275/700: Train Loss=0.214326, Val Loss=0.213523\n",
      "  Epoch 280/700: Train Loss=0.214915, Val Loss=0.213207\n",
      "  Epoch 280/700: Train Loss=0.214915, Val Loss=0.213207\n",
      "  Epoch 285/700: Train Loss=0.214087, Val Loss=0.213886\n",
      "  Epoch 285/700: Train Loss=0.214087, Val Loss=0.213886\n",
      "  Epoch 290/700: Train Loss=0.214253, Val Loss=0.214242\n",
      "  Epoch 290/700: Train Loss=0.214253, Val Loss=0.214242\n",
      "  Epoch 295/700: Train Loss=0.214684, Val Loss=0.214490\n",
      "  Epoch 295/700: Train Loss=0.214684, Val Loss=0.214490\n",
      "  Epoch 300/700: Train Loss=0.214586, Val Loss=0.212702\n",
      "  Epoch 300/700: Train Loss=0.214586, Val Loss=0.212702\n",
      "  Epoch 305/700: Train Loss=0.213702, Val Loss=0.215319\n",
      "  Epoch 305/700: Train Loss=0.213702, Val Loss=0.215319\n",
      "  Epoch 310/700: Train Loss=0.214148, Val Loss=0.214812\n",
      "  Epoch 310/700: Train Loss=0.214148, Val Loss=0.214812\n",
      "  Epoch 315/700: Train Loss=0.214158, Val Loss=0.213595\n",
      "  Epoch 315/700: Train Loss=0.214158, Val Loss=0.213595\n",
      "  Epoch 320/700: Train Loss=0.214350, Val Loss=0.214622\n",
      "  Epoch 320/700: Train Loss=0.214350, Val Loss=0.214622\n",
      "  Epoch 325/700: Train Loss=0.214639, Val Loss=0.213424\n",
      "  Epoch 325/700: Train Loss=0.214639, Val Loss=0.213424\n",
      "  Epoch 330/700: Train Loss=0.213857, Val Loss=0.214236\n",
      "  Epoch 330/700: Train Loss=0.213857, Val Loss=0.214236\n",
      "  Epoch 335/700: Train Loss=0.214364, Val Loss=0.214690\n",
      "  Epoch 335/700: Train Loss=0.214364, Val Loss=0.214690\n",
      "  Epoch 340/700: Train Loss=0.213401, Val Loss=0.214796\n",
      "  Epoch 340/700: Train Loss=0.213401, Val Loss=0.214796\n",
      "  Epoch 345/700: Train Loss=0.213609, Val Loss=0.216270\n",
      "  Epoch 345/700: Train Loss=0.213609, Val Loss=0.216270\n",
      "  Epoch 350/700: Train Loss=0.213603, Val Loss=0.215863\n",
      "  Epoch 350/700: Train Loss=0.213603, Val Loss=0.215863\n",
      "  Epoch 355/700: Train Loss=0.213124, Val Loss=0.215479\n",
      "  Epoch 355/700: Train Loss=0.213124, Val Loss=0.215479\n",
      "  Epoch 360/700: Train Loss=0.213433, Val Loss=0.215083\n",
      "  Epoch 360/700: Train Loss=0.213433, Val Loss=0.215083\n",
      "  Epoch 365/700: Train Loss=0.213033, Val Loss=0.215360\n",
      "  Epoch 365/700: Train Loss=0.213033, Val Loss=0.215360\n",
      "  Epoch 370/700: Train Loss=0.213371, Val Loss=0.214271\n",
      "  Epoch 370/700: Train Loss=0.213371, Val Loss=0.214271\n",
      "  Epoch 375/700: Train Loss=0.212943, Val Loss=0.214708\n",
      "  Epoch 375/700: Train Loss=0.212943, Val Loss=0.214708\n",
      "  Epoch 380/700: Train Loss=0.213072, Val Loss=0.216792\n",
      "  Epoch 380/700: Train Loss=0.213072, Val Loss=0.216792\n",
      "  Epoch 385/700: Train Loss=0.212947, Val Loss=0.216013\n",
      "  Epoch 385/700: Train Loss=0.212947, Val Loss=0.216013\n",
      "  Epoch 390/700: Train Loss=0.213154, Val Loss=0.216439\n",
      "  Epoch 390/700: Train Loss=0.213154, Val Loss=0.216439\n",
      "  Epoch 395/700: Train Loss=0.213570, Val Loss=0.215063\n",
      "  Epoch 395/700: Train Loss=0.213570, Val Loss=0.215063\n",
      "  Epoch 400/700: Train Loss=0.213319, Val Loss=0.214664\n",
      "  Epoch 400/700: Train Loss=0.213319, Val Loss=0.214664\n",
      "  Epoch 405/700: Train Loss=0.213197, Val Loss=0.215806\n",
      "  Epoch 405/700: Train Loss=0.213197, Val Loss=0.215806\n",
      "  Epoch 410/700: Train Loss=0.212786, Val Loss=0.216708\n",
      "  Epoch 410/700: Train Loss=0.212786, Val Loss=0.216708\n",
      "  Epoch 415/700: Train Loss=0.212794, Val Loss=0.215064\n",
      "  Epoch 415/700: Train Loss=0.212794, Val Loss=0.215064\n",
      "  Epoch 420/700: Train Loss=0.212450, Val Loss=0.215428\n",
      "  Epoch 420/700: Train Loss=0.212450, Val Loss=0.215428\n",
      "  Epoch 425/700: Train Loss=0.212467, Val Loss=0.215276\n",
      "  Epoch 425/700: Train Loss=0.212467, Val Loss=0.215276\n",
      "  Epoch 430/700: Train Loss=0.212506, Val Loss=0.215242\n",
      "  Epoch 430/700: Train Loss=0.212506, Val Loss=0.215242\n",
      "  Epoch 435/700: Train Loss=0.212078, Val Loss=0.215335\n",
      "  Epoch 435/700: Train Loss=0.212078, Val Loss=0.215335\n",
      "  Epoch 440/700: Train Loss=0.212732, Val Loss=0.215050\n",
      "  Epoch 440/700: Train Loss=0.212732, Val Loss=0.215050\n",
      "  Epoch 445/700: Train Loss=0.212560, Val Loss=0.215702\n",
      "  Epoch 445/700: Train Loss=0.212560, Val Loss=0.215702\n",
      "  Epoch 450/700: Train Loss=0.211756, Val Loss=0.214890\n",
      "  Epoch 450/700: Train Loss=0.211756, Val Loss=0.214890\n",
      "  Epoch 455/700: Train Loss=0.211920, Val Loss=0.215158\n",
      "  Epoch 455/700: Train Loss=0.211920, Val Loss=0.215158\n",
      "  Epoch 460/700: Train Loss=0.212093, Val Loss=0.215711\n",
      "  Epoch 460/700: Train Loss=0.212093, Val Loss=0.215711\n",
      "  Epoch 465/700: Train Loss=0.212038, Val Loss=0.216225\n",
      "  Epoch 465/700: Train Loss=0.212038, Val Loss=0.216225\n",
      "  Epoch 470/700: Train Loss=0.212063, Val Loss=0.218304\n",
      "  Epoch 470/700: Train Loss=0.212063, Val Loss=0.218304\n",
      "  Epoch 475/700: Train Loss=0.212139, Val Loss=0.216167\n",
      "  Epoch 475/700: Train Loss=0.212139, Val Loss=0.216167\n",
      "  Epoch 480/700: Train Loss=0.211526, Val Loss=0.217622\n",
      "  Epoch 480/700: Train Loss=0.211526, Val Loss=0.217622\n",
      "  Epoch 485/700: Train Loss=0.212408, Val Loss=0.215684\n",
      "  Epoch 485/700: Train Loss=0.212408, Val Loss=0.215684\n",
      "  Epoch 490/700: Train Loss=0.211463, Val Loss=0.216273\n",
      "  Epoch 490/700: Train Loss=0.211463, Val Loss=0.216273\n",
      "  Epoch 495/700: Train Loss=0.211500, Val Loss=0.216039\n",
      "  Epoch 495/700: Train Loss=0.211500, Val Loss=0.216039\n",
      "  Epoch 500/700: Train Loss=0.211424, Val Loss=0.215929\n",
      "  Epoch 500/700: Train Loss=0.211424, Val Loss=0.215929\n",
      "  Epoch 505/700: Train Loss=0.211327, Val Loss=0.216245\n",
      "  Epoch 505/700: Train Loss=0.211327, Val Loss=0.216245\n",
      "  Epoch 510/700: Train Loss=0.210646, Val Loss=0.214756\n",
      "  Epoch 510/700: Train Loss=0.210646, Val Loss=0.214756\n",
      "  Epoch 515/700: Train Loss=0.210974, Val Loss=0.215933\n",
      "  Epoch 515/700: Train Loss=0.210974, Val Loss=0.215933\n",
      "  Epoch 520/700: Train Loss=0.210297, Val Loss=0.219277\n",
      "  Epoch 520/700: Train Loss=0.210297, Val Loss=0.219277\n",
      "  Epoch 525/700: Train Loss=0.210509, Val Loss=0.217955\n",
      "  Epoch 525/700: Train Loss=0.210509, Val Loss=0.217955\n",
      "  Epoch 530/700: Train Loss=0.210184, Val Loss=0.217874\n",
      "  Epoch 530/700: Train Loss=0.210184, Val Loss=0.217874\n",
      "  Epoch 535/700: Train Loss=0.211170, Val Loss=0.217442\n",
      "  Epoch 535/700: Train Loss=0.211170, Val Loss=0.217442\n",
      "  Epoch 540/700: Train Loss=0.211070, Val Loss=0.217095\n",
      "  Epoch 540/700: Train Loss=0.211070, Val Loss=0.217095\n",
      "  Epoch 545/700: Train Loss=0.210254, Val Loss=0.217198\n",
      "  Epoch 545/700: Train Loss=0.210254, Val Loss=0.217198\n",
      "  Epoch 550/700: Train Loss=0.210233, Val Loss=0.217235\n",
      "  Epoch 550/700: Train Loss=0.210233, Val Loss=0.217235\n",
      "  Epoch 555/700: Train Loss=0.210586, Val Loss=0.216775\n",
      "  Epoch 555/700: Train Loss=0.210586, Val Loss=0.216775\n",
      "  Epoch 560/700: Train Loss=0.210193, Val Loss=0.218365\n",
      "  Epoch 560/700: Train Loss=0.210193, Val Loss=0.218365\n",
      "  Epoch 565/700: Train Loss=0.210284, Val Loss=0.216399\n",
      "  Epoch 565/700: Train Loss=0.210284, Val Loss=0.216399\n",
      "  Epoch 570/700: Train Loss=0.209687, Val Loss=0.217294\n",
      "  Epoch 570/700: Train Loss=0.209687, Val Loss=0.217294\n",
      "  Epoch 575/700: Train Loss=0.209862, Val Loss=0.218275\n",
      "  Epoch 575/700: Train Loss=0.209862, Val Loss=0.218275\n",
      "  Epoch 580/700: Train Loss=0.209935, Val Loss=0.216381\n",
      "  Epoch 580/700: Train Loss=0.209935, Val Loss=0.216381\n",
      "  Epoch 585/700: Train Loss=0.209883, Val Loss=0.218658\n",
      "  Epoch 585/700: Train Loss=0.209883, Val Loss=0.218658\n",
      "  Epoch 590/700: Train Loss=0.209191, Val Loss=0.218692\n",
      "  Epoch 590/700: Train Loss=0.209191, Val Loss=0.218692\n",
      "  Epoch 595/700: Train Loss=0.209356, Val Loss=0.217793\n",
      "  Epoch 595/700: Train Loss=0.209356, Val Loss=0.217793\n",
      "  Epoch 600/700: Train Loss=0.209008, Val Loss=0.218733\n",
      "  Epoch 600/700: Train Loss=0.209008, Val Loss=0.218733\n",
      "  Epoch 605/700: Train Loss=0.209255, Val Loss=0.218198\n",
      "  Epoch 605/700: Train Loss=0.209255, Val Loss=0.218198\n",
      "  Epoch 610/700: Train Loss=0.209094, Val Loss=0.217246\n",
      "  Epoch 610/700: Train Loss=0.209094, Val Loss=0.217246\n",
      "  Epoch 615/700: Train Loss=0.208422, Val Loss=0.219251\n",
      "  Epoch 615/700: Train Loss=0.208422, Val Loss=0.219251\n",
      "  Epoch 620/700: Train Loss=0.208623, Val Loss=0.219533\n",
      "  Epoch 620/700: Train Loss=0.208623, Val Loss=0.219533\n",
      "  Epoch 625/700: Train Loss=0.208604, Val Loss=0.218505\n",
      "  Epoch 625/700: Train Loss=0.208604, Val Loss=0.218505\n",
      "  Epoch 630/700: Train Loss=0.207922, Val Loss=0.219036\n",
      "  Epoch 630/700: Train Loss=0.207922, Val Loss=0.219036\n",
      "  Epoch 635/700: Train Loss=0.208824, Val Loss=0.219415\n",
      "  Epoch 635/700: Train Loss=0.208824, Val Loss=0.219415\n",
      "  Epoch 640/700: Train Loss=0.208165, Val Loss=0.219467\n",
      "  Epoch 640/700: Train Loss=0.208165, Val Loss=0.219467\n",
      "  Epoch 645/700: Train Loss=0.208038, Val Loss=0.219960\n",
      "  Epoch 645/700: Train Loss=0.208038, Val Loss=0.219960\n",
      "  Epoch 650/700: Train Loss=0.207883, Val Loss=0.219449\n",
      "  Epoch 650/700: Train Loss=0.207883, Val Loss=0.219449\n",
      "  Epoch 655/700: Train Loss=0.207593, Val Loss=0.218475\n",
      "  Epoch 655/700: Train Loss=0.207593, Val Loss=0.218475\n",
      "  Epoch 660/700: Train Loss=0.207402, Val Loss=0.219720\n",
      "  Epoch 660/700: Train Loss=0.207402, Val Loss=0.219720\n",
      "  Epoch 665/700: Train Loss=0.207655, Val Loss=0.220714\n",
      "  Epoch 665/700: Train Loss=0.207655, Val Loss=0.220714\n",
      "  Epoch 670/700: Train Loss=0.207264, Val Loss=0.220447\n",
      "  Epoch 670/700: Train Loss=0.207264, Val Loss=0.220447\n",
      "  Epoch 675/700: Train Loss=0.207644, Val Loss=0.220588\n",
      "  Epoch 675/700: Train Loss=0.207644, Val Loss=0.220588\n",
      "  Epoch 680/700: Train Loss=0.207139, Val Loss=0.221842\n",
      "  Epoch 680/700: Train Loss=0.207139, Val Loss=0.221842\n",
      "  Epoch 685/700: Train Loss=0.207069, Val Loss=0.220745\n",
      "  Epoch 685/700: Train Loss=0.207069, Val Loss=0.220745\n",
      "  Epoch 690/700: Train Loss=0.207130, Val Loss=0.222681\n",
      "  Epoch 690/700: Train Loss=0.207130, Val Loss=0.222681\n",
      "  Epoch 695/700: Train Loss=0.206896, Val Loss=0.220234\n",
      "  Epoch 695/700: Train Loss=0.206896, Val Loss=0.220234\n",
      "  Epoch 700/700: Train Loss=0.206072, Val Loss=0.222175\n",
      "\n",
      "Step 3: Evaluating Transformer...\n",
      "  Epoch 700/700: Train Loss=0.206072, Val Loss=0.222175\n",
      "\n",
      "Step 3: Evaluating Transformer...\n",
      "\n",
      "Transformer Final Results:\n",
      "  Best Hyperparams: {'d_model': 32, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 2}\n",
      "  HP Search CV Score: 0.218258\n",
      "  Train Time: 274.47s\n",
      "  Final Train Loss: 0.206072\n",
      "  Final Val Loss: 0.222175\n",
      "  Avg MSE: 0.215980\n",
      "  Avg Reward: -57.433\n",
      "  Success Rate: 0.000\n",
      "  Model Parameters: 275,554\n",
      "\n",
      "\n",
      "Transformer Final Results:\n",
      "  Best Hyperparams: {'d_model': 32, 'dropout': 0.1, 'lr': 0.0001, 'nhead': 8, 'num_layers': 2}\n",
      "  HP Search CV Score: 0.218258\n",
      "  Train Time: 274.47s\n",
      "  Final Train Loss: 0.206072\n",
      "  Final Val Loss: 0.222175\n",
      "  Avg MSE: 0.215980\n",
      "  Avg Reward: -57.433\n",
      "  Success Rate: 0.000\n",
      "  Model Parameters: 275,554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRANSFORMER: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "trans_best_hyperparams, trans_best_score, trans_search_results = hyperparameter_search(\n",
    "    'Transformer', train_data, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Transformer hyperparameters:\")\n",
    "for key, value in trans_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {trans_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training Transformer with best hyperparameters...\")\n",
    "transformer_agent = create_agent_with_hyperparams(\n",
    "    'Transformer', trans_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train Transformer with full epochs\n",
    "start_time = time.time()\n",
    "trans_train_losses, trans_val_losses = train_agent(\n",
    "    transformer_agent, train_data, val_data,\n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "trans_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating Transformer...\")\n",
    "start_time = time.time()\n",
    "trans_results = evaluate_agent(transformer_agent, env, CONFIG['num_test_episodes'])\n",
    "trans_test_time = time.time() - start_time\n",
    "\n",
    "# Store results with hyperparameter information\n",
    "results['algorithm'].append('Transformer')\n",
    "results['train_time'].append(trans_train_time)\n",
    "results['test_time'].append(trans_test_time)\n",
    "results['final_train_loss'].append(trans_train_losses[-1])\n",
    "results['final_val_loss'].append(trans_val_losses[-1])\n",
    "results['avg_mse'].append(trans_results['avg_mse'])\n",
    "results['avg_reward'].append(trans_results['avg_reward'])\n",
    "results['success_rate'].append(trans_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(transformer_agent))\n",
    "results['best_hyperparams'].append(trans_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(trans_best_score)\n",
    "\n",
    "print(f\"\\nTransformer Final Results:\")\n",
    "print(f\"  Best Hyperparams: {trans_best_hyperparams}\")\n",
    "print(f\"  HP Search CV Score: {trans_best_score:.6f}\")\n",
    "print(f\"  Train Time: {trans_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {trans_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {trans_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {trans_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {trans_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {trans_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(transformer_agent):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cda740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_models/transformer_model_20251010_202415.pt\n",
      "Hyperparameters saved to trained_models/transformer_hyperparams_20251010_202415.json\n"
     ]
    }
   ],
   "source": [
    "# Save the trained Bayesian model (robust universal method)\n",
    "model_dir = \"trained_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, f\"transformer_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "\n",
    "# Use universal save function for all agent types\n",
    "save_agent_model(transformer_agent, model_path)\n",
    "\n",
    "# Also save hyperparameters for reproducibility\n",
    "hyperparams_path = os.path.join(model_dir, f\"transformer_hyperparams_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(hyperparams_path, 'w') as f:\n",
    "    json.dump(trans_best_hyperparams, f, indent=2)\n",
    "    print(f\"Hyperparameters saved to {hyperparams_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81606290",
   "metadata": {},
   "source": [
    "## Linear: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82d981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LINEAR: HYPERPARAMETER TUNING & TRAINING\n",
      "============================================================\n",
      "Step 1: Hyperparameter Tuning\n",
      "Starting hyperparameter search for Linear...\n",
      "  Evaluating 9 hyperparameter combinations...\n",
      "    Trial 1/9: {'lr': 0.0001, 'weight_decay': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LINEAR: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "lin_best_hyperparams, lin_best_score, lin_search_results = hyperparameter_search(\n",
    "    'Linear', train_data, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Linear hyperparameters:\")\n",
    "for key, value in lin_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {lin_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training Linear with best hyperparameters...\")\n",
    "linear_agent = create_agent_with_hyperparams(\n",
    "    'Linear', lin_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train Linear with full epochs\n",
    "start_time = time.time()\n",
    "lin_train_losses, lin_val_losses = train_agent(\n",
    "    linear_agent, train_data, val_data,\n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "lin_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating Linear...\")\n",
    "start_time = time.time()\n",
    "lin_results = evaluate_agent(linear_agent, env, CONFIG['num_test_episodes'])\n",
    "lin_test_time = time.time() - start_time\n",
    "\n",
    "# Store results with hyperparameter information\n",
    "results['algorithm'].append('Linear')\n",
    "results['train_time'].append(lin_train_time)\n",
    "results['test_time'].append(lin_test_time)\n",
    "results['final_train_loss'].append(lin_train_losses[-1])\n",
    "results['final_val_loss'].append(lin_val_losses[-1])\n",
    "results['avg_mse'].append(lin_results['avg_mse'])\n",
    "results['avg_reward'].append(lin_results['avg_reward'])\n",
    "results['success_rate'].append(lin_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(linear_agent))\n",
    "results['best_hyperparams'].append(lin_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(lin_best_score)\n",
    "\n",
    "print(f\"\\nLinear Final Results:\")\n",
    "print(f\"  Best Hyperparams: {lin_best_hyperparams}\")\n",
    "print(f\"  HP Search CV Score: {lin_best_score:.6f}\")\n",
    "print(f\"  Train Time: {lin_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {lin_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {lin_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {lin_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {lin_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {lin_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(linear_agent):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Bayesian model (robust universal method)\n",
    "model_dir = \"trained_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, f\"linear_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt\")\n",
    "\n",
    "# Use universal save function for all agent types\n",
    "save_agent_model(transformer_agent, model_path)\n",
    "\n",
    "# Also save hyperparameters for reproducibility\n",
    "hyperparams_path = os.path.join(model_dir, f\"linear_model_hyperparams_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(hyperparams_path, 'w') as f:\n",
    "    json.dump(lin_best_hyperparams, f, indent=2)\n",
    "    print(f\"Hyperparameters saved to {hyperparams_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1fb4d",
   "metadata": {},
   "source": [
    "## VAE: Hyperparameter Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db959410",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"VAE: HYPERPARAMETER TUNING & TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Hyperparameter Tuning\n",
    "print(\"Step 1: Hyperparameter Tuning\")\n",
    "vae_best_hyperparams, vae_best_score, vae_search_results = hyperparameter_search(\n",
    "    'VAE', train_data, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "print(f\"\\nBest VAE hyperparameters:\")\n",
    "for key, value in vae_best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Best CV score: {vae_best_score:.6f}\")\n",
    "\n",
    "# Step 2: Train with Best Hyperparameters\n",
    "print(f\"\\nStep 2: Training VAE with best hyperparameters...\")\n",
    "vae_agent = create_agent_with_hyperparams(\n",
    "    'VAE', vae_best_hyperparams, STATE_DIM, ACTION_DIM, GOAL_DIM, CONFIG['device']\n",
    ")\n",
    "\n",
    "# Train VAE with full epochs\n",
    "start_time = time.time()\n",
    "vae_train_losses, vae_val_losses = train_agent(\n",
    "    vae_agent, train_data, val_data,\n",
    "    CONFIG['num_epochs'], CONFIG['batch_size']\n",
    ")\n",
    "vae_train_time = time.time() - start_time\n",
    "\n",
    "# Step 3: Evaluation\n",
    "print(f\"\\nStep 3: Evaluating VAE...\")\n",
    "start_time = time.time()\n",
    "vae_results = evaluate_agent(vae_agent, env, CONFIG['num_test_episodes'])\n",
    "vae_test_time = time.time() - start_time\n",
    "\n",
    "# Store results with hyperparameter information\n",
    "results['algorithm'].append('VAE')\n",
    "results['train_time'].append(vae_train_time)\n",
    "results['test_time'].append(vae_test_time)\n",
    "results['final_train_loss'].append(vae_train_losses[-1])\n",
    "results['final_val_loss'].append(vae_val_losses[-1])\n",
    "results['avg_mse'].append(vae_results['avg_mse'])\n",
    "results['avg_reward'].append(vae_results['avg_reward'])\n",
    "results['success_rate'].append(vae_results['success_rate'])\n",
    "results['model_params'].append(count_parameters(vae_agent))\n",
    "results['best_hyperparams'].append(vae_best_hyperparams)\n",
    "results['hp_search_cv_score'].append(vae_best_score)\n",
    "\n",
    "print(f\"\\nVAE Final Results:\")\n",
    "print(f\"  Best Hyperparams: {vae_best_hyperparams}\")\n",
    "print(f\"  HP Search CV Score: {vae_best_score:.6f}\")\n",
    "print(f\"  Train Time: {vae_train_time:.2f}s\")\n",
    "print(f\"  Final Train Loss: {vae_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final Val Loss: {vae_val_losses[-1]:.6f}\")\n",
    "print(f\"  Avg MSE: {vae_results['avg_mse']:.6f}\")\n",
    "print(f\"  Avg Reward: {vae_results['avg_reward']:.3f}\")\n",
    "print(f\"  Success Rate: {vae_results['success_rate']:.3f}\")\n",
    "print(f\"  Model Parameters: {count_parameters(vae_agent):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf1de7",
   "metadata": {},
   "source": [
    "## Final Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257b0ce",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning summary\n",
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all best hyperparameters and scores\n",
    "hp_summary = {\n",
    "    'Algorithm': [],\n",
    "    'Best_CV_Score': [],\n",
    "    'Best_Hyperparameters': []\n",
    "}\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('AutoEncoder')\n",
    "    hp_summary['Best_CV_Score'].append(ae_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(ae_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"AutoEncoder hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('Bayesian')\n",
    "    hp_summary['Best_CV_Score'].append(bay_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(bay_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"Bayesian hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('Transformer')\n",
    "    hp_summary['Best_CV_Score'].append(trans_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(trans_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"Transformer hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('Linear')\n",
    "    hp_summary['Best_CV_Score'].append(lin_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(lin_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"Linear hyperparameter tuning not completed yet.\")\n",
    "\n",
    "try:\n",
    "    hp_summary['Algorithm'].append('VAE')\n",
    "    hp_summary['Best_CV_Score'].append(vae_best_score)\n",
    "    hp_summary['Best_Hyperparameters'].append(vae_best_hyperparams)\n",
    "except NameError:\n",
    "    print(\"VAE hyperparameter tuning not completed yet.\")\n",
    "\n",
    "if hp_summary['Algorithm']:\n",
    "    hp_df = pd.DataFrame(hp_summary)\n",
    "    \n",
    "    print(\"\\nBest Cross-Validation Scores from Hyperparameter Tuning:\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in hp_df.iterrows():\n",
    "        print(f\"{row['Algorithm']:>12}: {row['Best_CV_Score']:.6f}\")\n",
    "    \n",
    "    print(f\"\\nBest performing algorithm in hyperparameter search: {hp_df.loc[hp_df['Best_CV_Score'].idxmin(), 'Algorithm']}\")\n",
    "    \n",
    "    print(\"\\nDetailed Best Hyperparameters:\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in hp_df.iterrows():\n",
    "        print(f\"\\n{row['Algorithm']}:\")\n",
    "        for key, value in row['Best_Hyperparameters'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Create visualization of hyperparameter search results\n",
    "    if len(hp_summary['Algorithm']) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        bars = ax.bar(hp_df['Algorithm'], hp_df['Best_CV_Score'], \n",
    "                     color=sns.color_palette(\"husl\", len(hp_df)))\n",
    "        ax.set_title('Best Cross-Validation Scores from Hyperparameter Tuning', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('CV Score (lower is better)')\n",
    "        ax.set_xlabel('Algorithm')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, hp_df['Best_CV_Score']):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{score:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No hyperparameter tuning results available yet.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display results table\n",
    "print(\"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.round(6))\n",
    "print()\n",
    "\n",
    "# Create summary statistics\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Best Average Reward: {df_results['algorithm'][df_results['avg_reward'].idxmax()]} ({df_results['avg_reward'].max():.3f})\")\n",
    "print(f\"Lowest MSE: {df_results['algorithm'][df_results['avg_mse'].idxmin()]} ({df_results['avg_mse'].min():.6f})\")\n",
    "print(f\"Highest Success Rate: {df_results['algorithm'][df_results['success_rate'].idxmax()]} ({df_results['success_rate'].max():.3f})\")\n",
    "print(f\"Fastest Training: {df_results['algorithm'][df_results['train_time'].idxmin()]} ({df_results['train_time'].min():.2f}s)\")\n",
    "print(f\"Lowest Validation Loss: {df_results['algorithm'][df_results['final_val_loss'].idxmin()]} ({df_results['final_val_loss'].min():.6f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b4ea0",
   "metadata": {},
   "source": [
    "## Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aaa428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Algorithm Comparison Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Average Reward Comparison\n",
    "axes[0, 0].bar(df_results['algorithm'], df_results['avg_reward'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[0, 0].set_title('Average Reward per Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. MSE Comparison\n",
    "axes[0, 1].bar(df_results['algorithm'], df_results['avg_mse'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[0, 1].set_title('Average Mean Squared Error')\n",
    "axes[0, 1].set_ylabel('MSE')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].set_yscale('log')  # Log scale for better visibility\n",
    "\n",
    "# 3. Success Rate Comparison\n",
    "axes[0, 2].bar(df_results['algorithm'], df_results['success_rate'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[0, 2].set_title('Success Rate')\n",
    "axes[0, 2].set_ylabel('Success Rate')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "axes[0, 2].set_ylim(0, 1)\n",
    "\n",
    "# 4. Training Time Comparison\n",
    "axes[1, 0].bar(df_results['algorithm'], df_results['train_time'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[1, 0].set_title('Training Time')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Model Parameters Comparison\n",
    "axes[1, 1].bar(df_results['algorithm'], df_results['model_params'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[1, 1].set_title('Model Parameters')\n",
    "axes[1, 1].set_ylabel('Number of Parameters')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].set_yscale('log')  # Log scale for better visibility\n",
    "\n",
    "# 6. Final Validation Loss Comparison\n",
    "axes[1, 2].bar(df_results['algorithm'], df_results['final_val_loss'], color=sns.color_palette(\"husl\", len(df_results)))\n",
    "axes[1, 2].set_title('Final Validation Loss')\n",
    "axes[1, 2].set_ylabel('Validation Loss')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ec4fc",
   "metadata": {},
   "source": [
    "## Detailed Performance Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7eee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for comprehensive comparison\n",
    "def create_radar_chart(df):\n",
    "    # Normalize metrics for radar chart (0-1 scale)\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # For metrics where lower is better (MSE, train_time, val_loss), invert them\n",
    "    normalized_df['norm_mse'] = 1 - (df['avg_mse'] - df['avg_mse'].min()) / (df['avg_mse'].max() - df['avg_mse'].min())\n",
    "    normalized_df['norm_train_time'] = 1 - (df['train_time'] - df['train_time'].min()) / (df['train_time'].max() - df['train_time'].min())\n",
    "    normalized_df['norm_val_loss'] = 1 - (df['final_val_loss'] - df['final_val_loss'].min()) / (df['final_val_loss'].max() - df['final_val_loss'].min())\n",
    "    \n",
    "    # For metrics where higher is better, normalize directly\n",
    "    normalized_df['norm_reward'] = (df['avg_reward'] - df['avg_reward'].min()) / (df['avg_reward'].max() - df['avg_reward'].min())\n",
    "    normalized_df['norm_success'] = df['success_rate']  # Already 0-1\n",
    "    \n",
    "    # Parameters normalized (smaller models get higher scores)\n",
    "    normalized_df['norm_params'] = 1 - (df['model_params'] - df['model_params'].min()) / (df['model_params'].max() - df['model_params'].min())\n",
    "    \n",
    "    # Metrics for radar chart\n",
    "    metrics = ['norm_reward', 'norm_mse', 'norm_success', 'norm_train_time', 'norm_val_loss', 'norm_params']\n",
    "    metric_labels = ['Reward', 'MSE (inv)', 'Success Rate', 'Train Time (inv)', 'Val Loss (inv)', 'Model Size (inv)']\n",
    "    \n",
    "    # Number of metrics\n",
    "    N = len(metrics)\n",
    "    \n",
    "    # Compute angles for each metric\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = sns.color_palette(\"husl\", len(normalized_df))\n",
    "    \n",
    "    for i, (idx, row) in enumerate(normalized_df.iterrows()):\n",
    "        values = [row[metric] for metric in metrics]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['algorithm'], color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Algorithm Performance Radar Chart\\n(Higher values = better performance)', size=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_radar_chart(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a7e67",
   "metadata": {},
   "source": [
    "## Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173589c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV with hyperparameter information\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"algorithm_comparison_results_{timestamp}.csv\"\n",
    "\n",
    "# Create a more detailed DataFrame for export\n",
    "export_df = pd.DataFrame(results)\n",
    "\n",
    "# Convert hyperparameters to string format for CSV\n",
    "export_df['best_hyperparams_str'] = export_df['best_hyperparams'].apply(lambda x: str(x) if x else 'N/A')\n",
    "\n",
    "# Save main results\n",
    "export_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to: {csv_filename}\")\n",
    "\n",
    "# Create a detailed summary report\n",
    "summary_report = f\"\"\"\n",
    "ALGORITHM COMPARISON SUMMARY REPORT (WITH HYPERPARAMETER TUNING)\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "{'='*80}\n",
    "\n",
    "EXPERIMENTAL SETUP:\n",
    "- Number of training episodes: {CONFIG['num_episodes']}\n",
    "- Training epochs per algorithm: {CONFIG['num_epochs']}\n",
    "- Batch size: {CONFIG['batch_size']}\n",
    "- Test episodes: {CONFIG['num_test_episodes']}\n",
    "- Device: {CONFIG['device']}\n",
    "\n",
    "HYPERPARAMETER TUNING SETUP:\n",
    "- Search method: {CONFIG['search_method']}\n",
    "- Number of trials per algorithm: {CONFIG['n_trials']}\n",
    "- Cross-validation folds: {CONFIG['cv_folds']}\n",
    "- Early stopping patience: {CONFIG['patience']}\n",
    "- HP search epochs: {CONFIG['hp_epochs']}\n",
    "\n",
    "RESULTS RANKING:\n",
    "\n",
    "1. Best Overall Performance (Average Reward):\n",
    "   {export_df.loc[export_df['avg_reward'].idxmax(), 'algorithm']} - {export_df['avg_reward'].max():.3f}\n",
    "   Best hyperparams: {export_df.loc[export_df['avg_reward'].idxmax(), 'best_hyperparams']}\n",
    "\n",
    "2. Most Accurate Predictions (Lowest MSE):\n",
    "   {export_df.loc[export_df['avg_mse'].idxmin(), 'algorithm']} - {export_df['avg_mse'].min():.6f}\n",
    "   Best hyperparams: {export_df.loc[export_df['avg_mse'].idxmin(), 'best_hyperparams']}\n",
    "\n",
    "3. Highest Success Rate:\n",
    "   {export_df.loc[export_df['success_rate'].idxmax(), 'algorithm']} - {export_df['success_rate'].max():.3f}\n",
    "   Best hyperparams: {export_df.loc[export_df['success_rate'].idxmax(), 'best_hyperparams']}\n",
    "\n",
    "4. Fastest Training:\n",
    "   {export_df.loc[export_df['train_time'].idxmin(), 'algorithm']} - {export_df['train_time'].min():.2f}s\n",
    "   Best hyperparams: {export_df.loc[export_df['train_time'].idxmin(), 'best_hyperparams']}\n",
    "\n",
    "5. Best Validation Performance:\n",
    "   {export_df.loc[export_df['final_val_loss'].idxmin(), 'algorithm']} - {export_df['final_val_loss'].min():.6f}\n",
    "   Best hyperparams: {export_df.loc[export_df['final_val_loss'].idxmin(), 'best_hyperparams']}\n",
    "\n",
    "6. Best Cross-Validation Score (from hyperparameter search):\n",
    "   {export_df.loc[export_df['hp_search_cv_score'].idxmin(), 'algorithm']} - {export_df['hp_search_cv_score'].min():.6f}\n",
    "\n",
    "DETAILED RESULTS:\n",
    "{export_df[['algorithm', 'avg_reward', 'avg_mse', 'success_rate', 'train_time', 'hp_search_cv_score']].to_string(index=False)}\n",
    "\n",
    "HYPERPARAMETER DETAILS:\n",
    "\"\"\"\n",
    "\n",
    "for _, row in export_df.iterrows():\n",
    "    summary_report += f\"\"\"\n",
    "{row['algorithm']}:\n",
    "  Best hyperparameters: {row['best_hyperparams']}\n",
    "  HP search CV score: {row['hp_search_cv_score']:.6f}\n",
    "  Final validation loss: {row['final_val_loss']:.6f}\n",
    "  Model parameters: {row['model_params']:,}\n",
    "\"\"\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "- For real-time applications: Choose the algorithm with fastest training/inference\n",
    "- For accuracy-critical tasks: Choose the algorithm with lowest MSE\n",
    "- For exploration tasks: Choose the algorithm with highest success rate\n",
    "- For resource-constrained environments: Choose the algorithm with fewest parameters\n",
    "- For robust performance: Consider the algorithm with best cross-validation score\n",
    "\n",
    "METHODOLOGY NOTES:\n",
    "- All algorithms underwent {CONFIG['cv_folds']}-fold cross-validation hyperparameter tuning\n",
    "- Best hyperparameters were selected based on validation loss minimization\n",
    "- Final models were trained with full epochs using the best hyperparameters\n",
    "- This ensures fair comparison and optimal performance for each algorithm\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "summary_filename = f\"algorithm_comparison_summary_{timestamp}.txt\"\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"Summary report saved to: {summary_filename}\")\n",
    "print(\"\\nSummary Report:\")\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of AutoEncoder results after fix\n",
    "print(\"=\"*60)\n",
    "print(\"AUTOENCODER RESULTS SUMMARY (AFTER FIX)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(f\"AutoEncoder hyperparameter tuning: COMPLETED\")\n",
    "    print(f\"Best hyperparameters found: {ae_best_hyperparams}\")\n",
    "    print(f\"Best CV score: {ae_best_score:.6f}\")\n",
    "    print(f\"Training completed in: {ae_train_time:.2f} seconds\")\n",
    "    print(f\"Final validation loss: {ae_val_losses[-1]:.6f}\")\n",
    "    print(f\"Average MSE: {ae_results['avg_mse']:.6f}\")\n",
    "    print(f\"Average reward: {ae_results['avg_reward']:.3f}\")\n",
    "    print(f\"Success rate: {ae_results['success_rate']:.3f}\")\n",
    "    print(f\"Model parameters: {count_parameters(autoencoder_agent):,}\")\n",
    "    \n",
    "    print(f\"\\n🎉 AutoEncoder is working correctly! No more matrix dimension errors.\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"❌ Some variables not defined: {e}\")\n",
    "    print(\"Please run the AutoEncoder cell above first.\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
