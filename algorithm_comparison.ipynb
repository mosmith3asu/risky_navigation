{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578ac5ae",
   "metadata": {},
   "source": [
    "# Operator Action Prediction for Teleoperation with Communication Delays\n",
    "\n",
    "This notebook compares neural network architectures for predicting operator actions during communication delays in space teleoperation scenarios.\n",
    "\n",
    "## Problem Context\n",
    "\n",
    "**Space teleoperation faces critical communication delays:**\n",
    "- Mars missions: up to 22 minutes round-trip delay\n",
    "- Moon missions: 1.25 seconds one-way delay\n",
    "- Operators cannot respond to obstacles in real-time\n",
    "\n",
    "**Our Approach:**\n",
    "Learn forward models that predict operator actions `μ(t+τ)` based on:\n",
    "- Visual observations (robot position, obstacles, goal)\n",
    "- Historical action sequences\n",
    "- Individual operator risk preferences (future: CPT integration)\n",
    "\n",
    "## Algorithms Evaluated\n",
    "\n",
    "**Single-State Models:**\n",
    "- **Linear**: Simple baseline - `state → action` \n",
    "- **MLP** (called \"AutoEncoder\"): Deep network with bottleneck\n",
    "- **Bayesian**: Uncertainty-aware prediction with probabilistic weights\n",
    "- **VAE**: Variational autoencoder with latent action distribution\n",
    "\n",
    "**Temporal Sequence Models:**\n",
    "- **Transformer**: Self-attention over state history - `[s_t-k, ..., s_t] → a_t`\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Data Collection**: Expert demonstrations from optimal visibility graph policy\n",
    "2. **Model Training**: Supervised learning on (state, action) pairs\n",
    "3. **Evaluation**: Test models in environment using predicted actions\n",
    "4. **Analysis**: Compare prediction accuracy, uncertainty quantification, computational efficiency\n",
    "\n",
    "## Expected Results (from FURI Poster)\n",
    "\n",
    "VAE should achieve lowest MSE (~0.2088) with crucial uncertainty estimates for safe autonomous operation during communication blackouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adccd8a7",
   "metadata": {},
   "source": [
    "## ✅ Algorithm Implementation Status\n",
    "\n",
    "### Architecture Correctness for Action Prediction:\n",
    "\n",
    "1. **Linear Regression** ✅ **CORRECT**\n",
    "   - Simple `state → action` mapping\n",
    "   - Good baseline for teleoperation tasks\n",
    "   - Fast inference for real-time systems\n",
    "\n",
    "2. **MLP** (named \"AutoEncoder\") ✅ **CORRECT** \n",
    "   - Deep network with latent bottleneck: `state → latent → action`\n",
    "   - Note: Not a true autoencoder (doesn't reconstruct input)\n",
    "   - Appropriate for non-linear action prediction\n",
    "\n",
    "3. **Bayesian Neural Network** ✅ **CORRECT**\n",
    "   - Probabilistic weights with uncertainty estimates\n",
    "   - Uses ELBO loss: likelihood + KL divergence to prior\n",
    "   - Critical for knowing when predictions are unreliable (safety)\n",
    "   - **Fixed**: Added KL loss (was missing before)\n",
    "\n",
    "4. **VAE (Variational AutoEncoder)** ✅ **CORRECT**\n",
    "   - Encodes states to latent distribution, decodes to actions\n",
    "   - Proper ELBO: reconstruction loss + β * KL divergence\n",
    "   - Best for multi-modal action distributions\n",
    "   - **Fixed**: Corrected KL normalization (per-sample)\n",
    "   - **Expected**: Lowest prediction error per FURI poster\n",
    "\n",
    "5. **Transformer** ✅ **CORRECT** (with sequences)\n",
    "   - For temporal modeling: `[s_t-k, ..., s_t] → a_t`\n",
    "   - Self-attention captures state history dependencies\n",
    "   - Falls back to MLP if `sequence_len=1` (more efficient)\n",
    "   - **Fixed**: Now accepts temporal sequences instead of fake length-1 sequences\n",
    "\n",
    "### Key Changes Applied:\n",
    "\n",
    "✅ **All algorithms predict actions** (not next states - this IS behavioral cloning)  \n",
    "✅ **Action bounds enforced** for safety (`[0.0, -π/6]` to `[2.0, π/6]`)  \n",
    "✅ **Bayesian uses ELBO** with KL divergence (not just MSE)  \n",
    "✅ **VAE uses proper ELBO** with correct normalization  \n",
    "✅ **Transformer supports sequences** for time-series forecasting  \n",
    "✅ **Save/load methods** for all models  \n",
    "\n",
    "### Research Context (FURI Project):\n",
    "\n",
    "This work addresses **NASA's communication delay challenges** in space exploration:\n",
    "- Predict operator actions during signal blackouts\n",
    "- Enable rovers to continue productive work autonomously\n",
    "- Maintain operator risk preferences (future: CPT integration)\n",
    "- Critical for deep-space missions with extended delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077109",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn torch torchvision torchaudio gymnasium tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c632d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/risky_navigation')\n",
    "\n",
    "from src.env.continuous_nav_env import ContinuousNavigationEnv\n",
    "from src.algorithms.AutoEncoder.agent import AutoEncoderAgent\n",
    "from src.algorithms.Bayesian.agent import BayesianAgent\n",
    "from src.algorithms.Transformer.agent import TransformerAgent\n",
    "from src.algorithms.Linear.agent import LinearAgent\n",
    "from src.algorithms.VAE.agent import VAEAgent\n",
    "from src.utils.file_management import save_pickle, load_pickle\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817ec8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a17c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'num_episodes': 1000,\n",
    "    'max_steps': 200,\n",
    "    'batch_size': 512,  # Increased for RTX 4090\n",
    "    'num_epochs': 200,\n",
    "    'val_ratio': 0.2,\n",
    "    'num_test_episodes': 50,\n",
    "    'lr': 1e-3,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'num_workers': 4,  # Parallel data loading\n",
    "    'prefetch_factor': 2,\n",
    "}\n",
    "\n",
    "# Grid search configurations for each algorithm\n",
    "GRID_SEARCH_CONFIGS = {\n",
    "    'AutoEncoder': {\n",
    "        'latent_dim': [16, 32, 64],\n",
    "        'hidden_dims': [[128, 64], [256, 128], [256, 128, 64]],\n",
    "        'dropout': [0.0, 0.1, 0.2],\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    },\n",
    "    'Transformer': {\n",
    "        'd_model': [32, 64, 128],\n",
    "        'nhead': [4, 8],\n",
    "        'num_layers': [2, 3, 4],\n",
    "        'dropout': [0.0, 0.1, 0.2],\n",
    "        'sequence_len': [1, 5, 10],  # Added: temporal sequence length for time-series modeling\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    },\n",
    "    'Bayesian': {\n",
    "        'hidden_dim': [64, 128, 256],\n",
    "        'prior_std': [0.5, 1.0, 2.0],\n",
    "        'kl_weight': [1e-5, 1e-4, 1e-3],  # Added: KL divergence weight for ELBO\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    },\n",
    "    'VAE': {\n",
    "        'latent_dim': [16, 32, 64],\n",
    "        'hidden_dim': [64, 128, 256],\n",
    "        'beta': [0.5, 1.0, 2.0],\n",
    "        'lr': [1e-3, 5e-4, 1e-4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Best baseline configs (for quick comparison)\n",
    "MODEL_CONFIGS = {\n",
    "    'AutoEncoder': {'latent_dim': 32, 'hidden_dims': [128, 64], 'dropout': 0.1},\n",
    "    'Linear': {},\n",
    "    'Transformer': {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1, 'sequence_len': 1},\n",
    "    'Bayesian': {'hidden_dim': 128, 'prior_std': 1.0, 'kl_weight': 1e-5},\n",
    "    'VAE': {'latent_dim': 32, 'hidden_dim': 128, 'beta': 1.0}\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']} (optimized for RTX 4090)\")\n",
    "print(f\"Max epochs: {CONFIG['num_epochs']} (with early stopping)\")\n",
    "print(f\"Grid search enabled for: {list(GRID_SEARCH_CONFIGS.keys())}\")\n",
    "print(f\"\\nTransformer will test sequence lengths: {GRID_SEARCH_CONFIGS['Transformer']['sequence_len']}\")\n",
    "print(f\"  - sequence_len=1: Single state (MLP mode)\")\n",
    "print(f\"  - sequence_len>1: Temporal sequences (true transformer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # RTX 4090 optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "    # Enable cuDNN autotuner for optimal convolution algorithms\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    \n",
    "    # Memory optimizations\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Mixed precision training setup\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    use_amp = True\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"GPU OPTIMIZATIONS ENABLED FOR RTX 4090\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✓ Batch size: {CONFIG['batch_size']}\")\n",
    "    print(f\"✓ TF32 matmul: Enabled\")\n",
    "    print(f\"✓ cuDNN benchmark: Enabled\")\n",
    "    print(f\"✓ Mixed precision (AMP): Enabled\")\n",
    "    print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✓ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    use_amp = False\n",
    "    scaler = None\n",
    "    print(\"WARNING: CUDA not available. Running on CPU will be very slow!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622191",
   "metadata": {},
   "source": [
    "## Training Functions with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_optimized(agent, train_states, train_expert_actions, train_goals,\n",
    "                          val_states, val_expert_actions, val_goals, \n",
    "                          num_epochs=100, batch_size=256, device='cpu', use_amp=False, \n",
    "                          scaler=None, verbose=True):\n",
    "    \"\"\"Optimized training with mixed precision, early stopping, LR scheduling, and gradient clipping.\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    n_train = len(train_states)\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    if hasattr(agent, 'optimizer'):\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            agent.optimizer, \n",
    "            max_lr=agent.optimizer.param_groups[0]['lr'] * 10,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=(n_train + batch_size - 1) // batch_size,\n",
    "            pct_start=0.3,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        indices = np.random.permutation(n_train)\n",
    "        \n",
    "        for start_idx in range(0, n_train, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_train)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            batch_states = torch.tensor(train_states[batch_indices], dtype=torch.float32, device=device)\n",
    "            batch_actions = torch.tensor(train_expert_actions[batch_indices], dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Mixed precision training (only if optimizer exists and loss is tensor)\n",
    "            if use_amp and scaler is not None and hasattr(agent, 'optimizer'):\n",
    "                with autocast():\n",
    "                    loss = agent.train_step(batch_states, None, None, batch_actions)\n",
    "                \n",
    "                # Check if loss is a tensor (some agents return float after backward)\n",
    "                if isinstance(loss, torch.Tensor):\n",
    "                    agent.optimizer.zero_grad()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(agent.optimizer)\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    if hasattr(agent, 'model'):\n",
    "                        torch.nn.utils.clip_grad_norm_(agent.model.parameters(), 1.0)\n",
    "                    elif hasattr(agent, 'encoder'):\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(agent.encoder.parameters()) + list(agent.decoder.parameters()), 1.0\n",
    "                        )\n",
    "                    \n",
    "                    scaler.step(agent.optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    loss = loss.item()\n",
    "                else:\n",
    "                    # Loss already computed and stepped inside train_step\n",
    "                    scheduler.step()\n",
    "            else:\n",
    "                # Standard training (no AMP)\n",
    "                loss = agent.train_step(batch_states, None, None, batch_actions)\n",
    "                \n",
    "                # Gradient clipping (if optimizer exists and hasn't been stepped yet)\n",
    "                if hasattr(agent, 'optimizer') and isinstance(loss, torch.Tensor):\n",
    "                    if hasattr(agent, 'model'):\n",
    "                        torch.nn.utils.clip_grad_norm_(agent.model.parameters(), 1.0)\n",
    "                    elif hasattr(agent, 'encoder'):\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(agent.encoder.parameters()) + list(agent.decoder.parameters()), 1.0\n",
    "                        )\n",
    "                    scheduler.step()\n",
    "                \n",
    "                # Convert tensor to float if needed\n",
    "                if isinstance(loss, torch.Tensor):\n",
    "                    loss = loss.item()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_states_t = torch.tensor(val_states, dtype=torch.float32, device=device)\n",
    "        val_actions_t = torch.tensor(val_expert_actions, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if hasattr(agent, 'model'):\n",
    "                agent.model.eval()\n",
    "                predictions = agent.model(val_states_t)\n",
    "                agent.model.train()\n",
    "            elif hasattr(agent, 'encoder'):\n",
    "                agent.encoder.eval()\n",
    "                agent.decoder.eval()\n",
    "                mu, _ = agent.encoder(val_states_t)\n",
    "                predictions = agent.decoder(mu)\n",
    "                agent.encoder.train()\n",
    "                agent.decoder.train()\n",
    "            else:\n",
    "                predictions = agent.predict_action(val_states_t, None)\n",
    "            \n",
    "            val_loss = torch.nn.functional.mse_loss(predictions, val_actions_t).item()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save best model\n",
    "            if hasattr(agent, 'model'):\n",
    "                best_model_state = agent.model.state_dict().copy()\n",
    "            elif hasattr(agent, 'encoder'):\n",
    "                best_model_state = {\n",
    "                    'encoder': agent.encoder.state_dict().copy(),\n",
    "                    'decoder': agent.decoder.state_dict().copy()\n",
    "                }\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stop at epoch {epoch}. Best val: {best_val_loss:.6f}\")\n",
    "                # Restore best\n",
    "                if best_model_state:\n",
    "                    if hasattr(agent, 'model'):\n",
    "                        agent.model.load_state_dict(best_model_state)\n",
    "                    elif hasattr(agent, 'encoder'):\n",
    "                        agent.encoder.load_state_dict(best_model_state['encoder'])\n",
    "                        agent.decoder.load_state_dict(best_model_state['decoder'])\n",
    "                break\n",
    "        \n",
    "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
    "            current_lr = agent.optimizer.param_groups[0]['lr'] if hasattr(agent, 'optimizer') else 0\n",
    "            print(f\"Epoch {epoch:3d}/{num_epochs}: Train={avg_train_loss:.6f}, Val={val_loss:.6f}, LR={current_lr:.6f}\")\n",
    "            \n",
    "            if np.isnan(avg_train_loss) or np.isnan(val_loss):\n",
    "                print(f\"WARNING: NaN at epoch {epoch}!\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"Optimized training function with mixed precision ready!\")\n",
    "print(\"Fixed: Handles both tensor and float returns from train_step()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d212700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def grid_search_algorithm(AgentClass, algo_name, grid_params, \n",
    "                          train_states, train_actions, train_goals,\n",
    "                          val_states, val_actions, val_goals,\n",
    "                          state_dim, action_dim, device, use_amp, scaler):\n",
    "    \"\"\"\n",
    "    Perform grid search for a single algorithm.\n",
    "    Returns: list of (config, agent, train_losses, val_losses, val_loss, train_time)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GRID SEARCH: {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate all combinations\n",
    "    param_names = list(grid_params.keys())\n",
    "    param_values = [grid_params[k] for k in param_names]\n",
    "    all_configs = list(itertools.product(*param_values))\n",
    "    \n",
    "    print(f\"Total configurations to test: {len(all_configs)}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, config_values in enumerate(all_configs, 1):\n",
    "        config = dict(zip(param_names, config_values))\n",
    "        \n",
    "        # Extract learning rate if present\n",
    "        lr = config.pop('lr', CONFIG['lr'])\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(all_configs)}] Testing config: {config}\")\n",
    "        \n",
    "        try:\n",
    "            # Create agent with this config\n",
    "            agent = AgentClass(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                lr=lr,\n",
    "                device=device,\n",
    "                **config\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            start_time = time.time()\n",
    "            train_losses, val_losses = train_agent_optimized(\n",
    "                agent, \n",
    "                train_states, train_actions, train_goals,\n",
    "                val_states, val_actions, val_goals,\n",
    "                num_epochs=CONFIG['num_epochs'],\n",
    "                batch_size=CONFIG['batch_size'],\n",
    "                device=device,\n",
    "                use_amp=use_amp,\n",
    "                scaler=scaler,\n",
    "                verbose=False\n",
    "            )\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            final_val_loss = val_losses[-1]\n",
    "            \n",
    "            # Store complete config including lr\n",
    "            full_config = {**config, 'lr': lr}\n",
    "            results.append((full_config, agent, train_losses, val_losses, final_val_loss, train_time))\n",
    "            \n",
    "            print(f\"  ✓ Val Loss: {final_val_loss:.6f}, Time: {train_time:.1f}s\")\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort by validation loss\n",
    "    results.sort(key=lambda x: x[4])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GRID SEARCH COMPLETE: {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best config: {results[0][0]}\")\n",
    "    print(f\"Best val loss: {results[0][4]:.6f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Grid search function ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652df29",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rl_experience(env, num_episodes=100, max_steps=200):\n",
    "    \"\"\"Collect RL training data using optimal policy with proper goal stopping.\"\"\"\n",
    "    data = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes), desc='Collecting RL experience'):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            current_pos = state[:2]\n",
    "            current_theta = state[2]\n",
    "            current_velocity = state[3]  # Current velocity from state\n",
    "            dist_to_goal = np.linalg.norm(current_pos - goal)\n",
    "            \n",
    "            # Calculate desired heading towards goal\n",
    "            direction = goal - current_pos\n",
    "            desired_theta = np.arctan2(direction[1], direction[0])\n",
    "            angle_diff = desired_theta - current_theta\n",
    "            angle_diff = (angle_diff + np.pi) % (2 * np.pi) - np.pi\n",
    "            steering = np.clip(angle_diff, env.action_space.low[1], env.action_space.high[1])\n",
    "            \n",
    "            # IMPROVED THROTTLE CONTROL: Gradual deceleration near goal\n",
    "            # Goal requires: distance <= 0.5 AND velocity < 0.1\n",
    "            if dist_to_goal < env.goal_radius * 3:  # Within 1.5 units of goal\n",
    "                # Proportional throttle based on distance\n",
    "                # At distance 1.5: throttle = 0.3 * max\n",
    "                # At distance 0.5: throttle = 0.1 * max  \n",
    "                # At distance 0.0: throttle = 0.0\n",
    "                throttle_factor = max(0.0, min(1.0, (dist_to_goal / (env.goal_radius * 3)) * 0.3))\n",
    "                throttle = env.action_space.high[0] * throttle_factor\n",
    "            else:\n",
    "                # Full speed when far from goal\n",
    "                throttle = env.action_space.high[0]\n",
    "            \n",
    "            # If very close and moving slowly, stop completely\n",
    "            if dist_to_goal < env.goal_radius and current_velocity < env.goal_velocity * 1.5:\n",
    "                throttle = 0.0\n",
    "                steering = 0.0\n",
    "            \n",
    "            action = np.array([throttle, steering])\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            data.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action.copy(),\n",
    "                'reward': reward,\n",
    "                'next_state': next_state.copy(),\n",
    "                'done': done,\n",
    "                'goal': goal.copy()\n",
    "            })\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    successful_episodes += 1\n",
    "                break\n",
    "    \n",
    "    print(f\"Collected {len(data)} transitions from {num_episodes} episodes\")\n",
    "    print(f\"Success rate: {successful_episodes/num_episodes:.2%}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"Data collection function defined!\")\n",
    "print(\"IMPROVED: Gradual deceleration with proportional throttle control near goal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a347d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or collect data\n",
    "dataset_path = 'rl_experience_dataset.pickle'\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Deleting old dataset (0% success rate)...\")\n",
    "    os.remove(dataset_path)\n",
    "\n",
    "print(f\"Collecting new dataset with fixed expert policy...\")\n",
    "env_collect = ContinuousNavigationEnv()\n",
    "data = collect_rl_experience(env_collect, num_episodes=CONFIG['num_episodes'], max_steps=CONFIG['max_steps'])\n",
    "save_pickle(data, dataset_path)\n",
    "print(f\"✓ Saved dataset to {dataset_path}\")\n",
    "\n",
    "# Extract states, actions, and goals from data\n",
    "states = np.array([d['state'] for d in data])\n",
    "actions = np.array([d['action'] for d in data])\n",
    "next_states = np.array([d['next_state'] for d in data])\n",
    "rewards = np.array([d['reward'] for d in data])\n",
    "dones = np.array([d['done'] for d in data])\n",
    "goals = np.array([d['goal'] for d in data])\n",
    "\n",
    "print(f\"\\nData extracted:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Actions shape: {actions.shape}\")\n",
    "print(f\"  Goals shape: {goals.shape}\")\n",
    "print(f\"  Rewards shape: {rewards.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check environment goal requirements\n",
    "env_test = ContinuousNavigationEnv()\n",
    "print(\"Environment Goal Requirements:\")\n",
    "print(f\"  Goal position: {env_test.goal}\")\n",
    "print(f\"  Goal radius: {env_test.goal_radius}\")\n",
    "print(f\"  Goal velocity threshold: {env_test.goal_velocity}\")\n",
    "print(f\"  Max throttle: {env_test.max_throttle}\")\n",
    "print(f\"  Max steps: {env_test.max_steps}\")\n",
    "print(f\"\\nTo reach goal, agent must:\")\n",
    "print(f\"  1. Be within {env_test.goal_radius} units of {env_test.goal}\")\n",
    "print(f\"  2. Have velocity < {env_test.goal_velocity}\")\n",
    "print(f\"\\nSlowing strategy:\")\n",
    "print(f\"  - Full throttle when distance > {env_test.goal_radius * 3:.2f}\")\n",
    "print(f\"  - Gradual deceleration when distance < {env_test.goal_radius * 3:.2f}\")\n",
    "print(f\"  - Stop when distance < {env_test.goal_radius} and velocity < {env_test.goal_velocity * 1.5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions from the collected data\n",
    "STATE_DIM = states.shape[1]\n",
    "ACTION_DIM = actions.shape[1]\n",
    "GOAL_DIM = goals.shape[1]\n",
    "\n",
    "print(f\"Data dimensions:\")\n",
    "print(f\"  STATE_DIM = {STATE_DIM}\")\n",
    "print(f\"  ACTION_DIM = {ACTION_DIM}\")\n",
    "print(f\"  GOAL_DIM = {GOAL_DIM}\")\n",
    "print(f\"  Total samples = {len(states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment for evaluation\n",
    "env = ContinuousNavigationEnv()\n",
    "print(f\"Environment initialized: {env}\")\n",
    "print(f\"  State space: {env.observation_space.shape}\")\n",
    "print(f\"  Action space: {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49cc82",
   "metadata": {},
   "source": [
    "## Grid Search Training - All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GRID SEARCH TRAINING - ALL ALGORITHMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data\n",
    "n_samples = len(states)\n",
    "n_train = int(n_samples * (1 - CONFIG['val_ratio']))\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "train_states = states[train_indices]\n",
    "train_actions = actions[train_indices]\n",
    "train_goals = goals[train_indices]\n",
    "\n",
    "val_states = states[val_indices]\n",
    "val_actions = actions[val_indices]\n",
    "val_goals = goals[val_indices]\n",
    "\n",
    "print(f\"Data split: {len(train_states)} train, {len(val_states)} val\")\n",
    "print(f\"Mixed precision: {use_amp}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max epochs: {CONFIG['num_epochs']}\\n\")\n",
    "\n",
    "# Store all grid search results\n",
    "grid_search_results = {}\n",
    "\n",
    "# Algorithms with grid search\n",
    "algorithms_with_grid = {\n",
    "    'AutoEncoder': (AutoEncoderAgent, GRID_SEARCH_CONFIGS['AutoEncoder']),\n",
    "    'Transformer': (TransformerAgent, GRID_SEARCH_CONFIGS['Transformer']),\n",
    "    'Bayesian': (BayesianAgent, GRID_SEARCH_CONFIGS['Bayesian']),\n",
    "    'VAE': (VAEAgent, GRID_SEARCH_CONFIGS['VAE'])\n",
    "}\n",
    "\n",
    "# Train Linear baseline (no grid search needed)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Linear (Baseline - No Grid Search)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "linear_agent = LinearAgent(\n",
    "    state_dim=STATE_DIM,\n",
    "    action_dim=ACTION_DIM,\n",
    "    lr=CONFIG['lr'],\n",
    "    device=CONFIG['device']\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "train_losses, val_losses = train_agent_optimized(\n",
    "    linear_agent,\n",
    "    train_states, train_actions, train_goals,\n",
    "    val_states, val_actions, val_goals,\n",
    "    num_epochs=CONFIG['num_epochs'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=CONFIG['device'],\n",
    "    use_amp=use_amp,\n",
    "    scaler=scaler,\n",
    "    verbose=True\n",
    ")\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "grid_search_results['Linear'] = [{\n",
    "    'config': {},\n",
    "    'agent': linear_agent,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'final_val_loss': val_losses[-1],\n",
    "    'train_time': train_time\n",
    "}]\n",
    "\n",
    "print(f\"Linear Complete! Val loss: {val_losses[-1]:.6f}, Time: {train_time:.1f}s\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Run grid search for each algorithm\n",
    "for algo_name, (AgentClass, grid_params) in algorithms_with_grid.items():\n",
    "    results = grid_search_algorithm(\n",
    "        AgentClass, algo_name, grid_params,\n",
    "        train_states, train_actions, train_goals,\n",
    "        val_states, val_actions, val_goals,\n",
    "        STATE_DIM, ACTION_DIM, \n",
    "        CONFIG['device'], use_amp, scaler\n",
    "    )\n",
    "    \n",
    "    # Store results as list of dicts for easier access\n",
    "    grid_search_results[algo_name] = [\n",
    "        {\n",
    "            'config': config,\n",
    "            'agent': agent,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'train_time': train_time\n",
    "        }\n",
    "        for config, agent, train_losses, val_losses, final_val_loss, train_time in results\n",
    "    ]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GRID SEARCH COMPLETE - ALL ALGORITHMS!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Print summary of best configs\n",
    "print(\"\\nBEST CONFIGURATIONS:\")\n",
    "print(f\"{'Algorithm':<15} {'Val Loss':<12} {'Config'}\")\n",
    "print(\"-\" * 80)\n",
    "for algo_name, results_list in grid_search_results.items():\n",
    "    best = results_list[0]  # Already sorted by val loss\n",
    "    config_str = ', '.join([f\"{k}={v}\" for k, v in best['config'].items()])\n",
    "    print(f\"{algo_name:<15} {best['final_val_loss']:<12.6f} {config_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Grid Search Visualization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZING GRID SEARCH RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Figure 1: Training and Validation Curves for Top 5 Configs per Algorithm\n",
    "fig1, axes1 = plt.subplots(len(grid_search_results), 2, figsize=(18, 4*len(grid_search_results)))\n",
    "if len(grid_search_results) == 1:\n",
    "    axes1 = axes1.reshape(1, -1)\n",
    "\n",
    "for idx, (algo_name, results_list) in enumerate(grid_search_results.items()):\n",
    "    # Training curves\n",
    "    ax_train = axes1[idx, 0]\n",
    "    ax_val = axes1[idx, 1]\n",
    "    \n",
    "    # Plot top 5 configs\n",
    "    for i, result in enumerate(results_list[:5]):\n",
    "        epochs = range(len(result['train_losses']))\n",
    "        \n",
    "        # Config label with key parameters\n",
    "        if result['config']:\n",
    "            key_params = {k: v for k, v in result['config'].items() if k in ['lr', 'latent_dim', 'd_model', 'hidden_dim']}\n",
    "            config_label = f\"Rank {i+1}: \" + \", \".join([f\"{k}={v}\" for k, v in key_params.items()])\n",
    "        else:\n",
    "            config_label = \"Baseline\"\n",
    "        \n",
    "        # Training loss\n",
    "        ax_train.plot(epochs, result['train_losses'], label=config_label, alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Validation loss\n",
    "        ax_val.plot(epochs, result['val_losses'], label=config_label, alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Configure training plot\n",
    "    ax_train.set_xlabel('Epoch', fontsize=11)\n",
    "    ax_train.set_ylabel('Training Loss', fontsize=11)\n",
    "    ax_train.set_title(f'{algo_name} - Training Loss (Top 5 Configs)', fontsize=12, fontweight='bold')\n",
    "    ax_train.legend(fontsize=9, loc='best')\n",
    "    ax_train.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax_train.set_yscale('log')\n",
    "    \n",
    "    # Configure validation plot\n",
    "    ax_val.set_xlabel('Epoch', fontsize=11)\n",
    "    ax_val.set_ylabel('Validation Loss', fontsize=11)\n",
    "    ax_val.set_title(f'{algo_name} - Validation Loss (Top 5 Configs)', fontsize=12, fontweight='bold')\n",
    "    ax_val.legend(fontsize=9, loc='best')\n",
    "    ax_val.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax_val.set_yscale('log')\n",
    "    \n",
    "    # Highlight best config\n",
    "    best = results_list[0]\n",
    "    best_epochs = range(len(best['val_losses']))\n",
    "    ax_val.plot(best_epochs, best['val_losses'], 'r--', linewidth=3, alpha=0.5, \n",
    "                label='Best Config', zorder=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Figure 2: Performance Comparison - Val Loss vs Training Time\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(grid_search_results)))\n",
    "markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'h']\n",
    "\n",
    "for idx, (algo_name, results_list) in enumerate(grid_search_results.items()):\n",
    "    val_losses = [r['final_val_loss'] for r in results_list[:10]]  # Top 10\n",
    "    train_times = [r['train_time'] for r in results_list[:10]]\n",
    "    \n",
    "    # Plot scatter\n",
    "    ax2.scatter(train_times, val_losses, \n",
    "               s=200, alpha=0.6, \n",
    "               color=colors[idx], \n",
    "               marker=markers[idx % len(markers)],\n",
    "               label=algo_name,\n",
    "               edgecolors='black', linewidth=1.5)\n",
    "    \n",
    "    # Annotate best config\n",
    "    ax2.annotate(f'{algo_name}\\nBest', \n",
    "                xy=(train_times[0], val_losses[0]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=9, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor=colors[idx], alpha=0.3),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', lw=1.5))\n",
    "\n",
    "ax2.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Final Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Grid Search: Validation Loss vs Training Time\\n(Pareto Frontier Analysis)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='best', framealpha=0.9)\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "ax2.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Figure 3: Hyperparameter Impact Analysis\n",
    "fig3, axes3 = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes3 = axes3.flatten()\n",
    "\n",
    "# Analyze impact of key hyperparameters\n",
    "hyperparams_to_analyze = ['lr', 'dropout', 'latent_dim', 'hidden_dim', 'd_model', 'beta']\n",
    "\n",
    "plot_idx = 0\n",
    "for param in hyperparams_to_analyze[:4]:  # Top 4 most common params\n",
    "    ax = axes3[plot_idx]\n",
    "    \n",
    "    for algo_name, results_list in grid_search_results.items():\n",
    "        # Extract configs with this parameter\n",
    "        param_vals = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for result in results_list:\n",
    "            if param in result['config']:\n",
    "                param_val = result['config'][param]\n",
    "                # Handle list values (like hidden_dims)\n",
    "                if isinstance(param_val, list):\n",
    "                    param_val = str(param_val)\n",
    "                param_vals.append(param_val)\n",
    "                val_losses.append(result['final_val_loss'])\n",
    "        \n",
    "        if param_vals:\n",
    "            # Group by unique param values\n",
    "            unique_vals = list(set(param_vals))\n",
    "            if all(isinstance(v, (int, float)) for v in unique_vals):\n",
    "                unique_vals = sorted(unique_vals)\n",
    "                avg_losses = [np.mean([val_losses[i] for i, v in enumerate(param_vals) if v == uv]) \n",
    "                             for uv in unique_vals]\n",
    "                \n",
    "                ax.plot(unique_vals, avg_losses, marker='o', linewidth=2, \n",
    "                       markersize=8, label=algo_name, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(param.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Avg Validation Loss', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Impact of {param.replace(\"_\", \" \").title()} on Performance', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plot_idx += 1\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(plot_idx, 4):\n",
    "    axes3[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed results table with more metrics\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"GRID SEARCH RESULTS - TOP 5 CONFIGS PER ALGORITHM\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "for algo_name, results_list in grid_search_results.items():\n",
    "    print(f\"\\n{algo_name}:\")\n",
    "    print(f\"{'Rank':<6} {'Val Loss':<12} {'Train Loss':<12} {'Time(s)':<10} {'Epochs':<8} {'Config'}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for rank, result in enumerate(results_list[:5], 1):\n",
    "        config_str = ', '.join([f\"{k}={v}\" for k, v in result['config'].items()])\n",
    "        if not config_str:\n",
    "            config_str = \"N/A (baseline)\"\n",
    "        \n",
    "        final_train_loss = result['train_losses'][-1]\n",
    "        num_epochs = len(result['train_losses'])\n",
    "        \n",
    "        print(f\"{rank:<6} {result['final_val_loss']:<12.6f} {final_train_loss:<12.6f} \"\n",
    "              f\"{result['train_time']:<10.1f} {num_epochs:<8} {config_str}\")\n",
    "\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"GRID SEARCH SUMMARY STATISTICS\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Algorithm':<15} {'Configs':<10} {'Best Val':<12} {'Worst Val':<12} \"\n",
    "      f\"{'Avg Val':<12} {'Std Val':<12} {'Avg Time(s)':<12}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for algo_name, results_list in grid_search_results.items():\n",
    "    val_losses = [r['final_val_loss'] for r in results_list]\n",
    "    train_times = [r['train_time'] for r in results_list]\n",
    "    \n",
    "    print(f\"{algo_name:<15} {len(results_list):<10} {min(val_losses):<12.6f} \"\n",
    "          f\"{max(val_losses):<12.6f} {np.mean(val_losses):<12.6f} \"\n",
    "          f\"{np.std(val_losses):<12.6f} {np.mean(train_times):<12.1f}\")\n",
    "\n",
    "print(\"=\"*120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cee6b",
   "metadata": {},
   "source": [
    "## Evaluate Best Models from Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING BEST MODELS FROM GRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_agent_optimized(agent, env, num_episodes=50, max_steps=200, device='cpu'):\n",
    "    \"\"\"Evaluate agent with detailed metrics.\"\"\"\n",
    "    results = {\n",
    "        'rewards': [], \n",
    "        'successes': [], \n",
    "        'steps': [],\n",
    "        'final_distances': [],\n",
    "        'final_velocities': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action = agent.predict_action(state_t, None)\n",
    "            \n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.cpu().numpy()\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        results['rewards'].append(episode_reward)\n",
    "        success = info.get('reason', '') == 'goal_reached' if done else False\n",
    "        results['successes'].append(1 if success else 0)\n",
    "        results['steps'].append(step + 1)\n",
    "        \n",
    "        # Track final state metrics\n",
    "        final_dist = np.linalg.norm(state[:2] - env.goal)\n",
    "        final_vel = state[3]\n",
    "        results['final_distances'].append(final_dist)\n",
    "        results['final_velocities'].append(final_vel)\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(results['rewards']),\n",
    "        'std_reward': np.std(results['rewards']),\n",
    "        'success_rate': np.mean(results['successes']),\n",
    "        'avg_steps': np.mean(results['steps']),\n",
    "        'avg_final_distance': np.mean(results['final_distances']),\n",
    "        'avg_final_velocity': np.mean(results['final_velocities'])\n",
    "    }\n",
    "\n",
    "eval_results = {}\n",
    "\n",
    "# Evaluate best model from each algorithm\n",
    "for algo_name, results_list in grid_search_results.items():\n",
    "    print(f\"\\nEvaluating {algo_name} (best config)...\")\n",
    "    best_result = results_list[0]\n",
    "    agent = best_result['agent']\n",
    "    \n",
    "    eval_res = evaluate_agent_optimized(\n",
    "        agent, env, \n",
    "        num_episodes=CONFIG['num_test_episodes'],\n",
    "        max_steps=CONFIG['max_steps'],\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    eval_results[algo_name] = eval_res\n",
    "    \n",
    "    print(f\"  Avg Reward: {eval_res['avg_reward']:.3f} ± {eval_res['std_reward']:.3f}\")\n",
    "    print(f\"  Success Rate: {eval_res['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Steps: {eval_res['avg_steps']:.1f}\")\n",
    "    print(f\"  Avg Final Distance: {eval_res['avg_final_distance']:.3f}\")\n",
    "    print(f\"  Avg Final Velocity: {eval_res['avg_final_velocity']:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results comparison\n",
    "import pandas as pd\n",
    "\n",
    "results_data = []\n",
    "for algo_name in grid_search_results.keys():\n",
    "    best_train_res = grid_search_results[algo_name][0]\n",
    "    eval_res = eval_results[algo_name]\n",
    "    \n",
    "    results_data.append({\n",
    "        'Algorithm': algo_name,\n",
    "        'Train Loss': best_train_res['train_losses'][-1],\n",
    "        'Val Loss': best_train_res['final_val_loss'],\n",
    "        'Train Time (s)': best_train_res['train_time'],\n",
    "        'Configs Tested': len(grid_search_results[algo_name]),\n",
    "        'Success Rate': eval_res['success_rate'],\n",
    "        'Avg Reward': eval_res['avg_reward'],\n",
    "        'Avg Steps': eval_res['avg_steps'],\n",
    "        'Final Distance': eval_res['avg_final_distance'],\n",
    "        'Final Velocity': eval_res['avg_final_velocity']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "df_results = df_results.sort_values('Val Loss')\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"FINAL RESULTS SUMMARY - BEST CONFIGS FROM GRID SEARCH\")\n",
    "print(\"=\"*120)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Visualize comprehensive results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "# Success Rate\n",
    "axes[0, 0].bar(df_results['Algorithm'], df_results['Success Rate'], color='skyblue')\n",
    "axes[0, 0].set_ylabel('Success Rate')\n",
    "axes[0, 0].set_title('Success Rate by Algorithm (Best Configs)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].set_ylim([0, 1.1])\n",
    "\n",
    "# Average Reward\n",
    "axes[0, 1].bar(df_results['Algorithm'], df_results['Avg Reward'], color='lightgreen')\n",
    "axes[0, 1].set_ylabel('Average Reward')\n",
    "axes[0, 1].set_title('Average Reward by Algorithm')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Validation Loss\n",
    "axes[0, 2].bar(df_results['Algorithm'], df_results['Val Loss'], color='salmon')\n",
    "axes[0, 2].set_ylabel('Validation Loss')\n",
    "axes[0, 2].set_title('Validation Loss by Algorithm')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 2].set_yscale('log')\n",
    "\n",
    "# Training Time\n",
    "axes[1, 0].bar(df_results['Algorithm'], df_results['Train Time (s)'], color='plum')\n",
    "axes[1, 0].set_ylabel('Training Time (s)')\n",
    "axes[1, 0].set_title('Training Time by Algorithm')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Configs Tested\n",
    "axes[1, 1].bar(df_results['Algorithm'], df_results['Configs Tested'], color='gold')\n",
    "axes[1, 1].set_ylabel('Number of Configs')\n",
    "axes[1, 1].set_title('Configs Tested in Grid Search')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Final Distance to Goal\n",
    "axes[1, 2].bar(df_results['Algorithm'], df_results['Final Distance'], color='lightcoral')\n",
    "axes[1, 2].set_ylabel('Distance')\n",
    "axes[1, 2].set_title('Avg Final Distance to Goal')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 2].axhline(y=env.goal_radius, color='r', linestyle='--', label='Goal Radius')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best config details\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"BEST CONFIGURATION DETAILS\")\n",
    "print(\"=\"*120)\n",
    "for algo_name, results_list in grid_search_results.items():\n",
    "    best = results_list[0]\n",
    "    print(f\"\\n{algo_name}:\")\n",
    "    print(f\"  Config: {best['config']}\")\n",
    "    print(f\"  Val Loss: {best['final_val_loss']:.6f}\")\n",
    "    print(f\"  Training Time: {best['train_time']:.1f}s\")\n",
    "    print(f\"  Success Rate: {eval_results[algo_name]['success_rate']:.1%}\")\n",
    "print(\"=\"*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf01ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best models\n",
    "import os\n",
    "save_dir = 'trained_models/grid_search_best'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for algo_name, results_list in grid_search_results.items():\n",
    "    best_result = results_list[0]\n",
    "    agent = best_result['agent']\n",
    "    \n",
    "    # Save model weights\n",
    "    model_path = os.path.join(save_dir, f'{algo_name}_best.pth')\n",
    "    \n",
    "    if hasattr(agent, 'model'):\n",
    "        torch.save(agent.model.state_dict(), model_path)\n",
    "    elif hasattr(agent, 'encoder') and hasattr(agent, 'decoder'):\n",
    "        torch.save({\n",
    "            'encoder': agent.encoder.state_dict(),\n",
    "            'decoder': agent.decoder.state_dict()\n",
    "        }, model_path)\n",
    "    \n",
    "    # Save config\n",
    "    config_path = os.path.join(save_dir, f'{algo_name}_config.pkl')\n",
    "    save_pickle({\n",
    "        'config': best_result['config'],\n",
    "        'val_loss': best_result['final_val_loss'],\n",
    "        'train_time': best_result['train_time'],\n",
    "        'eval_results': eval_results[algo_name]\n",
    "    }, config_path)\n",
    "    \n",
    "    print(f\"✓ Saved {algo_name} to {save_dir}\")\n",
    "\n",
    "print(f\"\\n✓ All best models saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be17733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEBUGGING: Action and State Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "agent = all_results['Linear']['agent']\n",
    "state = env.reset()\n",
    "goal = env.goal.copy()\n",
    "\n",
    "print(f\"\\nInitial State: {state}\")\n",
    "print(f\"Goal Position: {goal}\")\n",
    "print(f\"Distance to Goal: {np.linalg.norm(state[:2] - goal):.3f}\")\n",
    "print(f\"\\nAction Space: [{env.action_space.low}, {env.action_space.high}]\")\n",
    "\n",
    "print(f\"\\n{'Step':<6} {'Action':<20} {'State[:2]':<20} {'Distance':<10} {'Reward':<8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for step in range(5):\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=CONFIG['device'])\n",
    "    action = agent.predict_action(state_t, None)\n",
    "    \n",
    "    if isinstance(action, torch.Tensor):\n",
    "        action = action.cpu().numpy()\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    distance = np.linalg.norm(next_state[:2] - goal)\n",
    "    \n",
    "    print(f\"{step:<6} {str(action):<20} {str(next_state[:2]):<20} {distance:<10.3f} {reward:<8.3f}\")\n",
    "    \n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(f\"\\nEpisode ended: {info.get('reason', 'unknown')}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training Data Statistics:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Expert actions - Mean: {actions.mean(axis=0)}\")\n",
    "print(f\"Expert actions - Std:  {actions.std(axis=0)}\")\n",
    "print(f\"Expert actions - Min:  {actions.min(axis=0)}\")\n",
    "print(f\"Expert actions - Max:  {actions.max(axis=0)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
