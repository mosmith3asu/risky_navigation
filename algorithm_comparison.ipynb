{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578ac5ae",
   "metadata": {},
   "source": [
    "# Algorithm Comparison for Behavioral Cloning\n",
    "\n",
    "This notebook provides a comparison of different neural network architectures for behavioral cloning in the risky navigation environment.\n",
    "\n",
    "**Algorithms Evaluated:**\n",
    "- **Linear**: Simple linear regression baseline\n",
    "- **AutoEncoder**: Neural network encoder-decoder architecture  \n",
    "- **Bayesian**: Bayesian neural network with uncertainty quantification\n",
    "- **Transformer**: Self-attention based model\n",
    "- **VAE**: Variational AutoEncoder with probabilistic latent representations\n",
    "\n",
    "**Workflow:**\n",
    "1. **Data Collection**: Load expert demonstrations from optimal visibility graph policy\n",
    "2. **Model Training**: Train each algorithm with simple, transparent training loop\n",
    "3. **Evaluation**: Test models in environment and compare performance\n",
    "4. **Analysis**: Visualize results and compare metrics\n",
    "\n",
    "This simplified approach prioritizes debuggability and clarity over automated hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077109",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn scikit-learn torch torchvision torchaudio gymnasium tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019876de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  if on runpod\n",
    "!rm -rf risky_navigation\n",
    "!git clone https://github.com/mosmith3asu/risky_navigation.git\n",
    "!cd risky_navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel to reload updated modules\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear the module cache for AutoEncoder\n",
    "modules_to_reload = [\n",
    "    'src.algorithms.AutoEncoder.agent',\n",
    "    'src.algorithms.Bayesian.agent', \n",
    "    'src.algorithms.Transformer.agent',\n",
    "    'src.algorithms.Linear.agent',\n",
    "    'src.algorithms.VAE.agent'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "# print current path\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c632d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment and algorithms\n",
    "import sys\n",
    "sys.path.append('/risky_navigation')\n",
    "\n",
    "from src.env.continuous_nav_env import ContinuousNavigationEnv\n",
    "from src.algorithms.AutoEncoder.agent import AutoEncoderAgent\n",
    "from src.algorithms.Bayesian.agent import BayesianAgent\n",
    "from src.algorithms.Transformer.agent import TransformerAgent\n",
    "from src.algorithms.Linear.agent import LinearAgent\n",
    "from src.algorithms.VAE.agent import VAEAgent\n",
    "from src.utils.file_management import save_pickle, load_pickle\n",
    "from src.utils.logger import Logger\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Test AutoEncoder architecture to verify fix\n",
    "print(\"\\nTesting AutoEncoder architecture:\")\n",
    "test_model = AutoEncoderAgent(state_dim=4, action_dim=2, goal_dim=2, \n",
    "                             latent_dim=32, hidden_dims=[128, 64])\n",
    "print(\"AutoEncoder architecture verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817ec8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a17c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Configuration\n",
    "CONFIG = {\n",
    "    'num_episodes': 1000,          # Episodes for data collection\n",
    "    'max_steps': 200,             # Max steps per episode\n",
    "    'batch_size': 256,            # Batch size\n",
    "    'num_epochs': 100,            # Training epochs\n",
    "    'val_ratio': 0.2,             # Validation set ratio\n",
    "    'num_test_episodes': 50,      # Episodes for testing\n",
    "    'lr': 1e-3,                   # Learning rate\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "}\n",
    "\n",
    "# Model-specific hyperparameters (manually chosen)\n",
    "MODEL_CONFIGS = {\n",
    "    'AutoEncoder': {'latent_dim': 32, 'hidden_dims': [128, 64]},\n",
    "    'Linear': {},  # No special hyperparameters\n",
    "    'Transformer': {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1},\n",
    "    'Bayesian': {'hidden_dim': 128, 'prior_std': 1.0},\n",
    "    'VAE': {'latent_dim': 32, 'hidden_dim': 128, 'beta': 1.0}\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Training epochs: {CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell RIGHT AFTER the \"Config\" section (after the CONFIG dictionary cell)\n",
    "\n",
    "# ============================================================\n",
    "# GPU OPTIMIZATION SETTINGS FOR RTX 4090\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFIGURING GPU OPTIMIZATIONS FOR RTX 4090\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Enable cuDNN auto-tuner for optimal convolution algorithms\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"✓ cuDNN benchmark mode enabled\")\n",
    "    \n",
    "    # Use TensorFloat32 (TF32) for faster matrix multiplication on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"✓ TF32 enabled for matrix operations\")\n",
    "    \n",
    "    # Set matmul precision for better performance\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    print(\"✓ Float32 matmul precision set to 'high'\")\n",
    "    \n",
    "    # Enable memory efficient attention if available\n",
    "    try:\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "        print(\"✓ Memory efficient scaled dot product enabled\")\n",
    "    except:\n",
    "        print(\"⚠ Memory efficient SDP not available (PyTorch < 2.0)\")\n",
    "    \n",
    "    # Pre-allocate GPU memory for better performance\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ GPU cache cleared\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Running on CPU will be very slow!\")\n",
    "\n",
    "# ============================================================\n",
    "# UPDATE CONFIG WITH OPTIMIZED BATCH SIZE FOR RTX 4090\n",
    "# ============================================================\n",
    "\n",
    "# Update batch size to maximize GPU utilization\n",
    "CONFIG['batch_size'] = 256  # Increased from 128 to utilize RTX 4090's 24GB VRAM\n",
    "\n",
    "print(f\"\\n✓ Batch size optimized for RTX 4090: {CONFIG['batch_size']}\")\n",
    "print(f\"✓ Expected GPU memory usage: ~8-12GB out of 24GB available\")\n",
    "print(f\"✓ This should increase GPU utilization from 20% to 80-95%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622191",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Training Function (No Hyperparameter Search)\n",
    "\n",
    "def train_agent_simple(agent, train_states, train_expert_actions, train_goals,\n",
    "                       val_states, val_expert_actions, val_goals, \n",
    "                       num_epochs=100, batch_size=256, device='cpu', verbose=True):\n",
    "    \"\"\"\n",
    "    Simple training loop with validation tracking.\n",
    "    \n",
    "    Args:\n",
    "        agent: Agent instance with train_step method\n",
    "        train/val data: Numpy arrays of states, actions, goals\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        device: torch device\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses: Lists of losses per epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    n_train = len(train_states)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(n_train)\n",
    "        \n",
    "        for start_idx in range(0, n_train, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_train)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # Convert to tensors\n",
    "            batch_states = torch.tensor(train_states[batch_indices], dtype=torch.float32, device=device)\n",
    "            batch_actions = torch.tensor(train_expert_actions[batch_indices], dtype=torch.float32, device=device)\n",
    "            batch_goals = torch.tensor(train_goals[batch_indices], dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Train step (behavioral cloning: state+goal -> expert_action)\n",
    "            loss = agent.train_step(batch_states, None, batch_goals, batch_actions)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_states_t = torch.tensor(val_states, dtype=torch.float32, device=device)\n",
    "        val_actions_t = torch.tensor(val_expert_actions, dtype=torch.float32, device=device)\n",
    "        val_goals_t = torch.tensor(val_goals, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Predict on validation set\n",
    "            inputs = torch.cat([val_states_t, val_goals_t], dim=-1)\n",
    "            if hasattr(agent, 'model'):\n",
    "                agent.model.eval()\n",
    "                predictions = agent.model(inputs)\n",
    "            elif hasattr(agent, 'encoder'):  # VAE\n",
    "                agent.encoder.eval()\n",
    "                agent.decoder.eval()\n",
    "                mu, _ = agent.encoder(inputs)\n",
    "                predictions = agent.decoder(mu)\n",
    "            else:\n",
    "                predictions = agent.predict_action(val_states_t, val_goals_t)\n",
    "            \n",
    "            val_loss = torch.nn.functional.mse_loss(predictions, val_actions_t).item()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if verbose and (epoch % 10 == 0 or epoch == num_epochs - 1):\n",
    "            print(f\"Epoch {epoch:3d}/{num_epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "            \n",
    "            # Check for NaN\n",
    "            if np.isnan(avg_train_loss) or np.isnan(val_loss):\n",
    "                print(f\"WARNING: NaN detected at epoch {epoch}!\")\n",
    "                print(f\"  Train loss: {avg_train_loss}\")\n",
    "                print(f\"  Val loss: {val_loss}\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "print(\"Simple training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652df29",
   "metadata": {},
   "source": [
    "## Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rl_experience(env, num_episodes=100, max_steps=200):\n",
    "    \"\"\"\n",
    "    Collect RL training data using optimal policy from visibility graph.\n",
    "    This provides expert demonstrations for imitation learning.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    for ep in tqdm(range(num_episodes), desc='Collecting RL experience'):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(2)\n",
    "        episode_transitions = []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            # Get optimal action using visibility graph\n",
    "            current_pos = state[:2]\n",
    "            \n",
    "            try:\n",
    "                # Use environment's visibility graph for shortest path\n",
    "                if hasattr(env, 'vgraph'):\n",
    "                    path = env.vgraph.shortest_path(current_pos, goal)\n",
    "                    \n",
    "                    if len(path) > 1:\n",
    "                        # Direction to next waypoint\n",
    "                        next_waypoint = path[1]\n",
    "                        direction = next_waypoint - current_pos\n",
    "                        action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "                    else:\n",
    "                        action = np.zeros(2)\n",
    "                else:\n",
    "                    # Fallback: direct to goal\n",
    "                    direction = goal - current_pos\n",
    "                    action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Fallback: move towards goal\n",
    "                direction = goal - current_pos\n",
    "                action = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "            \n",
    "            # Clip to action space\n",
    "            action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition (s, a, r, s', done)\n",
    "            episode_transitions.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action.copy(),\n",
    "                'reward': reward,\n",
    "                'next_state': next_state.copy(),\n",
    "                'done': done,\n",
    "                'goal': goal.copy()\n",
    "            })\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if info.get('reason') == 'goal_reached':\n",
    "                    successful_episodes += 1\n",
    "                break\n",
    "        \n",
    "        # Add all transitions from this episode\n",
    "        data.extend(episode_transitions)\n",
    "    \n",
    "    print(f\"Collected {len(data)} transitions from {num_episodes} episodes\")\n",
    "    print(f\"Success rate: {successful_episodes/num_episodes:.2%}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_rl_data(data):\n",
    "    \"\"\"Convert RL experience to arrays for training.\"\"\"\n",
    "    states = np.stack([d['state'] for d in data])\n",
    "    actions = np.stack([d['action'] for d in data])\n",
    "    rewards = np.array([d['reward'] for d in data])\n",
    "    next_states = np.stack([d['next_state'] for d in data])\n",
    "    dones = np.array([d['done'] for d in data])\n",
    "    goals = np.stack([d['goal'] for d in data])\n",
    "    \n",
    "    return states, actions, rewards, next_states, dones, goals\n",
    "\n",
    "# Collect or load RL experience data\n",
    "dataset_path = 'rl_experience_dataset.pickle'\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Loading existing RL dataset from {dataset_path}\")\n",
    "    data = load_pickle(dataset_path)\n",
    "else:\n",
    "    print(\"Collecting RL experience data using optimal policy...\")\n",
    "    data = collect_rl_experience(env, CONFIG['num_episodes'], CONFIG['max_steps'])\n",
    "    save_pickle(data, dataset_path)\n",
    "    print(f\"RL dataset saved to {dataset_path}\")\n",
    "\n",
    "# Prepare RL data\n",
    "states, actions, rewards, next_states, dones, goals = prepare_rl_data(data)\n",
    "\n",
    "print(f\"\\nRL Dataset Statistics:\")\n",
    "print(f\"  Total transitions: {len(states)}\")\n",
    "print(f\"  Avg reward: {rewards.mean():.3f}\")\n",
    "print(f\"  Success rate (goal reached): {(rewards > 0).sum() / len(rewards):.2%}\")\n",
    "print(f\"  State shape: {states.shape}\")\n",
    "print(f\"  Action shape: {actions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions from the collected data\n",
    "STATE_DIM = states.shape[1]\n",
    "ACTION_DIM = actions.shape[1]\n",
    "GOAL_DIM = goals.shape[1]\n",
    "\n",
    "print(f\"Data dimensions:\")\n",
    "print(f\"  STATE_DIM = {STATE_DIM}\")\n",
    "print(f\"  ACTION_DIM = {ACTION_DIM}\")\n",
    "print(f\"  GOAL_DIM = {GOAL_DIM}\")\n",
    "print(f\"  Total samples = {len(states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment for evaluation\n",
    "env = ContinuousNavigationEnv()\n",
    "print(f\"Environment initialized: {env}\")\n",
    "print(f\"  State space: {env.observation_space.shape}\")\n",
    "print(f\"  Action space: {env.action_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49cc82",
   "metadata": {},
   "source": [
    "## Train All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SIMPLIFIED TRAINING - ALL ALGORITHMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data into train/val\n",
    "n_samples = len(states)\n",
    "n_train = int(n_samples * (1 - CONFIG['val_ratio']))\n",
    "\n",
    "# Shuffle data\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "# Split data\n",
    "train_states = states[train_indices]\n",
    "train_actions = actions[train_indices]\n",
    "train_goals = goals[train_indices]\n",
    "\n",
    "val_states = states[val_indices]\n",
    "val_actions = actions[val_indices]\n",
    "val_goals = goals[val_indices]\n",
    "\n",
    "print(f\"Data split: {len(train_states)} train, {len(val_states)} val\")\n",
    "print()\n",
    "\n",
    "# Dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "# Train each algorithm\n",
    "algorithms_to_train = {\n",
    "    'Linear': (LinearAgent, {}),\n",
    "    'AutoEncoder': (AutoEncoderAgent, MODEL_CONFIGS['AutoEncoder']),\n",
    "    'Transformer': (TransformerAgent, MODEL_CONFIGS['Transformer']),\n",
    "    'Bayesian': (BayesianAgent, MODEL_CONFIGS['Bayesian']),\n",
    "    'VAE': (VAEAgent, MODEL_CONFIGS['VAE'])\n",
    "}\n",
    "\n",
    "for algo_name, (AgentClass, model_config) in algorithms_to_train.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {algo_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Config: {model_config}\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = AgentClass(\n",
    "        state_dim=STATE_DIM,\n",
    "        action_dim=ACTION_DIM,\n",
    "        goal_dim=GOAL_DIM,\n",
    "        lr=CONFIG['lr'],\n",
    "        device=CONFIG['device'],\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nTraining for {CONFIG['num_epochs']} epochs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses, val_losses = train_agent_simple(\n",
    "        agent, \n",
    "        train_states, train_actions, train_goals,\n",
    "        val_states, val_actions, val_goals,\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        device=CONFIG['device'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    all_results[algo_name] = {\n",
    "        'agent': agent,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{algo_name} Training Complete!\")\n",
    "    print(f\"  Train time: {train_time:.2f}s\")\n",
    "    print(f\"  Final train loss: {train_losses[-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {val_losses[-1]:.6f}\")\n",
    "    print(f\"  Min val loss: {min(val_losses):.6f} (epoch {np.argmin(val_losses)})\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for algo_name, results in all_results.items():\n",
    "    epochs = range(len(results['train_losses']))\n",
    "    axes[0].plot(epochs, results['train_losses'], label=algo_name, alpha=0.7)\n",
    "    axes[1].plot(epochs, results['val_losses'], label=algo_name, alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Loss Summary:\")\n",
    "print(f\"{'Algorithm':<15} {'Train Loss':<15} {'Val Loss':<15} {'Train Time (s)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for algo_name, results in all_results.items():\n",
    "    print(f\"{algo_name:<15} {results['train_losses'][-1]:<15.6f} {results['val_losses'][-1]:<15.6f} {results['train_time']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cee6b",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agents\n",
    "\n",
    "Now test the trained agents in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dfdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATING ALL AGENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_agent_simple(agent, env, num_episodes=50, max_steps=200):\n",
    "    \"\"\"Evaluate agent in environment.\"\"\"\n",
    "    results = {\n",
    "        'rewards': [],\n",
    "        'successes': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = env.goal.copy() if hasattr(env, 'goal') else np.zeros(GOAL_DIM)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Predict action\n",
    "            state_t = torch.tensor(state, dtype=torch.float32, device=CONFIG['device'])\n",
    "            goal_t = torch.tensor(goal, dtype=torch.float32, device=CONFIG['device'])\n",
    "            action = agent.predict_action(state_t, goal_t)\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(action, torch.Tensor):\n",
    "                action = action.cpu().numpy()\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        results['rewards'].append(episode_reward)\n",
    "        # Check if goal was reached\n",
    "        success = info.get('reason', '') == 'goal_reached' if done else False\n",
    "        results['successes'].append(1 if success else 0)\n",
    "        results['steps'].append(step + 1)\n",
    "    \n",
    "    return {\n",
    "        'avg_reward': np.mean(results['rewards']),\n",
    "        'std_reward': np.std(results['rewards']),\n",
    "        'success_rate': np.mean(results['successes']),\n",
    "        'avg_steps': np.mean(results['steps'])\n",
    "    }\n",
    "\n",
    "# Evaluate each agent\n",
    "eval_results = {}\n",
    "\n",
    "for algo_name, results_dict in all_results.items():\n",
    "    print(f\"\\nEvaluating {algo_name}...\")\n",
    "    agent = results_dict['agent']\n",
    "    \n",
    "    eval_res = evaluate_agent_simple(\n",
    "        agent, env, \n",
    "        num_episodes=CONFIG['num_test_episodes'],\n",
    "        max_steps=CONFIG['max_steps']\n",
    "    )\n",
    "    \n",
    "    eval_results[algo_name] = eval_res\n",
    "    \n",
    "    print(f\"  Avg Reward: {eval_res['avg_reward']:.3f} ± {eval_res['std_reward']:.3f}\")\n",
    "    print(f\"  Success Rate: {eval_res['success_rate']:.1%}\")\n",
    "    print(f\"  Avg Steps: {eval_res['avg_steps']:.1f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results table\n",
    "import pandas as pd\n",
    "\n",
    "results_data = []\n",
    "for algo_name in all_results.keys():\n",
    "    train_res = all_results[algo_name]\n",
    "    eval_res = eval_results[algo_name]\n",
    "    \n",
    "    results_data.append({\n",
    "        'Algorithm': algo_name,\n",
    "        'Train Loss': train_res['train_losses'][-1],\n",
    "        'Val Loss': train_res['val_losses'][-1],\n",
    "        'Train Time (s)': train_res['train_time'],\n",
    "        'Avg Reward': eval_res['avg_reward'],\n",
    "        'Success Rate': eval_res['success_rate'],\n",
    "        'Avg Steps': eval_res['avg_steps']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "df_results = df_results.sort_values('Val Loss')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Success Rate\n",
    "axes[0].bar(df_results['Algorithm'], df_results['Success Rate'])\n",
    "axes[0].set_ylabel('Success Rate')\n",
    "axes[0].set_title('Success Rate by Algorithm')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average Reward\n",
    "axes[1].bar(df_results['Algorithm'], df_results['Avg Reward'])\n",
    "axes[1].set_ylabel('Average Reward')\n",
    "axes[1].set_title('Average Reward by Algorithm')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "axes[2].bar(df_results['Algorithm'], df_results['Val Loss'])\n",
    "axes[2].set_ylabel('Validation Loss')\n",
    "axes[2].set_title('Validation Loss by Algorithm')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
